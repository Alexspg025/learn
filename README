
Language Learning
-----------------
* Linas Vepstas December 2013
* Updated May 2017
* Updated June 2018

Current project, under construction.  See the
[language learning wiki](http://wiki.opencog.org/w/Language_learning)
for an alternate overview.

Summary
-------
The goal of the project is to build a system that can learn parse
dictionaries for different languages, and possibly do some rudimentary
semantic extraction.  The primary design point is that the learning is
to be done in an unsupervised fashion.  A sketch of the theory that
enables this can be found in the paper "Language Learning", B. Goertzel
and L. Vepstas (2014) on [ArXiv abs/1401.3372](https://arxiv.org/abs/1401.3372)
A shorter sketch is given below.  Most of this README concerns the
practical details of configuring and operating the system, and some
diary-like notes about system configuration and operation. A diary of
scientific notes and results is in the `learn-lang-diary` directory.

The basic algorithmic steps, as implemented so far, are as follows:
A) Ingest a lot of raw text, such as novels and narrative literature,
   and count the occurrence of nearby word-pairs.
B) Compute the mutual information (mutual entropy) between the word-pairs.
C) Use a Minimum-Spanning-Tree algorithm to obtain provisional parses
   of sentences.  This requires ingesting a lot of raw text, again.
   (Independently of step A)
D) Extract linkage disjuncts from the parses, and count their frequency.
E) Use agglomerative clustering to merge similar linkage disjuncts,
   while simultaneously merging similar words.  This will result in
   a set of word-classes (grammatical classes) with disjuncts on them
   that use word-class connectors.
F) Place word-classes and word-class connectors into a link-grammar
   dictionary.
G) Parse a large quantity of raw text, using the parser constructed in
   the previous step. Using maximum-entropy-style techniques, attempt
   to extract higher-ordder relations (resolve anaphora and other
   referents, determine rheme+theme, learn "lexical functions")

Currently, the software implements steps A, B, C, D, F and much of
step E, although this step remains a topic of current research.
Its quite unclear how to determine the quality of lexis that falls out
of step F.  Some initial experiments on step G have been performed,
and look promising.

Steps A-C are "well-known" in the academic literature, with results
reported by many researchers over the last two decades. The results
from Steps D & E are new, and have never been published before.
(Results from Step D can be found in the file, in this directory
`learn-lang-diary/drafts/connector-sets.lyx`, the PDF of which
was posted to the mailing lists)

It is very important to understand that Step E is VERY DIFFERENT from
commonly-reported algorithms used in the published literature, such as
using K-means clustering and dimensional reduction to obtain word
classes (gramatical categories). The reason for this is that the
disjuncts provide connectivity information, and one is NOT clustering
over a vector space, one must instead cluster over a "sheaf".  That is,
the basis-elements of the vector space themselves have non-trivial
structure (which must not be ignored).  Sheaves resemble vector spaces,
but are not the same.  They capture and describe the connectivity
information in the "basis elements".

As a result, NONE of the industry-standard classifiers and clustering
algorithms can be applied to step E: they all assume that the input
data can be structured as a vector space.  The axioms of algebraic
linguistics can resemble the axioms of a vector space, which is why
vector-space techniques can work pretty well in the machine-learning
of linguistic structure. However, ultimately, the axioms of algebraic
linguistics are NOT the axioms of a vector space.  In practical terms,
this means that clustering/classification algorithm used in step E is
completely different than anything else available in the industry.

The complete novelty of step E, coupled to the need to perform
experiments in earlier and later stages means that industry-standard,
generic big-data, machine-learning tools are wholly inadequate for the
task. It is for thus reason that all data processing is being done in
the AtomSpace, and not in some other machine learning or big-data
framework. Existing industry tools are not adequate for proper
linguistic analysis. This is a very important reason for developing
the AtomSpace, and other OpenCog infrastructure: to get past tool
limitations.

All of the statistics gathering is done within the OpenCog AtomSpace,
where counts and other statistical quantities are associated with various
different hypergraphs.  The contents of the atomspace are saved to an
SQL (Postgres) server for storage.  The system is fed with raw text
using assorted ad-hoc scripts, which include link-grammar as a central
component of the processing pipeline. Most of the data analysis is
performed with an assortment of scheme scripts.

Thus, operating the system requires three basic steps:
* Setting up the atomspace with the SQL backing store,
* Setting up the misc scripts to feed in raw text, and
* Processing the data after it has been collected.

Each of these is described in greater detail in separate sections below.

If you are lazy, you can just hop to the end, to the section titled
"Precomputed LXC containers", and grab one of those.  It will have
everything in it, up to the last step.  Of course, you won't know what
to do with it, if you don't read the below. Its not a magic "get smart"
pill. You've got a lot of RTFM in front of you.


Setting up the AtomSpace
------------------------
This section describes how to set up the atomspace to collect
statistics.  The most time-consuming, difficult and error-prone step
is the setup and configuration of postgres.  Postgres is centrally
important for saving partial results.

0.1) Optional. If you plan to run the pipeline on multiple
   different languages, it can be convenient, for various reasons,
   to run the processing in an LXC container. If you already know
   LXC, then do it. If not, or this is your first time, then don't
   bother right now. Come back to this later.

   LXC containsers are nice because:
   * Each container can have a different config, different datasets,
     and be executing different steps of the pipeline.  This is great,
     for juggling multiple jobs.
   * The system install in one container won't corrupt the other
     containers; you can experiment, without impacting stable systems.
   * LXC containers can be stopped and move to other system with more
     RAM (or less RAM), more disk, or less disk. Handy for load-
     balancing, and/or easily moving to a bigger system.

0.2) (Optional but strongly recommended) Install system shutdown scripts.
   This will help protect your data in case of an unexpected power loss.
   These scripts, and the instructions for them, are located in the
   `run/rc.local.shutdown`, `run/rc-local-shutdown.service` and the
   `run/rc.lxc.shutdown` files. You will need to alter these files, and
   use your own login credentials in place of the generic ones.

0.3) Probably mandatory.  Chances are good that you'll work with large
   datasets; in this case, you need to do the below. Skipping this
   step will lead to the error `Too many heap sections: Increase
   MAXHINCR or MAX_HEAP_SECTS`. So:
```
   git clone https://github.com/ivmai/bdwgc
   cd bdwgc
   git checkout release-7_6
   ./autogen.sh
   ./configure --enable-large-config
   make; sudo make install
```

0.4) The atomspace MUST be built with guile version 2.2.2.1 or newer,
   which can only be obtained from git: that is, by
```
   git clone git://git.sv.gnu.org/guile.git
   git checkout stable-2.2
```
   Earlier versions have problems of various sorts. Version 2.0.11
   will quickly crash with the error message: `guile: hashtab.c:137:
   vacuum_weak_hash_table: Assertion 'removed <= len' failed.`

   Also par-for-each hangs:
   https://debbugs.gnu.org/cgi/bugreport.cgi?bug=26616
   (in guile-2.2, it doesn't hang, but still behaves very badly)

0.5) Create/edit the `~/.guile` file and add the content below.
   This makes the arrow keys work, and prints nicer stack traces.
```
   (use-modules (ice-9 readline))
   (activate-readline)
   (debug-enable 'backtrace)
   (read-enable 'positions)
   (add-to-load-path "/usr/local/share/opencog/scm")
   (add-to-load-path ".")
```

0.6) Opencog should be built with `link-grammar-5.5.0` or newer.
   The currently installed version can be displayed by running
```
   link-parser --version
```
   Newer versions are available at:
   https://www.abisource.com/downloads/link-grammar/

   The main link-grammar project page is here:
   https://www.abisource.com/projects/link-grammar/

1) Set up and configure postgres, as described in
   `atomspace/opencog/persist/sql/README.md`

2) Create and initialize a database. Pick any name you want; here it
   is `learn-pairs`.  Later on, you will have to place this name into
   a config file (see below).
```
   createdb learn-pairs
   cat atomspace/opencog/persist/sql/multi-driver/atom.sql | psql learn_pairs
```

3) Copy all files from the `opencog/opencog/nlp/learn/run` directory to
   a new directory; suggest the directory `run-practice`.  Its best to
   try a few practice runs before commiting to serious data processing.
   The next number of steps describe how to do a practice run; a later
   section focuses on batch processing.

   There are two types of files in this directory: generic processing
   scripts, and language-specific configuration files. Different
   languages require different processing pipelines; e.g. most languages
   will require a morphology processing step; English does not.
   Different languages will typically train on a different set of
   corpora organized in different directories in different ways. The
   multitude of config files allows each language to be configured in
   sharply different ways, running different kinds of experiments.

   For the practice run, suggest picking English, and using the `en`
   files. The other language files can be ignored (and can be deleted).

4) Start the OpenCog server.  Later on, the batch processing
   instructions (below) indicate how to automate this. However, for
   the practice run, it is better to do all this by hand.

   First, review the contents of `config/opencog-pairs-en.conf`. This
	simply declares the prompts that the cogserver will use; most
   importantly, it declares the port number for the cogserver. Its's
   currently coded to be 17005.

   Edit the `pair-count-en.scm` and hard-code your database credentials
   into it. This sevaes you the trouble of having to remember them, and
   to type them in by hand.

   Finally, start the cogserver by
```
  guile -l pair-count-en.scm
```

5) Verify that the pair-counting pipeline works. In a second terminal,
   try this:
```
   rlwrap telnet localhost 17005
   opencog-en> (observe-text "this is a test")
```
   The port number 17005 was from the above-mentioned config file.

   Better yet:
```
   echo -e "(observe-text \"this is a another test\")" |nc localhost 17005
   echo -e "(observe-text \"Bernstein () (1876\")" |nc localhost 17005
   echo -e "(observe-text \"Lietuvos žydų kilmės žurnalistas\")" |nc localhost 17005
```

   This should result in activity in the cogserver and on the database:
   the "observe text" scheme code sends the text for parsing, counts
   the returned word-pairs, and stores them in the database.

   If you are truly curious, type in `(sql-stats)` in the guile shell.
   This will print some very technical stats about the Atomspace SQL
   database backend.

6) Verify that the above resulted in data sent to the SQL database.
   Log into the database, and check:
```
   psql learn-pairs
   learn-pairs=# SELECT * FROM atoms;
   learn-pairs=# SELECT COUNT(*) FROM atoms;
   learn-pairs=# SELECT * FROM valuations;
```
   The above shows that the database now contains word-counts for
   pair-wise linkages for the above sentences. If the above are empty,
   something is wrong. Go to back to step zero and try again.

7) Halt the coserver by killing the guile process. In the next stage,
   it will be started in a different way, and having a running server
   here will interfer with things.  Unless, of course, you are careful
   to juggle the config files. There's nothing wrong with running
   multiple servers at the same time: you just have to be careful to
   avoid clashes, and for that, you can to be careful with config-file
   settings.

That's for the practice run. If stuff is showing up in the database,
then processing is proceeding as expected.

The next step is to set up bulk test processing. There are three general
stages: (I) collection of word-pair statistics (II) collection of
disjunct statistics (III) clustering.  These are described next.

Note Bene: Please do NOT spend any time at all trying to figure out
what is stored in the SQL tables.  There are "no user-servicable parts
inside" and there is "risk of electrical shock". The SQL tables are a
very twisted, distorted and awkward representation of the AtomSpace
contents. Its kind-of like reading assembly code: you will not learn
how the AtomSpace works by reading the SQL code.  In fact, doing this
will probably make you anti-learn, by planting incorrect ideas in your
head.  Don't look in there. You have been warned!
dones revenging


Bulk Pair Counting
------------------
The first major stage of data processing is the generation of word-pair
statistics. Later stages depend on high-quality word-pair statistics,
and this is best obtained by processing a large number of text files;
typically thousands or tens of thousands of them, for run-time of a
few days to a week or two.  For a practice run, half-an-hour is
sufficient, but a half-hours worth of data will be quite low-quality.

The best text for "natural" English is narrative literature, adventure
and young-adult novels, and newspaper stories. These contain a good
mix of common nouns and verbs, which is needed for conversational
natural language.

It turns out that Wikipedia is a poor choice for a dataset. That's
because the "encyclopedic style" means it contains very few pronouns,
and very few action-verbs (hit, jump, push, take, sing, love). Most
Wikipedia articles state facts, describe objects, ideas and events
(primarily using the verbs is, has, was). It also contains large
numbers of product names, model numbers, geographical place names,
and foreign language words, which do little or nothing for learning
grammar. Finally, it has large numbers of tables and lists of dates,
awards, ceremonies, locations, sports-league names, battles, etc.
that get mistaken for valid sentences (they are not; they are lists),
and leads to unusual deductions of grammar.  Thus, it turns out
Wikipedia is a bad choice for learning text.

There are various scripts in the `download` directory for downloading
and pre-processing texts from Project Gutenberg, Wikipedia, and the
"Archive of Our Own" fan-fiction website.

8) Explore the scripts in the `download` directory. Download or
   otherwise obtain a collection of texts to process. Put them in
   some master directory, and also copy them to the `beta-pages`
   directory.  The name `beta-pages` is not mandatory, but some of
   the default configurations look for files there. During processing,
   files are moved to the `split-pages` directory and finally to the
   `submitted-pages` directory. Processing continues until the scripts
   are interupted, or until the `beta-pages` directory is empty.

   Little or no pre-processing of the text files are needed. Mostly,
   one just needs to remove HTML markup or other binary crud. The
   pipeline expects all files to be in UTF-8 format.  The pipeline
   does not support any other format; if you have other formats,
   learn how to use the `iconv` command.

   There is no need to perform sentence splitting or other normalization
   (such as down-casing of initial words).  In fact, just about any
   pre-processing will lower the quality of the input data;
   pre-processing is strongly discouraged.

   It is best to split texts into files containing a few hundred to
   at most ten-thousand sentences each; larger files cause trouble
   if the counting needs to be restarted.

9) Consider investing in an uninterruptible power supply (UPS) to
   avoid data loss/data corruption in the event of a power outage.
   Unexpected power outages can result in weeks or a month of lost
   work. You have been warned!

   Consider investing in a pair of solid-state disk drives (SSD),
   configured as RAID-1 (mirroring) to hold the postgres database.
   Pair-counting is database intensive, and the writes to disk can
   be a bottleneck to performance.

10) Review the `observe-text` function in `link-pipeline.scm`. The
   default, as it is, is fine, and this is almost surely what you want.
   (And so you can skip this step).  Just be aware that this function
   was written to collect a large amount of additional information,
   which you can explore, if you are adventurous. Caution, though:
   it creates a deluge of data.

   The `observe-text` function in 'link-pipeline.scm` collects
   observation counts on four different kinds of structures:

   * Lg "ANY" word pairs -- how often a word-pair is observed.
   * Word counts -- how often a word is seen.
   * Clique pairs, and pair-lengths -- This counts pairs using the "clique
                  pair" counting method.  The max length between words
                  can be specified. Optionally, the lengths of the pairs
                  can be recorded. Caution: enabling length recording
                  will result in 6x or 20x more data to be collected,
                  if you've set the length to 6 or 20.  That's because
                  any given word pair will be observed at almost any
                  length apart, each of these is an atom with a count.
                  Watch out!
   * Disjunct counts -- how often the random ANY disjuncts are used.
                  You almost surely do not need this.  This is for my
                  own personal curiosity.

   Only the first bullet is needed: Lg "ANY" word pairs. The other
   bullets are not needed. In particular, word counts are not needed;
   they are almost evil to collect, as they serve to confuse things.

   Knowing th length of the pairs might be important for "understanding"
   that adjectives, adverbs, determiners and possessives modify only
   nearby words. They might be important in analyzing morphology.
   Currently, no other code examines lengths; this is an open
   experiment.

11) (Optional) Review the `sometimes-gc` and `maybe-gc` settings in
   the file `link-pipeline.scm`.  These force garbage collection to
   occur more often than normal; they help keep the process size
   reasonable.  The current default is to force garbage collection
   whenever the guile heap exceeds 750 MBytes; this helps keep RAM
   usage down on small-RAM machines.  However, it does cost CPU time.
   Adjust the `max-size` parameter in `observe-text` in the
   `link-pipeline.scm` file to suit your whims in RAM usage and time
   spent in GC.  The default should be adequate for almost all users.


12) Its conenient to have lots of terminals open and ready for use;
   the `byobu+tmux` terminal server provides this, without chewing up
   a lot of screen real-estate.  The `run-shells.sh` script will run
   a `byobu/tmux` session, start the cogserver in one of the terminals,
   and leave open several other terminals for general use.

   Make a copy this file, and manually alter it to start the cogserver
   the way that you want it started.

   The `wiki-ss-en.sh` script starts the process of observing text
   for pair-counting. It looks for files in the `beta-pages` directory,
   performs sentence-splitting (ss) on each, and then passes these to
   the pair-counting scripts.

   Make a copy of this file, and edit it to change the location of your
   text files (if not using the default `beta-pages`) and to change the
   port number (if not using the default 17005 for English). For
   convenience, there are other files `wiki-ss-??.sh`, set up for other
   languages, using alternative default port numbers.

   Run `wiki-ss-en.sh` in one of the byobu terminals.

13) Pair-counting can take days or weeks.  The `wiki-ss-en.sh` will run
   until all of the text files have been processed, or until it is
   interrupted.  If it is interrupted, it can be restarted; it will
   resume with the previous unfinished text file. As text files are
   being processed, they are moved to the `split-articles` directory
   and then, to the `submitted-articles` directory.  The input directory
   will gradually empty out, the `submitted-articles` directory will
   gradually fill up.  Progress can be monitored by saying `find |wc`.

   Although high-quality pair-counts require a minimum of several days
   of processing, a half-hour or so is sufficient for a trial run.
   Feel free to ctrl-C at this point, and move to the next step
   (covered in the section **Mutual Information** below.)

Notes:
* Segmentation for Chinese. Currently, only some preliminary work has
  been done for word segmentation in Chinese. It is incomplete. In the
  meanwhile, if you want to just skip to the next step, you can use an
  off-the-shelf segmenter.  Its got OK-but-not-great accuracy.

  This can be done using jieba
      https://github.com/fxsjy/jieba
  If you are working with Chinese texts, install:
  `pip install jieba` and then segment text:
  `run/jieba-segment.py PATH-IN PATH-OUT`. This python utility is in
  the `run` directory.  You will need to create modified versions of
  the `run/ss-one.sh` and `run/mst-one.sh` scripts to invoke jieba.

  To run this segmenter, you will also need to run the sentence-splitter
  differently.  Use the `zh-pre` or `yue-pre` languages with the
  sentence splitter -- these will split on sentence boundaries, only,
  and do no other processing.  By contrast, the `zh` or `yue` languages
  insert a blank space between hanzi characters.

* Morphology. Some work on morphology has been done; but there are no
  pre-written scripts for the processing pipeline to handle this. You
  are on your own, for now.


XXXXXXXXXXXXXXX
everything below here is wrong or misleading or incomplete.
Please wait.

xxxxxx


Bulk Text Parsing
-----------------
Set up distinct databases, one for each language you will work with:
```
    createdb fr_pairs lt_pairs pl_pairs simple_pairs
    cat atomspace/opencog/persist/sql/multi-driver/atom.sql | psql ??_pairs
```

The pair-counting scripts expect your text files to be stored in a directory
called `beta-pages` in your working directory. If you used the provided
download scripts, you should have your files in `alpha-pages`.
Now, build some working directories (so that we don't mess up
alpha-pages after all that work)
```
    cp -pr alpha-pages beta-pages
```
If you got your texts in some other way, just copy the files you want to
process into beta-pages (keep in mind they will be removed from there).

* Review the file `opencog/nlp/learn/run/README`, and then copy the
  scripts into your working dir, if you didn't do it before.
* Here's a brief description of the relevant files for Text Parsing.
  They don't need modification by default, unless you want to do
  something unusual.

  -- `run-server-parse.sh` creates a `byobu` session with different
   terminals, where you can keep an eye on the processes. One terminal
   calls `observe-launch.scm` to start the cogserver, and another one
   telnets into it using a language-specific port.
   You need to pass it the language of the text to process and the database
   credentials (see next step).

  -- `wiki-ss-??.sh` performs batch processing of input text files in
     the specified language. This calls `ss-one.sh` to process one text file.

  -- `ss-one.sh` calls `split-sentences.pl` to split the text file into
     sentences, and then calls `submit-one.pl` to send the sentences to the
     cogserver for parsing. None of these should need any modification.

  -- `opencog-??.conf` contains configuration settings for the cogserver, 
     including the language-specific port to use.

  -- `observe-launch.scm` starts the cogserver and opens the database. 

* Run `run-server-parse.sh` in your working directory.
      ```
        ./run-server-parse.sh lang ??-pairs your_user your_password
      ```
      (user and password are optional, as explained in step 5 above).

* In the parse tab of byobu (you can navigate with the F3 and F4 keys),
  run `./wiki-ss-??.sh`
   If the above command shows the error
   ```
    nc: invalid option -- 'N'
   ```
   open ss-one.sh and remove the -N option from the nc commands.

* Wait a few days.
* When finished, stop the cogserver
* This requires postgres 9.3 or newer, for multiple reasons. One reason
  is that older postgres don't automatically VACUUM (see "performance"
  below) The other is the list membership functions are needed.
* Be sure to perform the postgres tuning recommendations found in
  various online postgres performance wikis, or in the
  `atomspace/opencog/persist/sql/README.md` file.  See also 'Performance' section
  below.

Some handy SQL commands:
```
SELECT count(uuid) FROM Atoms;
select count(uuid) from atoms where type =123;
```

type 123 is `WordNode` for me; verify with
```
SELECT * FROM Typecodes;
```

The total count accumulated is
```
select sum(floatvalue[3]) from valuations where type=7;
```
where type 7 is `CountTruthValue`.


Mutual Information of Word Pairs
--------------------------------
After accumulating a few million word pairs, we're ready to compute
the mutual entropy between them.
This is done by running

XXX This is too complicated!  Go back to the old way of doing things.

```
guile -l compute-mi.scm  -- --lang ?? --db ??-pairs --user your_user --password asdf
```
(again, --user and --password are optional, as explained in step 5 above).
It uses the `(opencog matrix)` subsystem to perform the core work.

Batch-counting might take hours or longer, depending on your dataset
size. The batching routine will print to stdout, giving a hint of
the rate of progress.

Example stats and performance:
current fr_pairs db has 16785 words and 177960 pairs.

This takes 17K + 2x 178K = 370K total atoms loaded.
These load up in 10-20 seconds-ish or so.

New fr_pairs has 225K words, 5M pairs (10.3M atoms):
Load 10.3M atoms, which takes about 10 minutes cpu time to load
20-30 minutes wall-clock time (500K atoms per minute, 9K/second
on an overloaded server).

RSS for cogserver: 436MB, holding approx 370K atoms
So this is about 1.2KB per atom, all included. Atoms are a bit fat...
... loading all pairs is very manageable even for modest-sized machines.

RSS for cogserver: 10GB, holding 10.3M atoms
So this is just under 1KB per atom.

(By comparison, direct measurement of atom size i.e. class Atom:
typical atom size: 4820384 / 35444 = 136 Bytes/atom
this is NOT counting indexes, etc.)

For dataset (fr_pairs) with 225K words, 5M pairs:
Current rate is 150 words/sec or 9K words/min.

After the single-word counts complete, and all-pair count is done.
This is fast, takes a couple of minutes.

Next: batch-logli takes 540 seconds for 225K words

Finally, an MI compute stage.
Current rate is 60 words/sec = 3.6K per minute.
This rate is per-word, not per word-pair .

Update Feb 2014: fr_pairs now contains 10.3M atoms
SELECT count(uuid) FROM Atoms;  gives  10324863 (10.3M atoms)
select count(uuid) from atoms where type = 77; gives  226030 (226K words)
select count(uuid) from atoms where type = 8;  gives 5050835 (5M pairs ListLink)
select count(uuid) from atoms where type = 27; gives 5050847 (5M pairs EvaluationLink)


Minimum Spanning Tree parsing
-------------------------------
The MST parser discovers the minimum spanning tree that connects the
words together in a sentence.  The link-cost used is (minus) the mutual
information between word-pairs (so we are maximizing MI). Thus, MST
parsing cannot be started before the above steps to compute word-pair
MI have been accomplished.

Minimum spanning tree code is in `mst-parser.scm`. The current version
works well.

To perform MST parsing in bulk:
* Review the file `opencog/nlp/learn/run/README`, and then copy the
  scripts into your working dir, if you didn't do it before.
  `run-server-mst.sh` starts the cogserver and sets a default prompt: set 
  up by default to avoid conflicts and confusion, and to allow multiple 
  languages to be processed at the same time. 
  You need to pass the language and database credentials as arguments; 
  see next steps
  `wiki-mst-??.scm` starts the process of observing the text files contained
  in the gamma-pages folder, by calling `mst-one.sh`, which sends a sentence
  to the cogserver with `submit-one.pl`, giving the correct opencog command.

* (Optional but suggested) Make a copy of your word-pair database, "just
  in case".  You can copy databases by saying:
````
  createdb -T existing_dbname new_dbname
```

* Edit `wiki-mst-??.sh` and change the directory `gamma-pages` to
  whatever directory holds your source text (or copy your text to
  the `gamma-pages` directory). Be aware that, during processing,
  text files are removed from this directory.

* If you are interested in the actual sentence parses, open
  `mst-one.sh` and change the observe variable from `observe-mst` to
  `observe-mst-extra`. Parses will be written in `mst-parses.txt`.

* Run `run-server-mst.sh` in your working directory.
      ```
        ./run-server-parse.sh lang new_dbname your_user your_password
      ```
      (user and password are optional, as explained before).

  Wait 10 to 60+ minutes for the guile prompt to appear.  This script
  opens a connection to the database, and then loads all word-pairs
  into the atomspace.  This can take a long time, depending on the
  size of the database. The word-pairs are needed to get the pair-costs
  needed to perform the MST parse.

* Once the above has finished loading, the parse script can be
  started. Run `wiki-mst-??.sh` in one of the unused byob windos,
  (navigate with F3, F4) and wait a few days for data to accumulate. 
   If the above command shows the error
   ```
    nc: invalid option -- 'N'
   ```
   open mst-one.sh and remove the -N option from the nc commands.

* When finished, stop the cogserver. Once this is done, you can move to 
  the next step, which is exploring the resulting connector-set (disjunct)
  database. You can also review the sentence parses in `mst-parses.txt`.

* If the process is stopped for any reason, you can just re-run these
  scripts; they will pick up where they left off.


Exploring Connector-Sets
-------------------------
Once you have a database with some fair number of connector sets in it,
you can start exploring.

* (optional) Make a copy of the MST database created above.

* Do not run the connector-set code at the same time (on the same
  database) as the MST parser. For one, if you are actively updating
  the counts, then the connector-set counting will get confused.
  Also, if you have two servers writing to the same database
  at the same time, the issueing of UUID's will get confused, and
  one or both the servers will crash, and possible database corruption
  may occur.  It is possible to have multiple writers (and I've done
  this before, any years ago), but this takes additional configuration,
  and a miscellany of coding changes and a couple of enhancements, to
  keep things in sync.

* So, assuming a clean start: just start `guile` by hand, and enter
  the following commands (by hand). They load the full disjunct/
  connector-set database into the atomspace; it needs to be loaded
  in order to get access to the cosine-distance tool.
```
  (use-modules (opencog) (opencog persist) (opencog persist-sql))
  (use-modules (opencog nlp) (opencog nlp learn))
  (use-modules (opencog matrix))
  (sql-open "postgres:///en_pairs_sim?user=linas")
  (fetch-all-words)
  (length (get-all-words))
  ; This reports 396262 for one my DB's has.

  (define pca (make-pseudo-cset-api))
  (define psa (add-pair-stars pca))
  (psa 'fetch-pairs)
  (define all-cset-words (get-all-cset-words))
  (length all-cset-words)
  ; This reports 37413 in for my `en_pairs_sim`.
  (define all-disjuncts (get-all-disjuncts))
  (length all-disjuncts)
  ; This reports 291637 in for my `en_pairs_sim`.

```
  You can now play games:
```
 (cset-vec-cosine (Word "this") (Word "that"))
 (cset-vec-cosine (Word "he") (Word "she"))

```
  Recall that subroutine documentation can be gotten by typing
  `,apropos` or `,a` for short at the guile command line.  Docs
  for individual routines can be read by saying `,describe subr-name`
  or `,d` for short.

  The `pseudo-csets.scm` file contains code for this stuff. Any routine
  that is `define-public` can be invoked at the guile prompt. Most are
  safe.  If its not `define-public`, you should not call it by hand.

  The `lang-learn-diary/disjunct-stats.scm` file contains ad-hoc code
  used to prepare the summary report.  To use it, just cut-n-paste to
  the guile prompt, to get it to do things.

  The marginal entropies and the mutual information between words and
  disjuncts can be computed in the same way that it's done for
  word-pairs:
```
  (define pca (make-pseudo-cset-api))
  (define psa (add-pair-stars pca))
  (batch-pairs psa)
```


Precomputed LXC containers
--------------------------
For your computing pleasure, there are some LXC containers that you
can download that have a fully-configured, functioning system on them.
The set of available containers will change over time.  The first one
actually has a bunch of bugs and other problems with it, but it's
enough to do some basic work. The steps are:

* Make sure you have some relatively modern Linux system handy.

* Install lxc on it. Like so:
```
  sudo apt-get install lxc
```

* Download a container from
  https://linas.org/lxc-nlp-containers/

* The following describe root-owned containers. You can also have
  user-owned containers, if you know how to do that.

* Go to the directory `/var/lib/lxc/lxc`.  You may need to create
  this directory.  Unpack the file you downloaded above, in this
  directory. Do this as root. Do NOT change the ownership of the
  files!  Avoid changing the datestamps on the files.

* Type in: `sudo lxc-ls -f` You should see a display similar to this:
```
NAME           STATE   AUTOSTART GROUPS IPV4      IPV6
simil-en       STOPPED 0         -      -         -
```

* Start the container by saying `lxc-start -n simil-en`. Give it
  5 or 10 or 30 seconds to boot up. Then `lxc-ls -f` again. You
  should see the below. It ay take a minute for the IPV4 address to
  show up. You will proably get a different IP than the one shown.
```
NAME           STATE   AUTOSTART GROUPS IPV4      IPV6
simil-en       RUNNING 0         -      10.0.3.89 -
```

* You can now `ssh` into this container, just as if it were some
  other machine on your network.  It more-or-less is another machine
  on your network.
```
  ssh ubuntu@10.0.3.89
```
  The password is `asdfasdf`.

* The run-scripts are in the `run` directory.  The opencog sources
  are in the `src` directory. You can `git pull; cmake..; make` these
  if you wish. Or not. Its all set up already.  Pull only if you need
  to get the latest.

  (For example, this README file, that you are reading, is in
  `/home/ubuntu/src/opencog/opencog/nlp/learn/README`. Right now.)

* You can now hop directly to the section "Exploring Connector-Sets"
  above, and just go for it.  Everything should be set and done.
  Well, you do need to start guile, of course, etc.



That's all for now!
-------------------
The next step is to cluster the pseudo-disjuncts into clusters, and
thereby extract grammar. There is no code for this, so far. You're
on your own.

Everything below this point consists of personal notes, of work-items
of stuff that still needs doing. You can ignore the rest of this file.
It will eventually get cleaned up.


TODO List
=========
* Count single-word probabilities.
* Resume work on morphology.
* Implement sql-delete.

DONE
----
* Make sure that link-parser fully randomizes linkage choices for
  long sentences. Done. See the rand_state and predictable_rand
  flags in version 4.8.3.
* Need a mutli-language sentence splitter; the maxent splitter only
  works for English sentences.  Probably something simple will do ...
  Done: copied one from moses-smt and put it in the run subdirectory
* Raw psql seems not to be utf8.  Were the tables created wrong,
  or is this a client bug?
  Fixed: turned out to be a guile-related bug.
* Handle database write asynchronously: have a thread collect
  up the atoms, and write them. Maybe even multiple threads.
  This is OK due to the atomptr design.
  Done.  Not just one, but multiple writers.
* Can we start a guile thread for each incoming sentence?
  Threading will require more subtle sentence cleanup.
  No. The main guile bug, opened 5 years ago, is still not fixed.
* Table counts need to be more than 32-bit. Looks like the
  any-language goes hog-wild and creates huge counts for just
  one sentence ... is this desired?  Anyway, they'll overflow...
  Done. atoms.sql now uses bigint 64-bit uuids.
* Investigate crash: while writing truuth value, stv_confidence
  was pure virtual (circa line 920 of AtomStorage.cc) There's some
  kind of smart-pointer race. See "Crash" section below.
  Done. Ran for a month without issues.
* Fix crash at linkage_set_domain_names+0xe0  Done. Need LG 5.4.0
* Create OO system for generic pair statistics. Done.
* Reduce num samples for 999 to 16. Done.
* Fix (WordNode "…!ANY-PUNCT") Done.
* Rework pipeline to use literary English sources. DONE.
  New scripts in the `download` directory.
* Prune low observation counts. Well, no that is a bad idea.
  However, the (opencog matrix) code now includes data filters.
* Create an LXC container for public use. DONE.
  But not updated.... Well no one is interested in thsi so punt.


Some typical entropies for word-pairs
-------------------------------------
Three experiments:
1) Get H(de,*) H(*,de)  H(en,*) H(*,en) and compare to
   H(de+en,*) H(*, de+en)

2) H(vieux,*) H(* vieux) H(nouveaux, *) H(*, nouveaux)

3) H(vieille, *) etc + vieux

4) H(le, *) H(la,*)  vs. H(le+la)

5) H(le,*) H(famille,*) which should fail ...!?



Some typical entropies for word-pairs
-------------------------------------
The below is arithmeticaly correct, but theoretically garbage.

(WordNode "famille") entropy H=11.185

H(*, famille) = 11.195548
H(famille, *) = 11.174561

MI(et, famille) = -5.2777815
H(et, *) = 5.5696678
P(et, *) = 0.021055372363972875

thus:
H(et, famille) = -MI(et, famille) + H(famille, *) + H(et, *) = 22.0220103
P(et, famille) = 2.348087815164205e-7

MI(de, famille) = 2.1422486
H(de, *) = 4.3749881
P(de, *) = 0.04819448582223504

H(de, famille) = -2.1422486 + 4.3749881 + 11.195548 = 13.4282875
P(de, famille) = 9.071574511509601e-5

P(de+et. *) = 0.06924985818620791
H(de+et, *) = 3.8520450730427047

P(de+et, famille) = 9.095055389661243e-5
H(de+et, famille) = 13.424558050397735
MI(de+et, famille) = 1.6230350226449701

So  MI(et, famille) < MI(de+et, famille) < MI(de, famille)
       -5.2777815   <   1.6230350226     <   2.1422486

By contrast, the arithmetic average is:
(MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille)) /
    (P(de, famille) + P(et, famille))
  = 2.1230921666199825

Change in entropy:
MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille) =  0.0012169

MI(de+et, famille) * P(de+et, famille) = 1.476159343e-4

Oh, wait ...
H(de, famille) * P(de, famille) + H(et, famille) * P(et, famille) =  0.001223328

H(de+et, famille) * P(de+et, famille) = 0.00122097099

Change in entropy = 0.00122097099 - 0.001223328 = -2.35701e-6

-------
H(de) = 4.3808608
H(et) = 5.5862331

P(de) = 0.04799870191172842
P(et) = 0.02081499323761464
P(de+et) = 0.06881369514934306
H(de+et) = 3.8611604742976153  = -log_2 (P(de)+P(et))
By contrast, the weighted average is

(P(de)*H(de) + P(et)*H(et)) /(P(de) + P(et)) = 4.745465784790553

Combinations:
  P(de+et)*H(de+et) = 0.2657007
  P(de)*H(de) + P(et)*H(et) = 0.32655303

The change in entropy, from forming a union, is:
  P(de+et)*H(de+et) - P(de)*H(de) - P(et)*H(et) = -0.060852316

Recap: Delta(de+et) = -0.060852316
       Delta(de+et, famille) = -2.35701e-6

Entropy increases (strongly) if word-pair merged, words are separated,

-------

MI(d'une, famille) = 5.230504
H(d'une, *) = 9.792551

H(la) = 5.6536283
H(la, *) = 5.5858526

sa
est
de
H(d'une) = 9.7960119
H(un) = 7.1578913
H(et) = 5.5862331

-----
repeat, for vielle+nouveaux
H(nouveaux) = 14.28815
P(nouveaux) = 4.998483553100357e-5

H(vieille) = 16.16037
P(vieille) = 1.365349e-5

P(nouveaux+vieille) = 6.363833e-5
H(nouveaux+vieille) = 13.93974
P(nouveaux+vieille)*H(nouveaux+vieille) = 8.87102088-4

P(nouveaux)*H(nouveaux) + P(vieille)*H(vieille) = 9.3483638e-4

Change in entropy is diff of the two: -4.7734297e-5


-----
repeat, for vielle+nouveaux
H(*, famille) = 11.195548

H(nouveaux, *) = 13.974219
P(nouveaux, *) = 6.213565989765264e-5
MI(nouveaux, famille) = 5.2966957
H(nouveaux, famille) = 19.8730713
P(nouveaux, famille) = 1.0413804797188067e-6

H(vieille, *) = 15.998603
P(vieille, *) = 1.5273571710064995e-5
MI(vieille, famille) = 10.195547
H(vieille, famille) = 16.998604
P(vieille, famille) = 7.636780561617735e-6

P(vieille+nouveaux, famille) = 8.678161041336542e-6
H(vieille+nouveaux, famille) = 16.814179210712517

P(vieille+nouveaux, *) = 7.740923160771763e-5
H(vieille+nouveaux, *) = 13.657134846045357

MI(vieille+nouveaux, famille) = 8.038503635332841

so MI(nouveaux, famille) < MI(vieille+nouveaux, famille) < MI(vieille, famille)
         5.2966957       <         8.038503635332841     <       10.195547

Change in entropy:
P(nouveaux, famille)*H(nouveaux, famille)  + P(vieille, famille)*H(vieille, famille)
    = 1.505100371e-4

P(vieille+nouveaux, famille) * H(vieille+nouveaux, famille) = 1.459161549e-4

Change = 1.459161549e-4 - 1.505100371e-4 = -4.5938821e-6

To recap: Delta(vieille+nouveaux) = -4.7734297e-5
               reduces the entropy more than
          Delta(vieille+nouveaux, famille) = -4.5938821e-6

i.e. entropy increses if the word-pairs are merged, the words are separated.


======================================================================
Minimal morphology output


;; Lets say that there was one word in the sentence, it was 'foobar'
;; and the splitter split it into foo and bar
;; then the following should be generated:

;; for each sentence, create one of these, each with a distinct uuid:
(ParseLink (stv 1 1)
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
   (SentenceNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9")
)

;; For each pair of morphemes, cereate the below:
(EvaluationLink (stv 1.0 1.0)
   (LinkGrammarRelationshipNode "MOR")
   (ListLink
      (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
      (WordInstanceNode "bar@cb2443bb-fbec-472c-baee-36b822579861")
   )
)

;; For each "word" aka morpheme, create these two clauses:
;; note that the UUID's match up exactly with the above.
;; the below shows only "foo", another pair is needed for "bar".
(ReferenceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (WordNode "foo")
)
(WordInstanceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
)


;; finally, at the very end:
;; again, the UUID must match with what was given above.

(ListLink (stv 1 1)
   (AnchorNode "# New Parsed Sentence")
   (SentenceNode "sentence@68e51cae-98bc-4102-b19c-78649c5f6cfb")
)

======================================================================
Tagalog status:

31 july 2014
4519631 = 4.5M morpehem pairs
204K morpehemes

======================================================================

Setup, July 2015
----------------
LXC container on gnucash.org

AtomSpace.cc line 303

LXC container on backlot
------------------------
nlp-base and nlp-server (currently used by rohit)
morf-server (currrently used by ainish)

LXC container on fanny
----------------------
ssh learn@10.0.3.182
cd src/learn
./run-all-servers.sh
tmux attach
psql en_pairs
