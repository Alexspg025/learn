#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary - Part Six
\end_layout

\begin_layout Date
Feb 2022 - present
\end_layout

\begin_layout Author
Linas Vepštas
\end_layout

\begin_layout Abstract
The language-learning effort involves research and software development
 to implement the ideas concerning unsupervised learning of grammar, syntax
 and semantics from corpora.
 This document contains supplementary notes and a loosely-organized semi-chronol
ogical diary of results.
 The notes here might not always makes sense; they are a short-hand for
 my own benefit, rather than aimed at you, dear reader!
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Part Six of the diary on the language-learning effort focuses on weaving
 together theory with experimental results.
 It is a bit anachronistic with the other diaries, as it explores possible
 theoretical frameworks and tools for earlier experimental results.
\end_layout

\begin_layout Section*
Summary Conclusions
\end_layout

\begin_layout Standard
A summary of what is found in this part of the diary:
\end_layout

\begin_layout Itemize
No summary yet.
\end_layout

\begin_layout Section*
Mutual Information as Laplacian
\end_layout

\begin_layout Standard
This is a random half-baked suggestive idea that dead-ends immediately.
\end_layout

\begin_layout Standard
The discrete Laplacian on a 1-dimensional grid is a tri-diagonal matrix
 of the form
\begin_inset Formula 
\[
\Delta x_{i}=2x_{i}-x_{i-1}-x_{i+1}
\]

\end_inset

The pair-wise mutual information 
\begin_inset Formula 
\[
MI\left(u,v\right)=\log_{2}p\left(u,v\right)-\log_{2}p\left(u,*\right)-\log_{2}p\left(*,v\right)
\]

\end_inset

Inserting a factor of 1/2 gives the ranked-MI
\begin_inset Formula 
\[
MI_{\mbox{ranked}}\left(u,v\right)=\log_{2}\frac{p\left(u,v\right)}{\sqrt{p\left(u,*\right)p\left(*,v\right)}}
\]

\end_inset

These two somehow feel 
\begin_inset Quotes eld
\end_inset

similar
\begin_inset Quotes erd
\end_inset

 but I cannot connect the dots.
\end_layout

\begin_layout Standard
The MI 
\begin_inset Quotes eld
\end_inset

feels like
\begin_inset Quotes erd
\end_inset

 some kind of re-normalized propagator where the 
\begin_inset Formula $\log_{2}p\left(u,*\right)$
\end_inset

 feel like 
\begin_inset Quotes eld
\end_inset

vacuum contributions
\begin_inset Quotes erd
\end_inset

, but the details remain opaque.
\end_layout

\begin_layout Section*
Field Theory Models
\end_layout

\begin_layout Standard
Field theory models applied to statistics and language have surely been
 thrashed to death in the literature (of which I am only dimly aware).
 The below is an attempted recap of some basic ideas, recast into the notation
 used locally in this diary.
\end_layout

\begin_layout Subsection*
Density of States
\end_layout

\begin_layout Standard
Starting point is a discrete linear lattice of words in a sentence.
 Associated to each sentence is a probability 
\begin_inset Formula $p\left(w_{1},\cdots,w_{n}\right)$
\end_inset

 for words 
\begin_inset Formula $w_{k}$
\end_inset

 and a sentence of length 
\begin_inset Formula $n$
\end_inset

.
 We do not know that probability; we just assume it exists 
\emph on
a priori
\emph default
.
 We can make experimental pair-wise observations of word-pairs as 
\begin_inset Formula $\left(*,*,\cdots,*,w_{i},*,\cdots,*,w_{j},*,\cdots,*\right)$
\end_inset

 of pairs of words 
\begin_inset Formula $\left(w_{i},w_{j}\right)$
\end_inset

 within the full sentence.
 Note the former is a cylinder set, i.e.
 an element of the product topology on strings.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\sigma=\left(w_{1},\cdots,w_{n}\right)$
\end_inset

 be a string (the sentence).
 Define the energy of a string as 
\begin_inset Formula $E\left(\sigma\right)=-\log_{2}p\left(\sigma\right)$
\end_inset

 and define the energy density as 
\begin_inset Formula 
\begin{align*}
\rho\left(E\right)= & \sum_{\sigma}\delta\left(E-E\left(\sigma\right)\right)\\
=C & \sum_{w_{i},w_{j}}\delta\left(E+\log_{2}p\left(w_{i},w_{j}\right)\right)
\end{align*}

\end_inset

where 
\begin_inset Formula $\delta\left(x\right)$
\end_inset

 is the Dirac delta function (in principle) or just a finite–width, but
 thin Gaussian in practice, or, more plainly, just a box filter, so that
 we can do histogram counting.
 The constant 
\begin_inset Formula $C$
\end_inset

 appears because the sum over pairs is a multiple of the sum over all states;
 it over-counts.
 A formal derivation of the value of 
\begin_inset Formula $C$
\end_inset

 from first principles seems tedious and unenlightening.
 Not to worry, we can force it experimentally simply by requiring that 
\begin_inset Formula 
\[
\int\rho\left(E\right)dE=1
\]

\end_inset

I honestly do not recall if any of the prior diary entries ever supplied
 a graph of 
\begin_inset Formula $\rho\left(E\right)$
\end_inset

.
 Better late than never?
\end_layout

\begin_layout Subsection*
Interpretation
\end_layout

\begin_layout Standard
The above definition of the density of states is motivated by the Boltzmann
 distribution 
\begin_inset Formula $p=e^{-\beta E}$
\end_inset

.
 Taking the log of both sides and setting 
\begin_inset Formula $\beta=1$
\end_inset

 gives 
\begin_inset Formula $E=-\log p$
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Boltzmann distribution"
target "https://en.wikipedia.org/wiki/Boltzmann_distribution"
literal "false"

\end_inset

 for additional details.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Density of States - Experimental Result
\end_layout

\begin_layout Standard
Working with the Run-1 dataset `
\family sans
run-1-en_pairs-tranche-123.rdb
\family default
`.
 This dataset is characterized in the subsection below.
 To generate the histogram, simply create N bins, and increment by one whenever
 
\begin_inset Formula $-\log_{2}p\left(w_{i},w_{j}\right)$
\end_inset

 lies within the bin boundaries.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Use the script in `utils/densidty-of-states.scm`
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
The graph below uses 200 bins, running between a lower bound of 7.0 and upper
 bound of 30.0.
 Thus, the width of each bin is 
\begin_inset Formula $dh=23/200$
\end_inset

.
 The data is as marked, and, to provide a sense of scale, the line 
\begin_inset Formula $2^{E-30}$
\end_inset

 is graphed.
 Note that there is a scattering of dots at the upper-right and lower left.
 Dots correspond to non-empty bins in the histogram, with empty neighbors.
 These dots have a special significance.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/density.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
This graph can be understood as a kind-of upside-down Zipfian distribution.
 The scatter of dots at the top-right are the pairs that were seen only
 a handful of times.
 The topmost, rightmost dot corresponds to the word-pairs that were observed
 only once, and thus have a very high 
\begin_inset Formula $E$
\end_inset

.
 Specifically, 
\begin_inset Formula $E_{1}=\log_{2}985483375\approx29.8763$
\end_inset

 for this point, as there was a grand total of 985 million pairs observed.
 The density here is a Dirac delta spike, since there were 9215082 distinct,
 unique word-pairs observed exactly once; thus 
\begin_inset Formula $\rho\left(E_{1}\right)$
\end_inset

 is normalized to 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
985483375/
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
9215082 / dh.
 The next dots correspond to the number of distinct word-pairs that were
 observed only twice, then three times, etc.
 until they run together into common bins in the histogram.
 The dots at the bottom-left correspond to word-pairs there are extremely
 common (typically involving the words 
\begin_inset Quotes eld
\end_inset

the
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

a
\begin_inset Quotes erd
\end_inset

, punctuation.) These would be ranked first in a Zipfian distribution, thus
 the bottom-left of this graph corresponds to the top-left of a Zipf graph.)
\end_layout

\begin_layout Standard
Note that if the counts in the right-most bins are smeared, so that they
 are not delta functions, but smooth, then the right side of the graph would
 twist down sharply.
 It appears that it could be approximated by 
\begin_inset Formula $(30-E)2^{E-30}$
\end_inset

.
 Not shown; it would be nice to show this graph.
\end_layout

\begin_layout Standard
For comparison, below left is the conventional Zipf distribution graph,
 and, on the right, the same graph flipped along the diagonal, together
 with density of states from above.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/pair-rank.eps
	width 48col%

\end_inset


\begin_inset Graphics
	filename p6-density/pair-rank-flip.eps
	width 48col%

\end_inset


\end_layout

\begin_layout Subsubsection*
Under-sampling
\end_layout

\begin_layout Standard
The humpback shape appears to be due to an under-sampling effect.
 This is exposed and explained in the next few sections.
 Due to a finite sample size, it appears that the only pairs that are sufficient
ly sampled are those up to a rank of about 1200.
 After that, pairs are under-sampled.
 The result of that under-sampling is a humpback shape, as seen above; the
 top of the hump is where the undersampling begins.
 This suggests that the eyeballed fit is incorrect, and that the Rank distributi
on shoud be considered only up to 1200.
 This is shown below.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/pair-rank-cut.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
This time, the slope is different: it is 0.75, which is, umm, err, I guess
 its a 
\begin_inset Quotes eld
\end_inset

small world
\begin_inset Quotes erd
\end_inset

 slope.
 This is no longer the canonical Zipf slope of 1.0.
 This raises the question: how many of the graphs in the earlier parts of
 the diary are compromised, as being mixtures of under-sampling and 
\begin_inset Quotes eld
\end_inset

actual effects
\begin_inset Quotes erd
\end_inset

? This also raises the question: aren't all learning effects always driven
 by an under-sampling? That is, isn't one always doomed to under-sample?
 How can one know this, and how can one take this into account? 
\end_layout

\begin_layout Subsubsection*
Top-ten Word Pairs
\end_layout

\begin_layout Standard
Given the above discussion about under-sampling, it is hard to avoid noticing
 that the top-ten word-pairs appear to follow an eve flatter slope.
 What does this mean? Clearly, they are not undersampled, and so the flatter
 slope nees to have some more sophisticated explanation.
 
\end_layout

\begin_layout Standard
The top-ten most frequently observed word-pairs are shown in the table below:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Rank
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pair (word <<> word)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
4765096
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
, <<>> and
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4254477
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
###LEFT-WALL### <<>> ,
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3714944
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
###LEFT-WALL### <<>> .
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3705739
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
of <<>> the
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3141824
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
- <<>> -
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2951005
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
###LEFT-WALL### <<>> the
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2603823
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
, <<>> the
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2177390
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
the <<>> of
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1926906
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
in <<>> the
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1915335
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
, <<>> ,
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

more sophisticated
\begin_inset Quotes erd
\end_inset

 explanation might be this: this is a finite-sentence-length effect.
 That is, the nature of human understanding is that we have a limited attention
 span, and a limited short-term memory.
 Sentence lengths, and the use of punctuation must accomadate these limits.
 Thus, sentence starters and sentence enders, and commas, for phrase identificat
ion, sould appear at a constant rate, rather than at a Zipfian rate.
 And that is indeed what the above seems to confirm.
 OK, so that's an interesting discovery, its new to me.
 
\end_layout

\begin_layout Subsection*
Physical Interpretation
\end_layout

\begin_layout Standard
Clearly, this graph suggests that the total energy
\begin_inset Formula 
\[
\int E\rho\left(E\right)dE
\]

\end_inset

is unbounded.
 Of course, it is finite for this particular dataset, but the trend suggests
 that if a trillion word-pairs were observed, then the high-end of the graph
 would be at 
\begin_inset Formula $E=40$
\end_inset

 instead of at 
\begin_inset Formula $E=30$
\end_inset

.
 Thus, this is not a 
\begin_inset Quotes eld
\end_inset

physical
\begin_inset Quotes erd
\end_inset

 system of finite energy, in the conventional sense.
 
\end_layout

\begin_layout Standard
The root cause of this is that the vocabulary is unbounded.
 As more and more text is observed, more and more vocabulary words are encounter
ed, and there appears to be no upper limit (geographical place-names, foreign
 loan-words, given names, imaginative sales terms, children's nonsense words,
 portmanteaus, ...) As a result, the number of distinct word pairs also grows,
 in an unbounded fashion, as the number of observations increase.
 
\end_layout

\begin_layout Standard
Thus we take the size of the vocabulary to be countable infinity and denote
 it as 
\begin_inset Formula $\mathbb{N}$
\end_inset

 the natural numbers.
 The space of all strings (sentences) of length 
\begin_inset Formula $n$
\end_inset

 is then the Cartesian product 
\begin_inset Formula 
\[
\mathbb{N}\times\cdots\times\mathbb{N}=\mathbb{N}^{n}
\]

\end_inset

It would be nice to be able to assign a measure 
\begin_inset Formula $\mu$
\end_inset

 to this space, but even this is problematic.
 Consider the cylinder set 
\begin_inset Formula $\left(*,\cdots,*,w_{k},*,\cdots,*\right)$
\end_inset

 of a word 
\begin_inset Formula $w_{k}$
\end_inset

 at location 
\begin_inset Formula $j$
\end_inset

 in the middle of all sentences of length 
\begin_inset Formula $n$
\end_inset

.
 Denote the probability as 
\begin_inset Formula $\mu_{j}\left(w_{k}\right)$
\end_inset

.
 For English, and for many languages, the probability of observing a word
 is mostly independent of it's location in the sentence, so drop the subscript
 
\begin_inset Formula $j$
\end_inset

 and just write 
\begin_inset Formula $\mu\left(w_{k}\right)$
\end_inset

 as the probability of observing a word (or just define 
\begin_inset Formula $\mu\left(w_{k}\right)$
\end_inset

 as the average over 
\begin_inset Formula $j$
\end_inset

.) This is the measure of the cylinder set 
\begin_inset Formula $\left(*,\cdots,*,w_{k},*,\cdots,*\right)$
\end_inset

.
\end_layout

\begin_layout Standard
For this to be a proper probability, we expect that we should be able to
 write 
\begin_inset Formula 
\[
1=\sum_{k}\mu\left(w_{k}\right)
\]

\end_inset

which is an eminently desirable property of any measure.
 But we are not so lucky: the distribution of words is Zipfian, with exponent
 1, and so this sum is logarithmically divergent.
 That is, the Zipfian distribution of individual words tells us that 
\begin_inset Formula 
\[
\mu\left(w_{k}\right)\approx\frac{1}{k^{s}}
\]

\end_inset

for exponent 
\begin_inset Formula $s$
\end_inset

, and experimentally, it is well-known that for natural language, 
\begin_inset Formula $s\approx1$
\end_inset

 and is typically measured to be 1.01 to 1.05 for datasets with millions or
 billions of words.
 For the infinite vocabulary 
\begin_inset Formula $\mathbb{N}$
\end_inset

, we are forced to take the value of 
\begin_inset Formula $s=1$
\end_inset

 and thus end up with a topological space without a conventional measure
 on it.
\end_layout

\begin_layout Standard
The only good news here is that it is only weakly divergent.
\end_layout

\begin_layout Standard
Except this is wrong.
 Based on the revised results, taking into account the limited sample size
 (as discussed above, and further below) we have to conclude that in the
 limit of large sample size, that 
\begin_inset Formula $s\approx0.75$
\end_inset

.
 In addition to this, sentences are not unbounded in length (German philosophers
 proving the rule) and so the actual normalization requirement is 
\begin_inset Formula 
\[
1=\sum_{k<N}\mu\left(w_{k}\right)
\]

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is an upper bound on 
\begin_inset Quotes eld
\end_inset

realistic
\begin_inset Quotes erd
\end_inset

 sentence lengths.
 
\end_layout

\begin_layout Standard
What all this points at is the lack of a theory that takes into account
 limited sample sizes, as well as taking into account human cognitive effects
 such as finite sentence lengths, driven by attention span and the limits
 of short-term memory.
 Developing such a theory appears to require considerable effort.
 
\end_layout

\begin_layout Standard
The above density-of-states results for word-pairs indicates that the same
 applies for 
\begin_inset Formula $\mu\left(w_{i},w_{j}\right)$
\end_inset

.
 This is the same as 
\begin_inset Formula $p\left(w_{i},w_{j}\right)$
\end_inset

, we're just bouncing around in notation, so that 
\begin_inset Formula $\mu$
\end_inset

 is the formal measure on the Cartesian product space, for given cylinder
 sets, while 
\begin_inset Formula $p$
\end_inset

 is the experimentally observed frequentist probability (the Bayesian probabilit
y with the trivial prior.)
\end_layout

\begin_layout Subsubsection*
Dataset Notes
\end_layout

\begin_layout Standard
This part is too big to fit in a footnote, so I put it here.
 At first, attempted to work with the Run-1 dataset `
\family sans
run-1-marg-tranche-1234.rdb
\family default
` which contains the marginals.
 This dataset is painfully large, taking too long to load.
 It was using 50 GB and swapping like mad after loading 29M of the total
 38M pairs.
 Ouch.
 Try again with `
\family sans
run-1-en_pairs-tranche-123.rdb
\family default
` which should not have any marginals, just the raw counts ...
 Hopefully, skipping the marginals takes less time to load (?).
 The dataset stats appear in Diary Part Two, at the very end.
\end_layout

\begin_layout Standard
Config files are in `
\family sans
Experiment-13
\family default
`.
 Don't really need the marginals, to graph the density of states.
\end_layout

\begin_layout Standard
Dataset summary:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Property
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Value
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Filename
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family sans
run-1-en_pairs-tranche-123.rdb
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dimensions
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
304085 
\begin_inset Formula $\times$
\end_inset

 306920
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Num Pairs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
28184319
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\log_{2}$
\end_inset

 Num Pairs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24.7484
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Total Count
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
985483375
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\log_{2}$
\end_inset

 Total Count
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
29.8763
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RAM Usage to Load
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
49.7 GB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RAM Usage to Run
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
61.3 GB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Entropy Total
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
18.378
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The above dataset summary agrees with what is reported at the very end of
 Diary Part Two (
\emph on
i.e.

\emph default
 during load, the same numbers are reported.) The 
\begin_inset Quotes eld
\end_inset

Entropy Total
\begin_inset Quotes erd
\end_inset

 is the same as defined in earlier diaries: 
\begin_inset Formula 
\[
\mbox{Entropy Total}=-\sum_{i,j}p\left(w_{i},w_{j}\right)\log_{2}p\left(w_{i},w_{j}\right)
\]

\end_inset

 
\end_layout

\begin_layout Subsection*
Other Densities
\end_layout

\begin_layout Standard
Well, OK, so we've done Zipf graphs of all kinds before, but somewhat haphazardl
y.
 It's worth regoing these as densities: i.e.
 bin-counting, with the horizontal axis being 
\begin_inset Formula $E=-\log_{2}p\left(w_{i},w_{j}\right)$
\end_inset

 and the vertical axis being other assorted quantities.
 All of these graphs will suffer from the under-sampling issues described
 above.
 We don't yet have a good theoretical foundation to deal with the under-sampling.
 So damn the torpedoes, full speed ahead.
\end_layout

\begin_layout Standard
Suitable quantities to plot: 
\end_layout

\begin_layout Itemize
The pair MI 
\end_layout

\begin_layout Itemize
The left and right marginal probabilities 
\begin_inset Formula $p\left(w_{i},*\right)$
\end_inset

 and 
\begin_inset Formula $p\left(*,w_{j}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
The log left marginal probability 
\begin_inset Formula $-\log_{2}p\left(w_{i},*\right)$
\end_inset

 
\end_layout

\begin_layout Itemize
The left fractional marginal entropy 
\begin_inset Formula $-\frac{1}{p\left(w,*\right)}\sum_{v}p\left(w,v\right)\log_{2}p\left(w,v\right)$
\end_inset


\end_layout

\begin_layout Itemize
The left fractional marginal MI
\end_layout

\begin_layout Section*
Word-Pair Vertex Degree Distributions
\end_layout

\begin_layout Standard
Oddly enough, I never really characterized the word-pair sets using more
 conventional graph-theoretic concepts.
 Time to make amends.
 The collection of word-pairs can be taken to define a set of edges, thus
 defining a graph.
 This is a directed graph, but I think this doesn't matter, so will mostly
 pretend it is undirected.
 The collection of verteces will be taken as the left-element of each pair.
 The edges will be the pairs going from left to right.
\end_layout

\begin_layout Standard
The (out-)degree of a vertex is just the number of edges leaving it.
 We work with out-degrees exclusively, excpet for a few spot-checks.
 Basically, the word-pair graph should be approximately symmetric, so there
 should not be much of a difference in distributions.
\end_layout

\begin_layout Standard
The results of this section:
\end_layout

\begin_layout Itemize
The scaling of frequency vs.
 degree goes with a power law of 
\begin_inset Formula $\gamma\approx1.6$
\end_inset


\end_layout

\begin_layout Itemize
There is an interesting sample-size effect, which prevents naive scaling
 of histogram bin-widths!
\end_layout

\begin_layout Itemize
There is no theory to guide one through the sample-size effect, and it is
 clearly pervasive, affecting pretty much every graph ever drawn, ever,
 in this diary.
 It's a foundational effect, that cannot be escaped.
 Its inherent in this kind of data.
\end_layout

\begin_layout Itemize
One can graphs all sorts of quantities as functions of the vertex degree.
 Does not seem to reveal anything noteworthy.
\end_layout

\begin_layout Subsection*
Vertex Degree
\end_layout

\begin_layout Standard
The first conventional question is 
\begin_inset Quotes eld
\end_inset

what is the vertex degree distribution?
\begin_inset Quotes erd
\end_inset

 This is shown below, a graph of the normalized frequency of a word, and
 it's out-degree.
 The degree ranges as high as 300K with significant counts up to 50K.
 The graph only shows degree up to 1200.
 There are 1200 bins in this graph, so each different degree gets it's own
 bin.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/degree-fine.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
The eyeballed fit has 
\begin_inset Formula $\mbox{frequency}=\left(1.1/\mbox{degree}\right)^{1.6}$
\end_inset

 and so that exponent is well below typical scale-free networks.
 The raw counts are shown on the right y-axis,
\emph on
 i.e.

\emph default
 un-normalized.
 The point of drawing it this way is that we see on the right where the
 count drops to one (and to zero).
\end_layout

\begin_layout Standard
Something unexpected happens if we go deeper.
 There are so few counts, that it seems like it should be reasonable to
 bin them together.
 The graph below shows degree out to 50K, collected into 200 bins.
 Thus, each bin is 250 degrees wide.
 The slope is remarkably different:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/degree.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
I think this is purely an under-sampling effect.
 In principle, the slope should not have changed.
 Here's a quick sketch.
 If the original distribution is 
\begin_inset Formula 
\[
f_{n}=\left(\frac{a}{n}\right)^{\gamma}
\]

\end_inset

then a re-binning into constant-sized bins of size 
\begin_inset Formula $k$
\end_inset

 is given by 
\begin_inset Formula 
\begin{align*}
g_{m}= & \sum_{m\left(k-1\right)<n\le mk}f_{n}\\
= & \left(\frac{a}{k\left(m-1\right)+1}\right)^{\gamma}+\cdots+\left(\frac{a}{k\left(m-1\right)+k}\right)^{\gamma}\\
= & \left(\frac{a}{k\left(m-1\right)}\right)^{\gamma}\left[\frac{1}{\left(1+\frac{1}{k\left(m-1\right)}\right)^{\gamma}}+\frac{1}{\left(1+\frac{2}{k\left(m-1\right)}\right)^{\gamma}}+\cdots+\frac{1}{\left(1+\frac{k}{k\left(m-1\right)}\right)^{\gamma}}\right]\\
\approx & \left(\frac{a}{k\left(m-1\right)}\right)^{\gamma}\left[k-\frac{\gamma}{k\left(m-1\right)}\left[1+2+\cdots+k\right]\right]\\
\approx & \left(\frac{a}{k\left(m-1\right)}\right)^{\gamma}k\left[1-\frac{\gamma}{2\left(m-1\right)}\right]\\
\approx & Cm^{-\gamma}
\end{align*}

\end_inset

where the approximations 
\begin_inset Formula $k\gg1,m\gg1$
\end_inset

 are made.
 That is, there is a change in the overall normalization, and the early
 part of the slope, for small 
\begin_inset Formula $m$
\end_inset

 is reduced, but the overall exponent is not affected.
 Yet this is ggiven lie to by the figure above.
 So what goes wrong? I think it is an under-sampling effect.
 For large 
\begin_inset Formula $n$
\end_inset

, the 
\begin_inset Formula $f_{n}$
\end_inset

are not as given above, but are zero.
 The fraction of the time that they are zero is determined by the sample
 size, and they are zero often enough that the overall slope is changed.
 The net efect of sample size could be computed; it does not seem to be
 a worthwhile exercise just right now.
\end_layout

\begin_layout Standard
Let's repeat the calculation above with an explicit log scale.
 This gives
\begin_inset Formula 
\begin{align*}
g_{m}= & \sum_{m\le\log_{2}n<m+1}GraphPropertiesf_{n}\\
= & \sum_{2^{m}\le n<2^{m}}\left(\frac{a}{n}\right)^{\gamma}\\
= & a^{\gamma}\sum_{1\le j<2^{m}}\frac{1}{\left(2^{m}+j\right)^{\gamma}}\\
\approx & \frac{a^{\gamma}}{2^{m\gamma}}\cdot2^{m}\left(1-\frac{\gamma}{2}\right)\\
= & C2^{m\left(1-\gamma\right)}
\end{align*}

\end_inset

Thus, since we saw 
\begin_inset Formula $\gamma\approx1.6$
\end_inset

 in the earliest figure, we expect a slope of 
\begin_inset Formula $\gamma=1\approx0.6$
\end_inset

 in the equivalent log figure.
 This is shown below.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/log2-degree.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
That initial slop is validup to 
\begin_inset Formula $n\approx1200$
\end_inset

 or 
\begin_inset Formula $\log-\frac{1}{p\left(w,*\right)}\sum_{v}p\left(w,v\right)\log_{2}p\left(w,v\right)_{2}n\approx10$
\end_inset

, after which a sharper slope sets in due to the finite-sample-size effect.
 This gives the figure an overall hump-back shape.
 
\end_layout

\begin_layout Subsection*
Weighted Vertex Degree
\end_layout

\begin_layout Standard
Same as above, but showing te weighted vertex degree, i.e.
 
\begin_inset Quotes eld
\end_inset

with multiplicity
\begin_inset Quotes erd
\end_inset

.
 That is, if each edge was observed 
\begin_inset Formula $N$
\end_inset

 times, then it is treated as if there were 
\begin_inset Formula $N$
\end_inset

 distinct edges.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/degree-m-fine-raw.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
There is a distinct oscillatory behavior at the center-left.
 It is perhaps some strange artifact, having to do with the fact that 24
 parse-trees are sampled, given that the first, most prominant peak occurs
 at 24, and later peaks are perhaps at multiples of 24.
\end_layout

\begin_layout Subsection*
Other weighting schemes
\end_layout

\begin_layout Standard
The following sequence of graphs use weighting schemes that are perhaps
 difficult to understand, but seem worth exploring.
 Thier physical interpretation is challenging and the significance is unclear.
 Do it anyway, just to say we covered all the bases.
\end_layout

\begin_layout Standard
In all of these, the horizontal axis shows the edge degree of the word,
 without multiplicity, so, the number of unique word-pairs that a word participa
tes in.
 The y-axis, however, uses weighted counting, with different weights.
 That is, if a word has an edge degree of 42, then instead of counting it
 exactly once, it is counted with a weight (mass) 
\begin_inset Formula $m\ne1$
\end_inset

.
 Graphs are shown for 
\end_layout

\begin_layout Itemize
Mass 
\begin_inset Formula $m=p\left(w,*\right)$
\end_inset

 the right marginal probability for word 
\begin_inset Formula $w$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Mass 
\begin_inset Formula $m=p\left(*,w\right)$
\end_inset

 the left marginal probability for word 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Itemize
Mass 
\begin_inset Formula $m=\log_{2}p\left(w,*\right)$
\end_inset

 the right marginal log-probability for word 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Itemize
Mass 
\begin_inset Formula $m=\sum_{v}p\left(w,v\right)\log_{2}p\left(w,v\right)$
\end_inset

 the right marginal entropy for word 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Itemize
Mass 
\begin_inset Formula $m=MI\left(w,*\right)$
\end_inset

 the right marginal MI for word 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Weighted by Marginal Probability
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/degree-p-fine.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
The vertical axis totals the marginal probability for that word.
 That is, instead of adding 1 for each observed edge (the support), it adds
 
\begin_inset Formula $p\left(w,*\right)$
\end_inset

, the left-marginal probability, or 
\begin_inset Formula $p\left(*,w\right)$
\end_inset

, the right marginal probability.
 So, for example, consider a vertex of degree 5.
 There might be 10K such vertexes in this dataset.
 However, each such vertex might be observed 30 or 50 times, and so (for
 left-marginal counts) we would see 300K or 500K on the y-axis here.
 
\end_layout

\begin_layout Subsubsection*
Weighted by Marginal Log Probability is nearly identical to the above; it
 is shifted ever so slightly upwards.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/degree-rlogp-fine.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
The vertical axis totals the log marginal probability for that word.
 The weight is 
\begin_inset Formula $-\log_{2}p\left(w,*\right)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Weighted by Marginal Entropy
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/degree-rfent-fine.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
The vertical axis totals the marginal fractional entropy for that word.
 The weight is 
\begin_inset Formula 
\[
-\frac{1}{p\left(w,*\right)}\sum_{v}p\left(w,v\right)\log_{2}p\left(w,v\right)
\]

\end_inset

Clearly, this graph is nearly identical to the above; it is shifted ever
 so slightly upwards.
\end_layout

\begin_layout Subsubsection*
Weighted by Marginal MI
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/degree-rfmi-fine.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
The vertical axis totals the marginal fractional MI for that word.
 The weight is 
\begin_inset Formula 
\[
\frac{1}{p\left(w,*\right)}\sum_{v}p\left(w,v\right)\log_{2}\frac{p\left(w,v\right)}{p\left(w,*\right)p\left(*,v\right)}
\]

\end_inset

This graph has the same general shape as the earlier ones, but has a distinctly
 different slope.
\end_layout

\begin_layout Section*
Word-Pair MI Distribution
\end_layout

\begin_layout Standard
We've graphed the MI distribution many times before.
 Notably, the 
\begin_inset Quotes eld
\end_inset

Word-Pair Distributions
\begin_inset Quotes erd
\end_inset

 document details these.
 But since we're on a roll here, lets redo it with the same dataset as all
 the other graphs.
 
\end_layout

\begin_layout Standard
The big new news for this section is that I think I finally understand the
 nature of this distribution.
 It is ....
\end_layout

\begin_layout Standard
The distribution of word-pair MI is shown below.
 As before, this is for dataset 3, which contains 28 million pairs.
 The MI is sorted into 500 histogram bins.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/pair-fmi.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
The distribution is clearly not symmetric.
 The two sides appear to be bounded by straight lines, with slopes as in
 the legend.
 Pairs with the highest MI are observed very infrequently.
\end_layout

\begin_layout Standard
Here's the same data, but with a different fit: 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/pair-fmi-signal.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
Why this shape? Here's a guess.
 If word-pairs are chosen completely at random, and the number of sampled
 pairs is much smaller than the total possible pairs, then one obtains a
 Gaussian distribution.
 Such a distribution is centered on a small but positive MI, due to sample-size
 effects.
 For larger samples, the mean tends to zero.
 Thus, perhaps the left-hand-side of this figure is just a Gaussian.
 
\end_layout

\begin_layout Standard
Taking this to be 
\begin_inset Quotes eld
\end_inset

common-mode noise
\begin_inset Quotes erd
\end_inset

, and subtracting it leaves an excess of word pairs with positive MI, having
 a peak near 
\begin_inset Formula $MI\sim4$
\end_inset

.
 The straight-line slope on the right suggests that the excess can be described
 by a log-normal distribution.
 Again, an eye-balled, imprecise fit is shown.
 These two, summed together, model the observed distribution almost perfectly.
 Perhaps a formal expression for the common-mode noise is easily derived,
 given a fixed vocabulary size and number of samples.
 Attempt to get this is made further below.
\end_layout

\begin_layout Standard
Theories of why the remainder would be a log-normal distribution are unknown
 to the author.
\end_layout

\begin_layout Standard
Pairs with the highest MI are observed very infrequently.
 The highest observable MI value is directly related to the sample size:
 it is a bit below the log of the number of observations.
 Thus, the sharp drop on the right side is purely a sample-size effect.
 Trimming does not appreciably change the sahpe of this distribution, other
 than to eliminate the very highest MI values.
\end_layout

\begin_layout Standard
This distribution is not language-specific; a nearly identical distribution
 is seen for Chinese Mandarin Hanzi pairs.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See 
\begin_inset Quotes eld
\end_inset

Word-Pair Distributions
\begin_inset Quotes erd
\end_inset

, page 18.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Ranked-MI Distribution
\end_layout

\begin_layout Standard
Just for giggles, here's the ranked-MI distribution:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p6-density/pair-rmi.eps
	width 60col%

\end_inset


\end_layout

\begin_layout Standard
Recall the ranked-MI is defined as
\begin_inset Formula 
\[
MI_{\mbox{ranked}}\left(w,v\right)=\log_{2}\frac{p\left(w,v\right)}{\sqrt{p\left(w,*\right)p\left(*,v\right)}}
\]

\end_inset

Superimposed on this graph is the distribution of the regular MI, multiplied
 by 1.5 to get the width correct, and shifted by 10 to the left, to get te
 zero correct.
 Apparently, ranked-MI does not alter the distribution.
 I wonder what it does, if it were used for MST/MPG parsing ...
\end_layout

\begin_layout Standard
Oh, hang on.
 the Ranked-MI is just 1/2 of the 
\begin_inset Quotes eld
\end_inset

variation of information
\begin_inset Quotes erd
\end_inset

, see Diary Part Two, page 54.
 Oh huh.
 OK, so I have to go back into the diary, and amend all of the entries to
 reflect this conventional name.
 Yow!
\end_layout

\begin_layout Subsection*
Neutral MI Distribution
\end_layout

\begin_layout Standard
Attempted theoretical calculation of the MI distribution that would result
 if word pairs wer chosen at completely at random.
 There are two distributions of interest.
 One uses a uniform distribution of the vocabulary words, the other a Zipfian
 distribution.
 
\end_layout

\begin_layout Standard
XXX FIXME Everything below is incomplete, incorrect, wrong.
 I'm too lazy to figure out why.
\end_layout

\begin_layout Subsubsection*
Uniform distribution
\end_layout

\begin_layout Standard
Assume a vocabulary size of 
\begin_inset Formula $N$
\end_inset

.
 A random word-pair consists of two random, uniformly-weighted draws from
 this vocabulary.
 We take the order of the draws as being important; thus, any given word-pair
 has a chance of 
\begin_inset Formula $1/N^{2}$
\end_inset

 of being drawn.
 Consider 
\begin_inset Formula $M$
\end_inset

 pair draws, with 
\begin_inset Formula $N\ll M\ll N^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The chance of a given pair being drawn once is 
\begin_inset Formula $CM/N^{2}$
\end_inset

 with 
\begin_inset Formula $C$
\end_inset

 a normalization constant to be determined.
 Basically, it can be drawn the first time, or the second time, or the third
 time ...
 etc.
 but never twice.
 The chance of it being drawn twice is 
\begin_inset Formula $CM\left(M-1\right)/2N^{4}$
\end_inset

 and so now we have the usual combinatorics.
 The chance of observing a word-pair 
\begin_inset Formula $\left(a,b\right)$
\end_inset

 a total of 
\begin_inset Formula $K$
\end_inset

 times is
\begin_inset Formula 
\[
p\left(K\vert a,b\right)=\frac{C}{N^{2K}}{M \choose K}
\]

\end_inset

Right(?) 
\end_layout

\begin_layout Subsubsection*
Zipfian distribution
\end_layout

\begin_layout Standard
Repeat the above, with a non-uniform distribution.
 Each word is distingusied by it's ordinal 
\begin_inset Formula $k$
\end_inset

 so that we have words 
\begin_inset Formula $w_{k}$
\end_inset

 for 
\begin_inset Formula $1\le k\le N$
\end_inset

.
 The probability of drawing word 
\begin_inset Formula $w_{k}$
\end_inset

 is then 
\begin_inset Formula $p\left(w_{k}\right)=Ak^{-\gamma}$
\end_inset

 for 
\begin_inset Formula $\gamma\approx1$
\end_inset

 and 
\begin_inset Formula $A$
\end_inset

 a normalization constant, so that 
\begin_inset Formula $1=\sum_{k=1}^{\infty}p\left(w_{k}\right)$
\end_inset

.
 The probability of drawing a pair 
\begin_inset Formula $\left(w_{i},w_{j}\right)$
\end_inset

 is then 
\begin_inset Formula $p\left(w_{i},w_{j}\right)=p\left(w_{i}\right)p\left(w_{j}\right)$
\end_inset

 since the probabilities are completely independent of one-another.
\end_layout

\begin_layout Standard
Now we have to iterate this experiment 
\begin_inset Formula $M$
\end_inset

 times.
 The probability of drawing a given pair 
\begin_inset Formula $K$
\end_inset

 times is then 
\begin_inset Formula 
\[
p\left(K\vert j,k\right)=C\left(p\left(w_{i},w_{j}\right)\right)^{K}{M \choose K}
\]

\end_inset

Write 
\begin_inset Formula $x=p\left(w_{i},w_{j}\right)$
\end_inset

 for short, then the normalization is 
\begin_inset Formula 
\[
f\left(w_{i},w_{j}\right)=C\sum_{K=0}^{M}x^{K}{M \choose K}=C\left(1+x\right)^{M}
\]

\end_inset

Right??? I'm confused.
 Now, since 
\begin_inset Formula $\epsilon=Mx\ll1$
\end_inset

 we can write
\begin_inset Formula 
\[
f\left(w_{i},w_{j}\right)=C\left(1+Mp_{i}p_{j}+\mathcal{O}\left(\epsilon^{2}\right)\right)\approx C\left(1+Mp_{i}p_{j}\right)
\]

\end_inset

and so the marginal is 
\begin_inset Formula 
\[
f\left(w_{i},*\right)=\sum_{j}f\left(w_{i},w_{j}\right)\approx CN+CMp_{i}
\]

\end_inset

but 
\begin_inset Formula $N\gg Mp_{i}$
\end_inset

 by assumption, so 
\begin_inset Formula $f\left(*,*\right)=1\approx CN^{2}$
\end_inset

 and so 
\begin_inset Formula 
\[
f\left(w_{i},*\right)=\frac{1}{N}
\]

\end_inset

which seems wrong, so I made a mistake above!?
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Six of the diary.
 
\end_layout

\end_body
\end_document
