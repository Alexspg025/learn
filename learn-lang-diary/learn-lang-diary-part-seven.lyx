#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary - Part Seven
\end_layout

\begin_layout Date
March 2022 - present
\end_layout

\begin_layout Author
Linas Vep≈°tas
\end_layout

\begin_layout Abstract
The language-learning effort involves research and software development
 to implement the ideas concerning unsupervised learning of grammar, syntax
 and semantics from corpora.
 This document contains supplementary notes and a loosely-organized semi-chronol
ogical diary of results.
 The notes here might not always makes sense; they are a short-hand for
 my own benefit, rather than aimed at you, dear reader!
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Part Seven of the diary on the language-learning effort opens the door to
 next steps.
 The last round of experiments appear to be successful, and there do not
 seem to be any nagging unresolved issues.
 What comes next?
\end_layout

\begin_layout Section*
Summary Conclusions
\end_layout

\begin_layout Standard
A summary of what is found in this part of the diary:
\end_layout

\begin_layout Itemize
No summary yet.
\end_layout

\begin_layout Section*
The Possibilities
\end_layout

\begin_layout Standard
The last round of experiments appear to be successful, and there do not
 seem to be any nagging unresolved issues.
 What comes next? Here's a list of possbilities.
\end_layout

\begin_layout Itemize

\series bold
Accuracy Evaluation.

\series default
 Compare dictionaries to the hand-crafted LG English dict.
 This is a bit tedious and boring, since it seems unlikely to yeild anything
 interesting.
 It seems inevitable, as its the kind of thing other people want to see.
 The only benefit is that it is a way of perhaps characterizing the the
 effects of different parameter choices.
 In current runs, the 
\begin_inset Quotes eld
\end_inset

noise
\begin_inset Quotes erd
\end_inset

 parameter is the most highly explored: but what setting yeilds the best
 results? Comparing to LG should reveal the answer.
 Estimate a few weeks to a month of sustained effort.
\end_layout

\begin_layout Itemize

\series bold
Data Cleanup.

\series default
 During pair-counting and/or MPG parsing, there is a bug that repeatedly
 escapes backslashes, leading to a cascade of backslashes in the dataset.
 This is just junk, and should be fixed.
 Fixing it will surely improve quality.
 It's tedious and boring.
 Two ways to fix: (1) start from scratch (2) hunt out multiple backslashes,
 and perform a custom merge, just like a word-class merge, but without forming
 a word-class.
 Option (2) is maybe easier and faster, but requires crafting custom code.
 Maybe a few weeks to write this code, another few weeks to fully debug
 it.
 Option (1) is foundationally better but tedious and time consuming.
 Estimate a month of keeping a watchful eye on the progress of the data
 processing.
 Yuck, either way.
 
\end_layout

\begin_layout Itemize

\series bold
Morphology.

\series default
 We've ignored the morphological structure of English.
 Morphology is crucial for most Indoeuropean and Arabic langauges, and so
 coverage could be vastly improved by putting together code for automatic
 morphology detection/processing.
 Diary Part One already sketched how this could be done, including a worked
 example confirming that the idea will provide good results.
 Implementing this in code, and then performing the experiments to confirm
 it, is a relatively straight-forward affair.
 Time-consuming, but well within reach.
 Estimate six months of sustained effort; more time if interrupted.
 A motivated grad student could do this, might take 12-18 months.
\end_layout

\begin_layout Itemize

\series bold
Reiterate classification.

\series default
 Given the initial dictionaries, the corpus can be parsed with the LG parser,
 using those dictionaries.
 The result of such parsing is again a collection of disjuncts, much like
 the ones from MPG parsing, but with different observation counts.
 After collecting such counts, the classification step can be perfomed again,
 presumably resulting in a somewhat different classification, perhaps one
 that is more accurate?
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

This appears to be a technically easy step to take, as it just follows well-trod
 ground, mostly.
 A few weeks or a month of close supervision of the training runs.
\end_layout

\begin_deeper
\begin_layout Itemize
A variant of the above is to use the initial category assignment of the
 word as a 
\begin_inset Quotes eld
\end_inset

word-sense
\begin_inset Quotes erd
\end_inset

, and to tag the new disjunct with that word-sense.
 One way to do this would be to treat the initial disjunct as a 
\begin_inset Quotes eld
\end_inset

subscript
\begin_inset Quotes erd
\end_inset

, and so the same text-word, but with two different subscripts, is treated
 as two distinct words.
 Counts and further clustering continue to treate these as two different
 words, until/unless the second round of clustering erases the distinction.
 Handling this subscript-tagging requires new code; it is perhaps similar
 to cross-sensory tagging, e.g.
 if/when correlating with audio, video data.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Refactorization.

\series default
 The disjuncts from above run provides a dataset that can be compared to
 the MPG-deived classes, and be used to refactor those, in several different
 ways.
 Perhaps some Sections are never used; they could be dropped.
 Perhaps a block-diagonal structure can be discovered.
 That is, a word-disjunct pair, the disjunct having N connectors, can be
 viewed s an N+1-rank tensor.
 Perhaps the collection of these tensors has some obvious diagonal structure.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Refactoring in this way feels like it might be both theoretically challenging,
 as well as presenting practical difficulties of discovering high-quality
 algorithms and then debugging them.
 This could easily take more than a few months.
 Compared to just re-iterating, this seems more difficult, more error-prone,
 and less robust.
\end_layout

\begin_layout Itemize

\series bold
Entities and References.

\series default
 A word-vector, for a given word, can be viewed in two ways.
 One way is to say that the disjunct describes the textual environment of
 the word: it's N-gram or skip-gram.
 Anothe This r way is to say is that it captures the semantic embedding
 of the word; its a list of all of teh 
\begin_inset Quotes eld
\end_inset

facts
\begin_inset Quotes erd
\end_inset

 known about that word.
 This is even more powerful, when the word is sense-tagged, i.e.
 tagged with the initial word-category.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

There are two types of entitites: common entities and text-specific entities.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
I want to write 
\begin_inset Quotes eld
\end_inset

common nouns
\begin_inset Quotes erd
\end_inset

, but in fact, the entities may be specific events in time, i.e.
 verbs.
 It would be awkward to write 
\begin_inset Quotes eld
\end_inset

common noun or common verb
\begin_inset Quotes erd
\end_inset

, so we'll just call them 
\begin_inset Quotes eld
\end_inset

entities
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

 Common entites hold across all texts, such as 
\begin_inset Quotes eld
\end_inset

cat
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

dog
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

run
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

jump
\begin_inset Quotes erd
\end_inset

.
 Text-specific entities occur in one text but not another: 
\begin_inset Quotes eld
\end_inset

John
\begin_inset Quotes erd
\end_inset

, which might be a different 
\begin_inset Quotes eld
\end_inset

John
\begin_inset Quotes erd
\end_inset

 in each text.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The most interesting/valuable task would be reference detection and reference
 resolution.
 How could this be done? A naive algo is to gather up a subset of a vector,
 specific to one text, and look for high-MI transitive relations.
 For example, 
\begin_inset Quotes eld
\end_inset

John ran the engine.
 It ran fine
\begin_inset Quotes erd
\end_inset

 has the relations 
\begin_inset Quotes eld
\end_inset

ran engine
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

it ran
\begin_inset Quotes erd
\end_inset

, which form a transitive relation between 
\begin_inset Quotes eld
\end_inset

it
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

engine
\begin_inset Quotes erd
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Of course, the pairing of 
\begin_inset Quotes eld
\end_inset

John
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

it
\begin_inset Quotes erd
\end_inset

 can also be deduced.
\end_layout

\end_inset

 For this to work well, though, stems are needed, or, more properly speaking,
 lexical functions.
\end_layout

\begin_layout Itemize

\series bold
Lexical Functions.

\series default
 This seems eminently important, but how? 
\end_layout

\begin_layout Itemize

\series bold
Synonymous Phrases.

\series default
 Word-classes are already a form of weak synonymy; how can one form strong
 synonymy? By appliying more stringent membership requirements? Based on
 current results, it appears that this would be enough, and that it would
 work fairly well.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Synonymous phrases require the ability to compare collections of partially-assem
bled disjuncts, to see how the connectors compare.
 This risks a combinatorial explosion.
 It does require new code.
\end_layout

\begin_deeper
\begin_layout Itemize
It might be possible and worthwhile to simultaneously fish for synonyms
 as well as grammar.
 Synonyms are already going to behave the same way grammatically, whereas
 part-of-speech groupings are much looser.
 This would result in a 
\begin_inset Quotes eld
\end_inset

multi-scale
\begin_inset Quotes erd
\end_inset

 dictionary, where each part-of-speech grouping can be further subdivided
 into synonym collections.
 Implementing this requires altering the 
\begin_inset Quotes eld
\end_inset

WordClass
\begin_inset Quotes erd
\end_inset

 construction to be marked with a class-type: a loose part-of-speech; a
 tighter synonym designation.
 This requires rejiggering the code a little bit; seems like a great idea.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Set Phrases, Institutional Phrases, Idioms.

\series default
 These are groupings of words that occur more frequently together, then
 apart.
 How can these be identified? Why would we be interested in performing such
 an identification? Is it a stepping stone to something better?
\end_layout

\begin_layout Itemize

\series bold
Antonyms.

\series default
 A famous deficiency in neural net approaches is the inability to identify
 antonyms.
 The current code & theory is equally blind to antonyms.
 Yet this is deeply, fundamentally important for understanding.
\end_layout

\begin_layout Itemize

\series bold
Sound, Pictures, Blueprints, Video
\series default
 The approach to this is sketched elsewhere, already.
 This is a huge, multi-year project.
 Intersting, too.
 Will it impress anyone in the short term? Probably not? Who has time to
 do this? How can I nurture it along? At any rate, code should be altered
 to at least allow multi-sensory data streams, which is not possible right
 now.
 
\end_layout

\begin_layout Itemize

\series bold
Common-sense Reasoning.
 
\series default
This is the holy grail.
 I had some insights into this.
 How did that go, again? Something about large-scale correlations.
 This is combinatorially-explosive territory, again.
 How can it be tackled?
\end_layout

\begin_layout Itemize

\series bold
System Interaction.

\series default
 Currently, only I can perceive results within the knowledge graph.
 How can it be exposed so that it can be viewed by outsiders? Even shallow
 perusal would help build interst and support.
\end_layout

\begin_layout Subsection*
Favorites
\end_layout

\begin_layout Standard
Lets narrow down the above.
 Favorite next tasks are:
\end_layout

\begin_layout Itemize
Reiterate classification.
 Run it a second time.
 This includes implementing word-sense tagging.
 Shouldn't be too hard.
 Interesting, and teh generalization seems useful, anyway.
\end_layout

\begin_layout Itemize
Multi-scale clustering.
 (aka synonyms) This requires developing multi-scale WordClass infrastructure.
 Shouldn't bee too hard.
 Seems useful, anyway.
\end_layout

\begin_layout Itemize
Add support for multi-sensory data streams.
 This is a refactorization of the current code, to allow it to operate on
 more general data streams.
 Might fit well with the multi-scale work, above.
\end_layout

\begin_layout Standard
Favorite theoretical activities:
\end_layout

\begin_layout Itemize
Lexical Functions.
 This seems important, but don't yet have a cler vision on how to do this.
 This needs to be developed.
 Perhaps this can be a heavily-abstracted sysnonym thingy? 
\end_layout

\begin_layout Itemize
Antonyms.
 This is important.
 But how?
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Seven of the diary.
 
\end_layout

\end_body
\end_document
