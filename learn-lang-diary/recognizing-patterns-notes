
Title page:
* Hi, My name is linas Vepstas, you can call me Linas.
* I'm here to talk about the automated, unsupervised learning
  of explainable, comprehendable patterns in language,
  and in a general setting that includes audio and video,
  or anything -- the noosphere.

Slide 1:
* I'm  going to start by making some outrageous claims.
* I'm going to claim that the system being presented here is capable of
  solving the famous Frame Problem, the Symol Grounding Problem.
* That common-sense reasoning can be learned.
* Not only are these claims the holy grail of AGI, which is why they are
  outrageous,
* But I will present an aglortihm that is not particularly complicated.
* It consists of some fairly well-known parts that many others have
  fooled with over the decades.
* Its not in uncharted territory.

* In this sense, most of what I will say has been said before.
* It might even seem obvious, or simplisitic to you, or even naive.
* I'd like to ask for your attention none-the-less, as you may find
  the particular combination here surprising and unusual.
* Bear with me. I'll start out simple.

Everything in the universe is a graph
* This is the universe not just of physical stars and planets,
  but also the universe of thoughts and concepts, the noosphere.
* Sparse graphs are necessarily labeled, as otherwise there is no
  way of talking about the things in the graph.
* The edges are necessarily labelled by the vertexes at thier
  end-points.

Graphs are decomposable
* This should be obvious, but is rarely mentioned in graph theory.
* You can gut an edge to get two half-edges.
* After cutting the egde, you need to label the cut ends so that
  you know how to join them again.
* The metaphor I will us repeatedly is that of the jigsaw puzzle-piece
  connector.
* Jigsaw puzzle pieces are designed so that the mating
  of the pieces is forced.
* When you cut an edge, attach a connector to it.
* In this example, polarity is indeterminate; flip a coin.

* In the gran scheme of things, you will discover that connectors
  can be multi-polar, multi-sexual.
* This is perhaps politically controversial, but biologists have found
  fungi with 43 different types of sex.
* Some of these can mate, and produce viable offsprint, and some cannot.
  Its quite strange.

* I've drawn a picture of a telescope jigsaw-puzzle piece, with
  connectors for looking at the sun and the moon.
* There are other things that can se the sun and the moon: eyes and
  lenses.  They have the same jigsaw shape.
* These shapes define the syntax of the graph..
* The syntax is exactly that collection of shapes that allow connections
  to occur.

* This might not be the defnition of syntax that you are comfortable
  with, or that you have learned.
* It is, however, entirely equivalent to the more conventional
  defintions you may know.
* This is not a particularly deep claim, it's fairly well-known and
  well-understood, I'm sorry only in not having a good reference for it
  at my fingertips.

Graphs are Compositional
* This is so basic, so obvious that
  We are like fish in water, we are not aware of the water.
  That's why I have to say these things out loud.
* At the top of this chart, I've drawn a conventional defintion of a
  term algebra.
* A term algebra consists of a set of function symbols taking arguments.
  The arguments can be constants, variables, or other terms.
* Plugging thing togther has a name: its called "beta reduction"
  terms are always DAG's, directed acyclic graphs.

* People in symbolic AI and proof theory and programming and compilers
  and chip design work with term algbras all the time. Its the stock
  of trade.

* The only point here is that jigsaw connectors generalize the
  concepts of term algebras.  One can assemble things that aren't just
  DAG's but can be any but directed or undirected graph, in general.

* The connectors are type theoretical types.
* Types have structure; types can have a very complex structure.
* Thus, perhaps the jigsaw diagram is misleading: that connector
  looks so simple.
* It can, in fact be, quite very complex.

* By they way: a type theory typ is the same thing as it is in computer
  science.
* It can be an int a float, a string: these are primitive types.
* It can be a compound type: a list, an OO class, a function signature.

* I'm drawing a jigsaw connector to represent any and all of these things.

* I told you that this will all be simple, yet confusing at the same
  time.  Perhaps it begins here.

Slide physics:
* In the next two slides, I simply want to indicate that these ideas are
  just absolutely everywhere.
* All over the place.
* Here's a quick tour.
* Anything tensorial.
* On the left: cobordisms, from string theory.
* On the right: natural language as a quantum algebra.
* The actual paper there is quite interesing.
* Coecke is saying that you can parse natural language quickly and
  easily with quantum algorithms.

* Anyway: Compostion is like contracting tensor indexes.
* Connecting connectors is like contracting tensor indexes.

Chemistry Biology
 * Some examples from chemistry and biology.
 * A chemical reaction has inputs and outputs.
 * These are the "connectors", they must connect, for a reaction to
   happen.
 * Prusinkiewicz has ben developing algorithmic botany for the last
   three decades. It is beautiful and stunning and relevant to AGI.
   You should check it out.

Link Grammar:
* My fetish for jigsaw connectors comes from Link Grammar.
* The diagram here is taken from the original 1991 LG paper.
  That paper explicitly talks about jigsaw-puzzle pieces.
* Above, you can see some words with connectors.
* Below, a parse:
  S for the subject-verb linkage
  O for the object-verb linkage
  D for connecting to the dterminer, which English demands.
* The idagram drawn here is correct for this sentence, but
  as a warning:
* I'm glossing over some more complex ideas in the connector
  type matching system. As I said earlier, types need not be primitive
  types; they can be compound types.

* A few quick side remarks: the LG grammar can be converted into HPSG,
  into dependency grammars, into categorial grammars, functional
  grammars.
* This can be done algorithmically.
* There's some eye-watering mathematical proofs of this stuff out there.

* If you haven't head of LG before ... well, now you have. It's real.
* There are complete dictionaries for English and Russian.
* There are smaller demo dictionaries for ten more.
* It's legit, linguistically-speaking; hundreds of published papers.

Vision:
* Now for the good stuff.
* The jigsaw paradigm can be applied to the analysis of shapes and
  colors.
* I've drawn the abstract graph for the corresponding image.
* There are connectors.
* Some connectors connect north-south directions
* Some connectors connect to colors
* Some connector connecto to round shapes.
* Some connectors connecto to a background.
* So its not just shape, or just color: some of these connections
  are abstract.

* A key point: it is not about pixels.
* Shape grammars really are different than pixel-collections.

Audio:
* We can do this for sound, too.
* This is a whale song from NOAA.
* Its a fast-fourier-transform FFT spectrum.
* On the bottom left, you can see a sequence if seven regularly spaced
  low frequency chirps.
* You know what a chirp is. Tweet.
* A rising or a falling pitch.
* The diagram on the right is a structural representation of the
  first 10 seconds of the FFT spectrum.
* I've tried to draw the diagram that captures and extracts that
  structure.

* AH HAH!
* But now we come to a key point.
* Where did those meaningful filters come from?
* How did we know to look for chirps?
* Surely it is not because some grad student hand-coded up a bunch of
  filters just so.
* I mean, you could do that, but that is not the point of AGI.
* You want to discover this filters from scratch. From Nothing at all.
* So how do you do that?
* How do you find structure from nothing?

Part Two
* Now we come to the dense, mathematical part of this talk.
* I will go fairly quickly, because many of these ideas are
  well-known in the machine-learning industry.
* I will show how to build up a graph structure from pair-wise
  correlations.
* Then I will show how to generalize from the particulars of
  specific graphs.
* This generalization turns out to be isomorphic to clustering,
  and to matrix factorization.
* You can use either clustering algorithms to do this, or matrix
  factorization algorithms to do this.
* I bet you can even use neural nets to do this.
* No seriously! You can! No one has done it, but you can!
* There's gonna be a lot of neural net fans at ths conference,
  I can't steal thier thunder.

So,the details.

Lexical attraction
* Start with a frequentist approach to probability.
* Observe things, and count how often they occur.
* Observing pairs is the easiest.
* Blast through some large corpus of English text.
* Say, a million sentences.
* Count how often you see word pairs: words close to one-another.
* 2-grams is the fanyc word for it.
* This gives you a count, call it N(u,w) of the word on the left and the
  word on the right.
* Divide by the total number of pairs to get a probability,
  a frequentist probability.
* The star here, the asterisk, is a wild-card.
* Divide the pair probability by the marginal probability of the left
  word, and the marginal probability of the right word.
* Take the logarithm
* This is basic textbook MI
* I'll call it Lexical attraction because that's what Yuret called it
  and because I need to distinguish from another MI, coming up.
* Otherwise, its the same thing.

* The diagram here is from Yuret's thesis.
* What he did here is very clever:
  He drew the maximum spanning tree.
* it is a tree that connects every word in the sentence,
  and the edges are choosen so that the grand total MI is the largest
  possible.

* Note that the LA/MI for "Norther Ireland" is huge.  That's because
  when these two words occur in a text, they commonly occur together.

* You don't much see "Northern" with that many other things, and
  you don't see lot of other kinds of Ireland.
* These two words occur together.  That is what a high MI score tells
  you.

* You can extract a grammar from this.

Lexical entries
*
* ampersand resembles "linear logic"
* Tensorial notation popular in quantum-influenced thinking
* d is for disjunct because the connector sequences are disjoined.
* The plus sign is a lie: its actually a menu-choice,
  to build once sentence you must pick one.
* hat-e is a unit basis vector

Similarity scores
* Problem with assuming Euclidean space is that taking orthogonal
  projections can lead to negative probabilities. Its pathological.
* Experimental measurements show that cosine distance is low quality
* MI is well-behaved under Markov (affine) transformations.
* Star means sum over everything
* It's a weighted word probability.
* Its a feynmann diagram with a vacuum contribution!

Factorization
* When I call two different words "a noun", I am assigning them to a
  "grammatical class".
* All words in that grammatical class behave similarly.
* I am going from a specific matrix of (word-disjunct) pairs to
  a matrix of (grammatical class, syntactic structure) pairs.
* Neural nets can accurately capture the dense, interconnected central
  region, but they necessarily perform dimensional reduction on
  the sparse left and right factors. By erasing the sparse factors,
  neural nets become no longer interpretable.

Compositionality
* Assembled portions of a jigsaw puzzle are just like a single jigsaw
  puzzle piece: an area with a bunch of unconnected connectors.

Something from Nothing
* We got lucky with the words
* We need to find processors that convert input intto "words".
* Extremely high dimensional .. billions!

Symbol grounding problem
* This is interpretability.
* What is a "whistle"? Oh, it is a specific filter sequence
* "I know a whistle when I hear it" but how can I explain it?

* The frame problem asks: what parts of the environment are
  important for making deciisions?
* Answer: those parts of the environment that have been filtered
  out by the currrent active filters.
* But what are those?
* Answer: they were learned, by applying mutual information to
  extract similarity, and then factorize/generalize into common
  situations we find ourselves in.

Common Sense Reasoning
* Olde-school AI work on reasoning harks to the foundations of
  mathematical logic developed in the first half of the 20th century.
* The trope is that common sense is somehow like mathematical logic
* "The rational actor" of the enlightenment, of economics.
  (Its not)
* Never explain a joke.
* But this is important.
* This is funny precisely because its logic that correct, but
  fails the meta problem of why we went to the doctor in the first
  place.
* The common-sense reasoning context is that this is taking place in a
  doctors office, which drags in the frame (the frame problem) of
  everything that doctors do, which include curing ailments.
  (Frames are like the lexical functions of MTT. They tell you how to
  navigate the network of relations.)
* If we can learn lexical functions, then we can learn common-sense
  behavior patterns, and common-sense reasoning.
* We can learn the network of what happens in a doctors office, and
  how to navigate that network. (The same way we navigate lexical
  functions).
* We can determine that the advice "don't do that" fails to match the
  "rational" expectations of what happens in a doctors office.
* But what are the "rational expectations" of what happens in a doctors
  office? it is that learned network.
* That's the joke, explained.
* We can learn the rules of reasoning; they are not God-given (aka
  hard-coded by some programmer.)

Conclusions
* I skipped over experimental results. There are a lot of them.
  They come out of the blue - like the gaussian distribution.
  I have no clue why. Nor do any search engines.  There is
  effectively no published theory on anything I talked about today.
* Its a HUGE vaccum, and its also an opportunity for any student who
  want to make a mark on the AGI world.

----

is this a gaussian unitary ensemble?

Extra material:
the definition of syntax here is the same as the conventinoal defn of
syntax.
