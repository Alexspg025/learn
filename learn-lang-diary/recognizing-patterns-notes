

Slide 1:
* Most of what I will say has been said before.
* It's not just obvious, but simplisitic, bordering on the naive.
* But if you don't pay attention, you will become confused.
* How is it that simple, obvious things can be so confusing?
* Pay attention!

Everything is a graph
* The universe not just of physical stars and planets,
  but also the universe of thoughts and concepts, the noosphere.

Graphs are decomposable
* Polarity is indeterminate in this example.
* It can be multi-polar, and not just two polarities.
* Biologists have found species of fungus that have
  43 different types of sex, some of which can mate,
  and some which cannot.

Graphs are Compositional
* This stuff is so basic, so obvious that
  We are like fish in water, we are not aware of the water.
  That's why I have to say it out loud.
* The jigsaw connectors generalize the concepts of term algebras
  to not be not just DAG's, but diected/undirected graphs in general.
* Type theory types may themselves decomposable, may have structure.

Slide physics:
* Anything tensorial.
* Compostion is like contracting tensor indexes.
* Coecke actually says you can parse natural language in linear time
  with quantum algorithms.

Chemistry Biology
 * A chemical reaction has inputs and outputs.
 * These are the "connectors", the jigsaw puzzle pieces.
 * The algorithmic botany work is beautiful and stunning.

Link Grammar:
* The diagram is taken from the original 1991 LG paper.
  It explicitly uses jigsaw-puzzle pieces.
* The LG type system is oversimplified, here. The type matching system
  is more complex, and polarity +/- indicate left-right.

Vision:
* Polarity direction is arbitrary, here, but there are differrent
  kinds of connectors - up-down vs shape vs color.

Audio:
* This is a whale song from NOAA.
* The diagram is a structural representation of the first 10 seconds
  of the chart, where you can see (plainly see) chirps, every two
  seconds.
* Make chirp/tweet sound

Lexical attraction
* Diagram is from Yuret's thesis

Lexical entries
* ampersand resembles "linear logic"
* Tensorial notation popular in quantum-influenced thinking
* d is for disjunct because the connector sequences are disjoined.
* The plus sign is a lie: its actually a menu-choice,
  to build once sentence you must pick one.
* hat-e is a unit basis vector

Similarity scores
* Problem with assuming Euclidean space is that taking orthogonal
  projections can lead to negative probabilities. Its pathological.
* Experimental measurements show that cosine distance is low quality
* MI is well-behaved under Markov (affine) transformations.
* Star means sum over everything
* It's a weighted word probability.
* Its a feynmann diagram with a vacuum contribution!

Factorization
* When I call two different words "a noun", I am assigning them to a
  "grammatical class".
* All words in that grammatical class behave similarly.
* I am going from a specific matrix of (word-disjunct) pairs to
  a matrix of (grammatical class, syntactic structure) pairs.
* Neural nets can accurately capture the dense, interconnected central
  region, but they necessarily perform dimensional reduction on
  the sparse left and right factors. By erasing the sparse factors,
  neural nets become no longer interpretable.

Compositionality
* Assembled portions of a jigsaw puzzle are just like a single jigsaw
  puzzle piece: an area with a bunch of unconnected connectors.

Something from Nothing
* We got lucky with the words
* We need to find processors that convert input intto "words".
* Extremely high dimensional .. billions!

Symbol grounding problem
* This is interpretability.
* What is a "whistle"? Oh, it is a specific filter sequence
* "I know a whistle when I hear it" but how can I explain it?

* The frame problem asks: what parts of the environment are
  important for making deciisions?
* Answer: those parts of the environment that have been filtered
  out by the currrent active filters.
* But what are those?
* Answer: they were learned, by applying mutual information to
  extract similarity, and then factorize/generalize into common
  situations we find ourselves in.

Common Sense Reasoning
* Olde-school AI work on reasoning harks to the foundations of
  mathematical logic developed in the first half of the 20th century.
* The trope is that common sense is somehow like mathematical logic
  "The rational actor" of the enlightenment, of economics.
  (Its not)
* Never explain a joke.
* But this is important.
* This is funny precisely because its logic that correct, but
  fails the meta problem of why we went to the octor in the first
  place.
* The common-sense reasoning context is that this is taking place in a
  doctors office, which drags in the frame (the frame problem) of
  everything that doctors do, which include curing ailments.
  (Frames are like the lexical functions of MTT. They tell you how to
  navigate the network of relations.)
* If we can learn lexical functions, then we can learn common-sense
  behavior patterns, and common-sense reasoning.
* We can learn the network of what happens in a doctors office, and
  how to navigate that network. (The same way we naviage lexical
  functions)
* We can determine that the advice "don't do that" fails to match the
  "rational" expectations of what happens in a doctors office.
* But what are the "rational expectations" of what happens in a doctors
  office? it is that learned network.
* That's the joke, explained.
* We can learn the rules of reasoning; they are not God-given (aka
  hard-coded by some programmer.)

Conclusions
* I skipped over experimental results. There are a lot of them.
  They come out of the blue - like the gaussian distribution.
  I have no clue why. Nor do any search engines.  There is
  effectively no published theory on anything I talked about today.
* Its a HUGE vaccum, and its also an opportunity for any student who
  want to make a mark on the AGI world.

----

is this a gaussian unitary ensemble?
