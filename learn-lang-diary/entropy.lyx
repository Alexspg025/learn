#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Entropy
\end_layout

\begin_layout Abstract
The language-learning diary entertains a recurring theme of entropy and
 various related principles.
 Although there are many many many sources for these concepts, it is convenient
 to put them here in an overview form.
 The content here is extracted from varioustexts.
\end_layout

\begin_layout Section
Fast Overview
\end_layout

\begin_layout Standard
A generic fast overview.
 See Wikipedia 
\begin_inset Quotes eld
\end_inset

Partition function (mathematics)
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Markov random field
\begin_inset Quotes erd
\end_inset

 for more.
\end_layout

\begin_layout Itemize
States denoted by 
\begin_inset Formula $\sigma$
\end_inset

 (spins, from Ising model) with distribution 
\begin_inset Formula $p\left(\sigma\right)$
\end_inset

...
 there are 
\begin_inset Formula $N\gg1$
\end_inset

 i.e.
 
\begin_inset Formula $N\to\infty$
\end_inset

 states.
 
\end_layout

\begin_layout Itemize
In machine learning, one writes 
\begin_inset Formula $x$
\end_inset

 for 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\sigma$
\end_inset

 is a vector, 
\begin_inset Formula $N$
\end_inset

-dimensional
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Energy
\begin_inset Quotes erd
\end_inset

 of a state is 
\begin_inset Formula $E\left(\sigma\right)=-\log p\left(\sigma\right)+const.$
\end_inset

 
\end_layout

\begin_layout Itemize
Density of states: 
\begin_inset Formula $\rho(E)=\sum_{\sigma}\delta(E-E(\sigma))$
\end_inset


\end_layout

\begin_layout Itemize
Total Entropy is 
\begin_inset Formula $S\left(E\right)=\log\rho\left(E\right)$
\end_inset


\end_layout

\begin_layout Itemize
Both contain have leading large-
\begin_inset Formula $N$
\end_inset

 term i.e.
 are extensive properties.
 
\end_layout

\begin_layout Itemize
Without loss of generality, can write the Boltzmann distribution
\begin_inset Formula 
\[
p\left(\sigma|\beta\right)=\frac{1}{Z\left(\beta\right)}\exp-N\sum_{i}\beta_{i}H_{i}\left(\sigma\right)
\]

\end_inset

where there are 
\begin_inset Formula $M$
\end_inset

 parameters 
\begin_inset Formula $\beta_{i}$
\end_inset

 called order parameters, Lagrange multipliers, etc.
 and the 
\begin_inset Formula $H_{i}\left(\sigma\right)$
\end_inset

 are constants of motion.
 
\end_layout

\begin_layout Itemize
In probability theory and information geometry, one often writes 
\begin_inset Formula $\theta$
\end_inset

 instead of 
\begin_inset Formula $\beta$
\end_inset

 as the parameter, and 
\begin_inset Formula $f_{i}$
\end_inset

 instead of 
\begin_inset Formula $H_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
In machine learning, one often writes 
\begin_inset Formula $w$
\end_inset

 instead of 
\begin_inset Formula $\beta$
\end_inset

.
 Then, 
\begin_inset Formula $w$
\end_inset

 is a 
\begin_inset Quotes eld
\end_inset

weight vector
\begin_inset Quotes erd
\end_inset

, allowing a neural-net interpretation.
\end_layout

\begin_layout Itemize
The partition function is
\begin_inset Formula 
\[
Z\left(\beta\right)=\sum_{\sigma}\exp-N\sum_{i}\beta_{i}H_{i}\left(\sigma\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
The above describes a 
\begin_inset Quotes eld
\end_inset

pure state
\begin_inset Quotes erd
\end_inset

, where the parameters 
\begin_inset Formula $\beta{}_{i}$
\end_inset

 are fixed constants.
\end_layout

\begin_layout Subsection
Mixed States and Saddle point methods
\end_layout

\begin_layout Standard
Often the order parameters do not have a sharp value, and one must instead
 assume a mixture model, where the order paramters have some distribution.
 
\end_layout

\begin_layout Itemize
If the order parameter has some distribution 
\begin_inset Formula $q\left(\beta\right)$
\end_inset

 then one has a 
\begin_inset Quotes eld
\end_inset

mixed state
\begin_inset Quotes erd
\end_inset

 and must write
\begin_inset Formula 
\[
p\left(\sigma\right)=\int d\beta q\left(\beta\right)e^{-N\mathcal{H}\left(\sigma,\beta\right)}
\]

\end_inset

where the integral is over the 
\begin_inset Formula $M$
\end_inset

 parameters: 
\begin_inset Formula $\int d\beta=\int d\beta_{1}d\beta{}_{2}\cdots d\beta_{M}$
\end_inset

 and 
\begin_inset Formula $\mathcal{H}\left(\sigma,\beta\right)=\sum_{i}\beta_{i}H_{i}\left(\sigma\right)+\frac{1}{N}\log Z\left(\beta\right)$
\end_inset

.
\end_layout

\begin_layout Standard
With some mild assumptions, one can make approximations
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $q\left(\beta\right)$
\end_inset

 is smooth, if 
\begin_inset Formula $q\left(\beta\right)$
\end_inset

 does not depend on 
\begin_inset Formula $N$
\end_inset

 and if 
\begin_inset Formula $q\left(\beta\right)$
\end_inset

 has non-vanishing support at the saddle point 
\begin_inset Formula $\beta^{*}$
\end_inset

, then the above can be approximated using saddle-point methods, giving
 
\begin_inset Formula 
\[
E\left(\sigma\right)=-\frac{1}{N}\log p\left(\sigma\right)=\sum_{i}\beta_{i}^{*}H_{i}\left(\sigma\right)+\frac{1}{N}\log Z\left(\beta^{*}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
The saddle point 
\begin_inset Formula $\beta^{*}$
\end_inset

 is the solution to 
\begin_inset Formula 
\[
\frac{1}{N}\left.\frac{\partial\log Z\left(\beta\right)}{\partial\beta_{i}}\right|_{\beta^{*}}=-H_{i}\left(\sigma\right)
\]

\end_inset

Note that 
\begin_inset Formula $\beta^{*}=\beta^{*}\left(\sigma\right)$
\end_inset

 that is, 
\begin_inset Formula $\beta^{*}$
\end_inset

 is a function of 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout Subsection
Ising Models and Markov Random Fields
\end_layout

\begin_layout Subsection
Zipf's Law
\end_layout

\begin_layout Standard
See 
\begin_inset Quotes eld
\end_inset

Zipf â€™s law and criticality in multivariate data without fine-tuning
\begin_inset Quotes erd
\end_inset

 David J.
 Schwab, Ilya Nemenman, Pankaj Mehta, https://arxiv.org/pdf/1310.0448.pdf
\end_layout

\begin_layout Standard
This paper provides a proof that Zipf's law arises whenve
\end_layout

\end_body
\end_document
