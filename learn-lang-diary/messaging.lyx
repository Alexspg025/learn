#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Messaging
\end_layout

\begin_layout Abstract
Thoughts about messaging and a relationship to grammar.
 Some old ideas, rephrased, some new ideas, incocomplete.
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Two almost inrelated ideas, and one theme.
\end_layout

\begin_layout Standard
Lets start with an obvious observation: natural language is used to transmit
 messages, from one human mind to another.
 Language carries messages.
 Painfully obviously.
 Now pair this with another idea: message-passing algorithms are a good
 way of solving NP-complete graphical constraint satisfaction problems.
 This pairing suggests a wild insight: that a collection of human minds,
 working together, are using message passing as a technique for collectively
 solving some difficult problem.
 To what degree can this insight be developed into a formal theory of the
 mind, and speciffically, a formalm theory of the collective, social mind?
 I don't know, and what follows below will be mostly unrelated to that,
 and will instead tackle grammar again, the grammar of natural language.
\end_layout

\begin_layout Subsection*
Grammar via Beleif Propagation
\end_layout

\begin_layout Standard
This is a reprise of a recurring theme.
 The goal is to find effective and tractable computational strategies for
 extracting meaning from natural language.
 The current work concerns itself with a very basic layer, that of extracting
 a lexical grammar, describing the syntax of the language, and a crude level
 of semantics that follows therefrom.
 By a 
\begin_inset Quotes eld
\end_inset

lexical grammar
\begin_inset Quotes erd
\end_inset

, it is meant that sentences of the language can be broken down into words,
 and that the relatioinship between words can be obtained from a lexicon,
 that is, from a dictionary where each word can be looked up to discover
 the grammatical relationships that word can engage in.
 It is useful to note that such lexicons are redily extended to include
 idioms, set phrases, institutional expressions, colocations; such multi-word
 constructions do not alter the underlying concept.
\end_layout

\begin_layout Standard
Lexicality implies that language can be analyzed in terms of words (or set
 phrases, 
\emph on
etc.
\emph default
).
 But language is also fundamentally statistical and probabilisitic: there
 is no ultimate, final truth to syntax and semantics, but only likely meanings
 and interpretations.
 In this setting, lexicality means not only that language can be viewed
 as a graph of relationships between words, but also that the graph can
 be factored into local components.
 Specifically, each local component consists of a word, and the other nearby
 words that it may interact with: a word, and its syntactico-semantically-neares
t neighbors.
\end_layout

\begin_layout Standard
Modern probability theory has a standard formulation using the terminology
 and notation of statistical mechanics.
 In this formulation, one begins by asserting that the universe is described
 by a summation over all possibilities: everything that might happen, can
 happen, with some associated probability.
 This sum is called the partition function; it is symbolized by 
\begin_inset Formula $Z$
\end_inset

, and the partitioning is simply the statement each possibility has a probabilit
y.
 For natural language, this just means that every possible sequence of words
 (a 
\begin_inset Quotes eld
\end_inset

sentence
\begin_inset Quotes erd
\end_inset

) occurs with some probability; ungrammatical sentences have a low, approximatel
y vanishing probability.
\end_layout

\begin_layout Standard
It is a theorem of Boltzmann that paritition functions can be written as
 sums over exponentials, and that the most likely possibility is given by
 maximizing the entropy.
 This is not an assumption that has to be artificially forced onto the system;
 rather, it is the factual statement that, if you beleive in probability,
 then there is no other way: it is a theorem.
 Combining natural language with probability then suggests that it is fruitful
 to articulate the statistical mechanics thereof.
\end_layout

\begin_layout Standard
In what follows, the formal grammar of choice is Link Grammar.
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"

\end_inset

 This choice is made for several reasons.
 First, one may argue that the actual choice of a grammar formalism is immateria
l, as all grammars are effectvely inter-convertable between one-another
 by algorithmic means.
 Thus, the choice of formalism boils down to convenience; what notational
 system is most convenient? Here, Link Grammar stands out.
 First, it is effectively a form of dependency grammar, and so is natural
 to linguists trained in that tradition.
 Second, by expressing grammar in terms of link types, it leverages type
 theory, and has a very natural bridge to categorial grammars and pregroup
 grammars: link types are just the type-theoretical types of the relationships
 that categorial grammars articulate.
 As categorial grammars are normally considered to be a form of phrase-structure
 grammars, exposing the relationships as types provides the natural bridge
 between the phrase-structure and dependency grammar schools of thought.
 Finally, Link Grammar is appealing from the tradition of mathematics: Link
 Grammar is a tensor algebra.
 Lexical entries are tensors, and lexical entries are composed into sentences
 in exactly the same way that one composes tensors in a tensor algebra.
 It is precisely this tensorial nature that then enables Curry–Howard correspond
ance; Link Grammar is the 
\begin_inset Quotes eld
\end_inset

internal language
\begin_inset Quotes erd
\end_inset

 of monoidal categories.
\end_layout

\begin_layout Standard
The next section articulates the statistical mechanics Link Grammar.
 This presents Link Grammar as both a constraint-statistfaction problem
 as well as a maximum entropy problem.
 This is followed by a section looking at the belief-propagation aglorithm,
 inspired by and devloped along the lines described by Mézard and Mora
\begin_inset CommandInset citation
LatexCommand cite
key "Mezard2008"

\end_inset

.
\end_layout

\begin_layout Subsection*
Statistical Mechanics of Link Grammar
\end_layout

\begin_layout Standard
To do so will require the articulation of a lot of notation to make things
 clear.
 A lot of this will repeat what I've written before, brought together into
 one place.
 Following the template given in section 4 of Mézard and Mora, one begins
 from first principles by defining the probability of a word sequence 
\begin_inset Formula $\underline{w}=\left(w_{1},\cdots,w_{n}\right)$
\end_inset

 as 
\begin_inset Formula 
\[
P\left(\underbar{w}\right)=\lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^{N}\delta\left(\underbar{w},\underbar{w}_{i}\right)
\]

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the number of sentences in the training corpus, each 
\begin_inset Formula $\underbar{w}_{i}$
\end_inset

 is a grammatically valid sentence.
 The above states that, basicly, a word sequence 
\begin_inset Formula $\underbar{w}$
\end_inset

 is valid if it occurs in the training corpus; else it is not.
 While perhaps this might be formally appropriate expression, this is neither
 a good practical definition of language, nor does it even give much of
 a theoretical foothold.
\end_layout

\begin_layout Standard
One then applies the standard Ansatz that grammar is lexical, or rather,
 that a lexical approach can describe most of the aspects of human language.
 As usual, we presume that the Link Grammar formalism is the most directly
 appriopriate for the task.
 Lexical entries are then of the form 
\begin_inset Formula 
\[
\left(\left(w,d\right),h\right)
\]

\end_inset

where 
\begin_inset Formula $w$
\end_inset

 is a word, 
\begin_inset Formula $d$
\end_inset

 is a disjunct describing the the grammatical relationship the word can
 engage in, and 
\begin_inset Formula $h$
\end_inset

 is a 
\begin_inset Quotes eld
\end_inset

cost
\begin_inset Quotes erd
\end_inset

 or entropy associated with the word-disjunct pair.
 It is convenient to write 
\begin_inset Formula $h=h\left(w,d\right)$
\end_inset

 as the cost of the word-disjunct pair; only the lowest cost is meaningful,
 and it does not make sense for a given disjunct to have more than one cost.
 Impossible word-disjunct pairs have infinite cost, rendering thier probability
 zero.
 That is, the probability of a word-disjunct pair, up to an overall normalizatio
n, can be taken as
\begin_inset Formula 
\[
P\left(w,d\right)\sim\exp-h\left(w,d\right)
\]

\end_inset


\end_layout

\begin_layout Standard
In this Ansatz, the probability of a word sequence 
\begin_inset Formula $\underbar{w}$
\end_inset

 is then 
\begin_inset Formula 
\[
P\left(\underbar{w}\right)=\frac{1}{Z}\prod_{w_{j}}\prod_{\left(d_{1},\cdots,d_{n}\right)}P\left(w_{j},d_{j}\right)\Delta\left(d_{1},\cdots,d_{n}\right)
\]

\end_inset

where the formal grammar is encoded as a boolean satisfiability factor 
\begin_inset Formula 
\[
\Delta\left(d_{1},\cdots,d_{n}\right)=\begin{cases}
1 & \left(d_{1},\cdots,d_{n}\right)\mbox{ is a valid parse}\\
0 & \mbox{otherwise}
\end{cases}
\]

\end_inset

while the probabilistic aspects, that some parses are more likely than others,
 are encoded in the lexical factors 
\begin_inset Formula $P\left(w,d\right)$
\end_inset

.
 Replacing probabilities by thier logs, one can equivalently write
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(\underbar{w}\right)=\frac{1}{Z}\prod_{\underbar{d}}\Delta\left(\underbar{d}\right)\exp-\mathcal{A}\left(\underbar{w},\underbar{d}\right)
\]

\end_inset

using as shorthand 
\begin_inset Formula $\underbar{d}=\left(d_{1},\cdots,d_{n}\right)$
\end_inset

 as the string of 
\begin_inset Formula $n$
\end_inset

 disjuncts associated with the 
\begin_inset Formula $n$
\end_inset

 words 
\begin_inset Formula $\underbar{w}=\left(w_{1},\cdots,w_{n}\right)$
\end_inset

.
 The 
\begin_inset Formula $Z$
\end_inset

 is famously a normalization: the partition function.
 The 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

action
\begin_inset Quotes erd
\end_inset

, given by 
\begin_inset Formula 
\[
\mathcal{A}\left(\underbar{w},\underbar{d}\right)=\sum_{j=1}^{n}h\left(w_{j},d_{j}\right)
\]

\end_inset

A slightly more formal and more accepted notation would be to write this
 as 
\begin_inset Formula 
\[
P\left(\underbar{w}\right)=\frac{1}{Z}\exp-\mathcal{A}\left(\underbar{w}\right)
\]

\end_inset

so that the action now incorporates the boolean satisfiability factor:
\begin_inset Formula 
\[
\mathcal{A}\left(\underbar{w}\right)=\sum_{\underbar{d}}\left[\Xi\left(\underbar{d}\right)+\sum_{j=1}^{n}h\left(w_{j},d_{j}\right)\right]
\]

\end_inset

so that 
\begin_inset Formula $\Xi\left(\underbar{d}\right)=-\ln\Delta\left(\underbar{d}\right)$
\end_inset

 is either minus-infinity for an invalid parse, or zero for a valid parse.
 The sum is taken over all possble lists 
\begin_inset Formula $\underbar{d}$
\end_inset

 of disjuncts 
\begin_inset Formula $d_{j}$
\end_inset

.
 The 
\begin_inset Formula $\Xi$
\end_inset

 term excludes those lists that do not correspond to valid parses in the
 formal grammar.
 The 
\begin_inset Formula $h$
\end_inset

 term excludes those pairings of words-to-disjuncts that do not occur in
 the lexis; and it they do occur, then 
\begin_inset Formula $h$
\end_inset

 provides a weight (a lexical weight) indicating how likely that word-disjunct
 pairing may be.
\end_layout

\begin_layout Standard
In the Link Grammar formulation, the term 
\begin_inset Formula $\Delta\left(\underbar{d}\right)$
\end_inset

 can also be factorized locally, as it is a product of graphical elements;
 the disjuncts consist of lists of connectors, which either can connect,
 or not.
 That is, an arity-
\begin_inset Formula $m$
\end_inset

 disjunct is a conjunction of connectors 
\begin_inset Formula $c$
\end_inset

 written as
\begin_inset Formula 
\[
d_{j}=c_{j1}\&\cdots\&c_{jm}
\]

\end_inset

with each connector being a half-link, that is, a link with a direction
 indicator: 
\begin_inset Formula 
\[
c=\left(\ell,\sigma\right)
\]

\end_inset

with 
\begin_inset Formula $\ell\in L$
\end_inset

 being one of the Link Grammar link types, and 
\begin_inset Formula $\sigma\in\left\{ +,-\right\} $
\end_inset

 being a direction indicator.
 Define the conjugate direction indicator 
\begin_inset Formula $\bar{\sigma}$
\end_inset

 as 
\begin_inset Formula 
\[
\bar{\sigma}=\begin{cases}
- & \mbox{if }\sigma=+\\
+ & \mbox{if }\sigma=-
\end{cases}
\]

\end_inset

Thus, two connectors 
\begin_inset Formula $c_{1}$
\end_inset

 and 
\begin_inset Formula $c_{2}$
\end_inset

 connect if the link types match, and the direction indicators are conjugate.
 Thus, one may define
\begin_inset Formula 
\[
\delta\left(c_{1},c_{2}\right)=\delta\left(\ell_{1},\ell_{2}\right)\delta\left(\sigma_{1},\bar{\sigma}_{2}\right)
\]

\end_inset

A sentence is parsable when all connectors are connectable, and so
\begin_inset Formula 
\[
\Delta\left(\underbar{d}\right)=\Delta\left(d_{1},\cdots,d_{n}\right)=\prod_{d_{j}=\left(c_{j1}\&\cdots\&c_{jm}\right)}\delta\left(c_{jk},c_{j^{\prime}k^{\prime}}\right)
\]

\end_inset

with the product being over all of the individual connectors in each disjunct.
 Implicit in the above is a further constraint that the parse graph be a
 planar graph, i.e.
 that there is a no-links-crossing constraint.
 This constraint is very useful for controlling the combinatorial explosion
 of possible parses; unfortunately, it is a non-local constraint, and thus
 cannot be easily written in a factorizable manner.
 Rather than tackling the difficulty of obtaining an adequate notation for
 such a non-local constraint, it is easier, for now, to implicitly keep
 this in the background.
 There are multiple techniques that can be used when dealing with this;
 for now these are secondary concerns.
\end_layout

\begin_layout Standard
Taking the logarithm, so as to turn products into sums, the action 
\begin_inset Formula $\mathcal{A}$
\end_inset

 can then be expressed as a summation of local interactions.
\begin_inset Formula 
\[
\mathcal{A}\left(\underbar{w}\right)=\sum_{\underbar{d}}\sum_{j=1}^{n}\left[h\left(w_{j},d_{j}\right)+\sum_{d_{j}=\left(c_{j1}\&\cdots\&c_{jm}\right)}\Xi\left(c_{jk},c_{j^{\prime}k^{\prime}}\right)\right]
\]

\end_inset

The intent of the above expression is that it is to be interpreted as a
 summation over Feynmann diagrams, with 
\begin_inset Formula $h$
\end_inset

 corresponding to a 
\begin_inset Formula $m$
\end_inset

-point vertex (when the disjunct 
\begin_inset Formula $d_{j}$
\end_inset

 has arity 
\begin_inset Formula $m$
\end_inset

), and the 
\begin_inset Formula $\Xi$
\end_inset

 terms corresponding to propagators connecting vertexes.
 The propagaotrs are exactly the Link Grammar links, weighted in such a
 way that it contributes zero to the action, when the link is allowed, and
 otherwise contributes minus-infinity.
\end_layout

\begin_layout Standard
The partition function can then be formally written as
\begin_inset Formula 
\[
Z=\sum_{\underbar{w}}\exp-\mathcal{A}\left(\underbar{w}\right)
\]

\end_inset

with the summation over 
\begin_inset Formula $\underbar{w}$
\end_inset

 running over all possible word-sequences of arbitrary length.
 As usual, is is convenient to introduce external currents 
\begin_inset Formula $\underbar{J}$
\end_inset

 so that variational principles can be used to extract quantities of interest.
 Thus, for example, writing 
\begin_inset Formula 
\[
Z\left[J\right]=\sum_{\underbar{w}}\exp-\mathcal{A}\left(\underbar{w}\right)+\underbar{J}\cdot\underbar{w}
\]

\end_inset

and taking a variation 
\begin_inset Formula $\delta J$
\end_inset

 along the direction 
\begin_inset Formula $\underbar{w}$
\end_inset

, the limit reduces to 
\begin_inset Formula 
\[
P\left(\underbar{w}\right)=\frac{1}{Z}\left.\frac{\delta Z\left[J\right]}{\delta J}\right|_{J=0}=\frac{1}{Z}\exp-\mathcal{A}\left(\underbar{w}\right)
\]

\end_inset


\end_layout

\begin_layout Subsection*
Grammar via Beleif Propagation
\end_layout

\begin_layout Standard
The learning task is that the valid disjuncts are not known 
\emph on
a priori
\emph default
, they must be discovered.
 To accomplish this, the first step is to replace each possible connection-pair
 
\begin_inset Formula $\delta\left(c_{jk},c_{j^{\prime}k^{\prime}}\right)$
\end_inset

 by a belief of the possibility 
\begin_inset Formula $p\left(c_{jk},c_{j^{\prime}k^{\prime}}\right)$
\end_inset

 of such a connection being present, interpreted as a probability: 
\begin_inset Formula $0\le p\le1$
\end_inset

 with the goal of eventually driving each connectable pair to be zero or
 one.
 Similarly
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
The other thing to point out is due to loops, etc.
 it won't be naive beleif propagation, but it will be Survey Propagation,
 as mentioned there...
 The starting point appears to have an Ansatz about the form of partition
 function.
 The appropriate form seems to be 
\begin_inset Formula 
\[
P\left(\right)=\frac{1}{Z}
\]

\end_inset


\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "tufte"

\end_inset


\end_layout

\end_body
\end_document
