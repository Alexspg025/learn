#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary - Part Nine
\end_layout

\begin_layout Date
Oct 2022 – Present
\end_layout

\begin_layout Author
Linas Vepštas
\end_layout

\begin_layout Abstract
The language-learning effort involves research and software development
 to implement the ideas concerning unsupervised learning of grammar, syntax
 and semantics from corpora.
 This document contains supplementary notes and a loosely-organized semi-chronol
ogical diary of results.
 The notes here might not always makes sense; they are a short-hand for
 my own benefit, rather than aimed at you, dear reader!
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Part Nine of the diary explores continuous learning.
\end_layout

\begin_layout Section*
Summary Conclusions
\end_layout

\begin_layout Standard
A summary of what is found in this part of the diary:
\end_layout

\begin_layout Itemize
None yet.
 
\end_layout

\begin_layout Section*
Hard lessons learned
\end_layout

\begin_layout Standard
Experiment-17 is the teacher.
 Here is what we learned:
\end_layout

\begin_layout Itemize
The disjuncts in `r16-merge.rdb` and `r13-all-in-one.rdb` are insufficient
 to generate interesting sentences.
 There are too few of them.
\end_layout

\begin_layout Itemize
Apparently, trimming has depleted the ranks.
 Thus, although they "look good" when examine individually, they're not
 rich enough to be used.
 
\end_layout

\begin_layout Standard
Here is what we can do differently, going forwards:
\end_layout

\begin_layout Itemize
This suggests clustering should be more aggressive.
 Clustering enriches the number of available disjuncts on any given word.
\end_layout

\begin_layout Itemize
A solution to not having enough disjuncts of the right shape is to supplements
 existing disjuncts with optional single links taken from word-pairs.
 This explodes the RAM usage in LG, up to 10 GB or 20GB or maybe more, depending.
 
\end_layout

\begin_layout Itemize
The LG `dict-atomese` backend was extended to use word-pairs and also ANY
 links.
 Disjuncts can now have optional word-pair connectors on them.
 (Done, Nov 2022)
\end_layout

\begin_layout Itemize
Since the LG atomese dict can now use single word-pairs, that means it can
 do MST/MPG parsing.
 Thus, we can get rid of the atomspace MST parser.
 (Done, Jan 2023)
\end_layout

\begin_layout Itemize
The LG backend can also supplement disjuncts with ANY links.
 (Done, Nov 2022)
\end_layout

\begin_layout Itemize
The MST/MPG mode can also use ANY links.
 (Done, Nov 2022)
\end_layout

\begin_layout Itemize
As a result, the LG parser can do all of it -- random-tree ANY parsing,
 MST/MPG parsing, and Section/disjunct parsing.
 
\end_layout

\begin_layout Itemize
This creates a possibility of doing "continuous learning": learning word
 pairs and disjuncts at the same time.
 
\end_layout

\begin_layout Itemize
However, the more complex portions cannot run until marginals recomputed.
 This suggests a natural awake/asleep cycle.
 During the awake cycle, data is ingested.
 During the asleep cycle, marginals are (re-) computed, MI is (re-)computed,
 similarities are updated.
 This is very nice, it gets rid of the pipeline.
 
\end_layout

\begin_layout Itemize
So it seems like it's time to abolish the pipeline.
\end_layout

\begin_layout Itemize
Easier said than done (Dec 2022) Computing MI on the fly raises issues with
 caching, stale data, write-back to the DB, read from the DB, general data
 flow.
 It's a bit messy.
 So this has to be a back-burner project.
\end_layout

\begin_layout Itemize
For example, a caching ProxyNode can be created.
 This would effectively be the old ECAN idea, this time done right.
\end_layout

\begin_layout Itemize
BTW, we can do GOE similarity with just word-pair MI.
 So clustering can begin before disjuncts have been created.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section*
The Plan
\end_layout

\begin_layout Itemize
Start with pair counting.
 Do NOT trim until after MST.
\end_layout

\begin_layout Itemize
Use uniform sentence lengths.
\end_layout

\begin_layout Itemize
During MST, count the pairs that contributed to the MST.
 Lets call this 
\begin_inset Quotes eld
\end_inset

second counting
\begin_inset Quotes erd
\end_inset

.
 Never trim second-counted pairs (at least not for the next few steps.)
\end_layout

\begin_layout Itemize
Perform tentative GOE clustering before MST, perform MST with and without
 clusters, try to see which is betters!? How to tell which is better? I
 guess higher totla MI.
 But how to count/trakc which contributed the most, and still maintain detailed
 balance? I.e.
 how to 
\begin_inset Quotes eld
\end_inset

undo
\begin_inset Quotes erd
\end_inset

 clustering? Or will second-counting be sufficinet to track? 
\end_layout

\begin_layout Section*
Notes
\end_layout

\begin_layout Standard
Nov 2022 – Tried restarting with `run-1-marg-tranche-123.rdb` which is not
 trimmed.
 But it's huge.
 300K x 300K and *lots* of the words have backslashes in them Yuck! 52GB
 to load ...
 Need to start over.
\end_layout

\begin_layout Standard
Bringup of the above ideas is in Expt-18.
\end_layout

\begin_layout Section*
Hypervector ruminations
\end_layout

\begin_layout Standard
Some questions:
\end_layout

\begin_layout Itemize
Given a vector in the GOE (say, a particular word, with coordinates measured
 via MI, offset and normalized to unit length), which is the nearest cube-corner
 (bipolar hypervector)? I assume it's the one with with all coordinated
 rounded to either +1 or -1.
 But this needs proof.
\end_layout

\begin_layout Itemize
What is the angular distance to that corner?
\end_layout

\begin_layout Itemize
Given a corner in the cube, what is the nearest actual vector?
\end_layout

\begin_layout Itemize
What is the distribution of Hamming distance vs.
 actual distance? That is, pick two actual vectors from the dataset.
 Their dot product is the 
\begin_inset Quotes eld
\end_inset

actual
\begin_inset Quotes erd
\end_inset

 distance.
 The Hamming distance between them is the Hamming distance between their
 nearest cube corners.
\end_layout

\begin_layout Itemize
Given a random cube corner, what is the most efficient way of finding the
 nearest element in the dataset?
\end_layout

\begin_layout Standard
Other unrelated questions:
\end_layout

\begin_layout Itemize
What is the QR decomposition of word-pair MI? viz 
\begin_inset Formula $M=QR$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

is orthogonal and 
\begin_inset Formula $R$
\end_inset

 is upper right triangular...
 is there significance to this decomposition? Is there a past (left) vs
 (right) future light-cone thing going on? 
\end_layout

\begin_layout Standard
Then the oldie but goodie:
\end_layout

\begin_layout Itemize
The GOE vectors form a de facto vierbein.
 What happens when I move from word to word? Can I define a connection?
 a curvature? a torsion? Even if it's flat, when I travel around a loop,
 is there a holonomy?
\end_layout

\begin_layout Section*
MST/MPG Distributions
\end_layout

\begin_layout Standard
When MST/MPG parses are created, how should the disjuncts in them be weighted,
 during counting? We can weight several different ways:
\end_layout

\begin_layout Itemize
Each is counted with a weight of 1.0
\end_layout

\begin_layout Itemize
Each is counted with a weight proportional to the total sentence MI.
\end_layout

\begin_layout Itemize
Each is counted with a weight that is a sum of the MI's of the word-pairs
 in that disjunct.
\end_layout

\begin_layout Itemize
Each is counted with a weight that is the average of the MI's of the pairs
 in that disjunct.
\end_layout

\begin_layout Itemize
Each is counted with the weighted average of the MI's of the pairs in the
 disjunct.
 
\end_layout

\begin_layout Itemize
The weighting is adjusted in some way by the number of connectors in the
 disjunct.
\end_layout

\begin_layout Standard
The last option seems to be the most appealing, the most 
\begin_inset Quotes eld
\end_inset

natural
\begin_inset Quotes erd
\end_inset

 to me.
 There are several variants for that.
 Lets define these clearly.
 Let 
\begin_inset Formula $w$
\end_inset

 be the germ of the section, and 
\begin_inset Formula $c$
\end_inset

 be the other connecting words in the section.
 Let 
\begin_inset Formula $MI\left(w,c\right)$
\end_inset

 be the MI, with the understanding that the order is reversed, when the
 word order is reversed (
\emph on
i.e.

\emph default
 connecting to left instead of right.) Let 
\begin_inset Formula $p\left(w,c\right)$
\end_inset

 be the corresponding frequency.
 Let 
\begin_inset Formula $\left|C\right|$
\end_inset

 be the total number of connectors in the section.
 Then the unweighted average is
\begin_inset Formula 
\[
\mbox{avg}_{C}\left(w\right)=\frac{1}{\left|C\right|}\sum_{c\in C}MI\left(w,c\right)
\]

\end_inset

The weighted average is
\begin_inset Formula 
\[
E_{C}\left(w\right)=\frac{\sum_{c\in C}p\left(w,c\right)MI\left(w,c\right)}{\sum_{c\in C}p\left(w,c\right)}
\]

\end_inset

This weighted average is problematic because common words drown out rare
 words in the summation.
 This is because 
\begin_inset Formula $p\left(w,c\right)$
\end_inset

 is huge, if 
\begin_inset Formula $c$
\end_inset

 is a word like 
\begin_inset Quotes eld
\end_inset

the
\begin_inset Quotes erd
\end_inset

.
 We don't really want such pairs to drown out the average, since the link
 to 
\begin_inset Quotes eld
\end_inset

the
\begin_inset Quotes erd
\end_inset

 is likely the least important contribution to this average.
\end_layout

\begin_layout Standard
Thus, perhaps the MI itself can provide a weight:
\begin_inset Formula 
\[
WI_{C}\left(w\right)=\frac{\sum_{c\in C}2^{MI\left(w,c\right)}MI\left(w,c\right)}{\sum_{c\in C}2^{MI\left(w,c\right)}}
\]

\end_inset

This uses 2 not exp because we measure MI in bits not nats.
 Moving forward, WI will be used, mostly because it seems like the best
 candidate, and I can't think of anything that feels more suitable.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
I find this very annoying.
 A decade later, I am still flying blind, with no theoretical foundations
 for any of the decisions being made here; just a bunch of gut-feels.
 I admit that perhaps I am not reading enough papers.
 But I do read a lot of paper abstracts, and none of the abstracts mention
 any work along these lines.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Next, we have the question: how many MST parses should be sampled? If it's
 only a few, then weighting probably doesn't matter.
 If its a lot, then we want to make sure that the top parses contribute
 the most, and are NOT drowned out by the large number of mediocre parses.
 So we're back again, asking old questions:
\end_layout

\begin_layout Itemize
For a fixed sentence, with parses ranked by MST, how does the sentence total
 MI decrease as a function of rank? (Relative to the parse with the highest
 MI?)
\end_layout

\begin_layout Itemize
What is the integral of the above? i.e.
 for a fixed sentence, what is the average sentence total MI? 
\end_layout

\begin_layout Itemize
What is the distribution of the total MI of the disjuncts, (given as the
 sum of MI for the pairs)? How does this vary as a function of the number
 of parses that are accepted?
\end_layout

\begin_layout Itemize
Should we assign
\end_layout

\begin_layout Standard
To make the above more precise: Let 
\begin_inset Formula $L$
\end_inset

 be the set of links in a sentence.
 The total MI for that sentence is then
\begin_inset Formula 
\[
MI_{\mbox{Tot}}=\sum_{\left(u,w\right)\in L}MI\left(u,w\right)
\]

\end_inset

This is naturally larger for longer sentences, which would seem to give
 undue weight to long sentences.
 How to rectify this? One available average is to divide by the number of
 words in the sentence.
 This then encourages parses with more links in them, 
\emph on
i.e.

\emph default
 maximizes for parses which have lots of loops.
 Again, perhaps undesirable? Another average is to divide by the total number
 of links in the sentence.
 This seems the most fair, although it does seem to (mildly) discourage
 parses with loops: Adding one more link, to form a loop, will 
\emph on
always
\emph default
 decrease this average, since the added link must necessarily have a lower
 MI than the others that were chosen.
\end_layout

\begin_layout Standard
A spanning tree with 
\begin_inset Formula $W$
\end_inset

 vertices will consist of 
\begin_inset Formula $W-1$
\end_inset

 edges.
 Thus, a sentence of length 
\begin_inset Formula $\left|W\right|$
\end_inset

 connected by 
\begin_inset Formula $\left|L\right|$
\end_inset

 links will have 
\begin_inset Formula $\left|L\right|-\left|W\right|+1$
\end_inset

 
\begin_inset Quotes eld
\end_inset

fundamental cycles
\begin_inset Quotes erd
\end_inset

 in it.
 Hmm.
 Is there a middle ground? Can we mildly encourage parses with cycles in
 them, without over-connecting everything to everything? So perhaps divide
 by 
\begin_inset Formula $\left|W\right|+0.5\left(\left|L\right|-\left|W\right|+1\right)$
\end_inset

? 
\end_layout

\begin_layout Standard
Planar graphs already limit the number of possible cycles...
 Assuming that neighboring words are always 
\begin_inset Quotes eld
\end_inset

related
\begin_inset Quotes erd
\end_inset

 to each other is a reasonable default, usually.
 But long-distance links are important, too.
 Finding the best long-distance links is a good thing.
 Should we weight in a way that encourages long-distance links? How? The
 number of alternatives available here is starting to be overwhelming here.
 There's no obvious clear-cut 
\begin_inset Quotes eld
\end_inset

best answer.
\begin_inset Quotes erd
\end_inset

 I don't know of any theoretical foundations that can point at a best answer.
\end_layout

\begin_layout Standard
Is MI actually 
\begin_inset Quotes eld
\end_inset

additive
\begin_inset Quotes erd
\end_inset

? I've always been assuming that, yes, it is, its foundational.
 That's a foundational assumption.
 The example above with disjuncts suggests otherwise.
 Thus, we also have a non-additive possibility for sentence MI, the WI analog
 for sentences:
\begin_inset Formula 
\[
WI\left(L\right)=\frac{\sum_{\left(u,w\right)\in L}2^{MI\left(u,w\right)}MI\left(u,w\right)}{\sum_{\left(u,w\right)\in L}2^{MI\left(u,w\right)}}
\]

\end_inset

Hmm.
 I kind-of like that.
 It seems to have OK properties.
 It is NOT additive: 
\begin_inset Formula $WI\left(L\right)\ne\sum_{w\in W}WI\left(w\right)$
\end_inset

 but it IS weighted-additive:
\begin_inset Formula 
\[
WI\left(L\right)=\frac{1}{2Z\left(L\right)}\sum_{w\in W}Z_{C}\left(w\right)WI_{C}\left(w\right)
\]

\end_inset

where 
\begin_inset Formula 
\[
Z_{C}\left(w\right)=\sum_{c\in C}2^{MI\left(w,c\right)}
\]

\end_inset

is the normalizing denominator for a disjunct, and 
\begin_inset Formula 
\[
Z\left(L\right)=\sum_{\left(u,w\right)\in L}2^{MI\left(u,w\right)}
\]

\end_inset

is the normalizing denominator for 
\begin_inset Formula $WI\left(L\right)$
\end_inset

.
 We call it 
\begin_inset Formula $Z$
\end_inset

 because its like a thermodynamic partition function.
 There's an extra factor of 2 because the connectors are double-counted.
\end_layout

\begin_layout Standard
BTW, note also that the averages are plain-additive:
\begin_inset Formula 
\[
MI_{\mbox{Tot}}\left(L\right)=\sum_{\left(u,w\right)\in L}MI\left(u,w\right)=\frac{1}{2}\sum_{w\in W}\left|C\right|\mbox{avg}_{C}\left(w\right)
\]

\end_inset

with the factor of 1/2 coming from the double-counting of links by connectors
 (since each connector is a half-link.)
\end_layout

\begin_layout Standard
So, from now on, lets track WI and Z in the code-base, as well as the usual
 averages.
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Nine of the diary.
 
\end_layout

\end_body
\end_document
