#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary - Part Nine
\end_layout

\begin_layout Date
Oct 2022 – Present
\end_layout

\begin_layout Author
Linas Vepštas
\end_layout

\begin_layout Abstract
The language-learning effort involves research and software development
 to implement the ideas concerning unsupervised learning of grammar, syntax
 and semantics from corpora.
 This document contains supplementary notes and a loosely-organized semi-chronol
ogical diary of results.
 The notes here might not always makes sense; they are a short-hand for
 my own benefit, rather than aimed at you, dear reader!
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Part Nine of the diary explores continuous learning.
\end_layout

\begin_layout Section*
Summary Conclusions
\end_layout

\begin_layout Standard
A summary of what is found in this part of the diary:
\end_layout

\begin_layout Itemize
None yet.
 
\end_layout

\begin_layout Section*
Mutual Information vs.
 Laplacian
\end_layout

\begin_layout Standard
This is a random half-baked suggestive idea that dead-ends immediately.
\end_layout

\begin_layout Standard
The discrete Laplacian on a 1-dimensional grid is a tri-diagonal matrix
 of the form
\begin_inset Formula 
\[
\Delta x_{i}=2x_{i}-x_{i-1}-x_{i+1}
\]

\end_inset

The pair-wise mutual information 
\begin_inset Formula 
\[
MI\left(u,v\right)=\log_{2}p\left(u,v\right)-\log_{2}p\left(u,*\right)-\log_{2}p\left(*,v\right)
\]

\end_inset

Inserting a factor of 1/2 gives the ranked-MI
\begin_inset Formula 
\[
MI_{\mbox{ranked}}\left(u,v\right)=\log_{2}\frac{p\left(u,v\right)}{\sqrt{p\left(u,*\right)p\left(*,v\right)}}
\]

\end_inset

These two somehow feel 
\begin_inset Quotes eld
\end_inset

similar
\begin_inset Quotes erd
\end_inset

 but I cannot connect the dots.
\end_layout

\begin_layout Standard
The MI 
\begin_inset Quotes eld
\end_inset

feels like
\begin_inset Quotes erd
\end_inset

 some kind of re-normalized propagator where the 
\begin_inset Formula $\log_{2}p\left(u,*\right)$
\end_inset

 feel like 
\begin_inset Quotes eld
\end_inset

vacuum contributions
\begin_inset Quotes erd
\end_inset

, but the details remain opaque.
\end_layout

\begin_layout Standard
Recall that 
\begin_inset Formula 
\[
p\left(u,*\right)=\sum_{w}p\left(u,w\right)
\]

\end_inset

Consider then the very high-dimensional difference equation
\begin_inset Formula 
\[
-\Delta E\left(u,v\right)=\log_{2}p\left(u,v\right)-\frac{1}{2N}\sum_{w\ne v}\log_{2}p\left(u,w\right)-\frac{1}{2N}\sum_{w\ne u}\log_{2}p\left(w,v\right)
\]

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the size of the vocabulary, viz.
 
\begin_inset Formula $N=\sum_{w}1$
\end_inset

.
 Using terminology from Chapter 6, this was being called 
\begin_inset Quotes eld
\end_inset

the energy
\begin_inset Quotes erd
\end_inset

, via analogy to the Boltzmann distribution.
 That is, 
\begin_inset Formula 
\[
E\left(u,v\right)=-\log_{2}p\left(u,v\right)
\]

\end_inset

and so the difference eqn is 
\begin_inset Formula 
\[
\Delta E\left(u,v\right)=E\left(u,v\right)-\frac{1}{2N}\left[\sum_{w\ne v}E\left(u,w\right)+\sum_{w\ne u}E\left(w,v\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Basically, we're fixing a point 
\begin_inset Formula $\left(u,v\right)$
\end_inset

 in the high-dimensional space 
\begin_inset Formula $N\times N$
\end_inset

 and then we are differencing to all of it's nearest neighbors.
 This is confusing because we really should have started with single words.
 Consider the observation frequency of a single word 
\begin_inset Formula $p\left(w\right)$
\end_inset

 and define 
\begin_inset Formula $E\left(w\right)=-\log_{2}p\left(w\right)$
\end_inset

.
 Experimentally, we don't track these values (why not? they've never seemed
 useful.
 But perhaps we should revisit.) Anyway, its clear that 
\begin_inset Formula $\left(w\right)\in N$
\end_inset

 is a single point in a high-dimensional space.
 In this case, the Laplacian really is clear and unabiguous: it is
\begin_inset Formula 
\[
\Delta E\left(w\right)=E\left(w\right)-\frac{1}{N}\sum_{u\ne w}E\left(u\right)
\]

\end_inset

This is the conventional 
\begin_inset Formula $N$
\end_inset

-dimensional finite-difference Laplacian, where we've taken the liberty
 of dividing by 
\begin_inset Formula $N$
\end_inset

 because it is so large.
 If this still feels odd: bear in mind that all points 
\begin_inset Formula $\left(u\right)$
\end_inset

 are nearest neighbors of the point 
\begin_inset Formula $\left(w\right)$
\end_inset

.
 The space itself is a simplex: all 
\begin_inset Formula $N$
\end_inset

 points are equidistant from all the others (Hamming distance).
\end_layout

\begin_layout Standard
For word-pairs, fixing a word-pair 
\begin_inset Formula $\left(u,v\right)$
\end_inset

 and then asking what all the Hamming distance-one pairs are, these are
 precisely 
\begin_inset Formula $\left\{ \left(u,w\right):w\ne v\right\} \cup\left\{ \left(w,v\right):w\ne u\right\} $
\end_inset

.
 That is the set involved in the definition of the pair-Laplacian.
 Should we call this the Hamming-Laplacian? The generalization to N-grams
 is obvious; we don't need this generalization (yet; we'll need something
 like of for the disjuncts! As disjuncts are just skip-grams in disguise.)
\end_layout

\begin_layout Standard
We've not looked at this beastie before.
 Let's take a look now.
\end_layout

\begin_layout Standard
The dataset being used is `
\family sans
run-1-marg-tranche-123.rdb
\family default
` – this is well-decribed elsewhere.
 It's untrimmed, its got a vocabulary of N=391548 words and a total of 28184319
 word pairs.
 The graph below shows a historgram of the distribution.
 There are 400 buckets in the histogram.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p9-laplace/laplace.png
	width 80col%

\end_inset


\end_layout

\begin_layout Standard
The reason for the cliff at 30 is unclear.
 Note that 
\begin_inset Formula $\log_{2}N=18.58$
\end_inset

 and that 
\begin_inset Formula $\log_{2}N_{\mbox{pairs}}=24.75$
\end_inset

.
 
\end_layout

\begin_layout Section*
Hard lessons learned
\end_layout

\begin_layout Standard
Experiment-17 is the thing.
\end_layout

\begin_layout Itemize
The disjuncts in `r16-merge.rdb` and `r13-all-in-one.rdb` are insufficient
 to generate interesting sentences.
 There are too few of them.
\end_layout

\begin_layout Itemize
Apparently, trimming has depleted the ranks.
 Thus, although they "look good" when examine individually, they're not
 rich enough to be used.
 
\end_layout

\begin_layout Itemize
The LG `dict-atomese` backend was extended to use word-pairs and also ANY
 links.
 
\end_layout

\begin_layout Itemize
Existing disjuncts can be supplemented with optional single links taken
 from word-pairs.
 This explodes the RAM usage in LG, up to 10 GB or 20GB or maybe more, depending.
 
\end_layout

\begin_layout Itemize
This suggests clustering should be more aggressive.
 
\end_layout

\begin_layout Itemize
Since the LG atomese dict can now use single word-pairs, that means it can
 do MST/MPG parsing.
 Thus, we can get rid of the atomspace MST parser.
 
\end_layout

\begin_layout Itemize
The LG backend can also suppliment disjuncts with ANY links.
 
\end_layout

\begin_layout Itemize
The MST/MPG mode can also use ANY links.
 
\end_layout

\begin_layout Itemize
As a result, the LG parser can do all of it -- random-tree ANY parsing,
 MST/MPG parsing, and Section/disjunct parsing.
 
\end_layout

\begin_layout Itemize
This creates a possibility of doing "continuous learning": learning word
 pairs and disjuncts at the same time.
 
\end_layout

\begin_layout Itemize
However, the more complex portions cannot run until marginals recomputed.
 This suggests a natural awake/asleep cycle.
 During the awake cycle, data is ingested.
 During the asleep cycle, marginals are (re-) computed, MI is (re-)computed,
 similarities are updated.
 This is very nice, it gets rid of the pipeline.
 
\end_layout

\begin_layout Itemize
So it seems like it's time to abolish the pipeline.
 
\end_layout

\begin_layout Itemize
BTW, we can do GOE similarity with just word-pair MI.
 So clustering can begin before disjuncts have been created.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Nine of the diary.
 
\end_layout

\end_body
\end_document
