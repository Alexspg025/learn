#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary - Part Nine
\end_layout

\begin_layout Date
Oct 2022 – Present
\end_layout

\begin_layout Author
Linas Vepštas
\end_layout

\begin_layout Abstract
The language-learning effort involves research and software development
 to implement the ideas concerning unsupervised learning of grammar, syntax
 and semantics from corpora.
 This document contains supplementary notes and a loosely-organized semi-chronol
ogical diary of results.
 The notes here might not always makes sense; they are a short-hand for
 my own benefit, rather than aimed at you, dear reader!
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Part Nine of the diary explores continuous learning.
\end_layout

\begin_layout Section*
Summary Conclusions
\end_layout

\begin_layout Standard
A summary of what is found in this part of the diary:
\end_layout

\begin_layout Itemize
None yet.
 
\end_layout

\begin_layout Section*
Mutual Information vs.
 Laplacian
\end_layout

\begin_layout Standard
This is the development of a suggestive idea that first appeared in Chapter
 Six.
 It should be put back there.
\end_layout

\begin_layout Standard
The discrete Laplacian on a 1-dimensional grid is a tri-diagonal matrix
 of the form
\begin_inset Formula 
\[
\Delta x_{i}=2x_{i}-x_{i-1}-x_{i+1}
\]

\end_inset

The pair-wise mutual information 
\begin_inset Formula 
\[
MI\left(u,v\right)=\log_{2}p\left(u,v\right)-\log_{2}p\left(u,*\right)-\log_{2}p\left(*,v\right)
\]

\end_inset

Inserting a factor of 1/2 gives the ranked-MI
\begin_inset Formula 
\[
MI_{\mbox{ranked}}\left(u,v\right)=\log_{2}\frac{p\left(u,v\right)}{\sqrt{p\left(u,*\right)p\left(*,v\right)}}
\]

\end_inset

The discrete laplacian somehow feels 
\begin_inset Quotes eld
\end_inset

similar
\begin_inset Quotes erd
\end_inset

 to the ranked-MI, but how? Can this be developed?
\end_layout

\begin_layout Subsection*
(Dead-end?) Ideas
\end_layout

\begin_layout Standard
Here are some suggestive ideas that don't sem to quite get traction.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $MI$
\end_inset

 
\begin_inset Quotes eld
\end_inset

feels like
\begin_inset Quotes erd
\end_inset

 some kind of re-normalized propagator, where the 
\begin_inset Formula $\log_{2}p\left(u,*\right)$
\end_inset

 feel like vacuum corrections; but how this could be is opaque.
\end_layout

\begin_layout Standard
The point 
\begin_inset Formula $\left(u,v\right)$
\end_inset

 feels like a point in a base-space, and the 
\begin_inset Formula $\left(u,*\right)$
\end_inset

 and 
\begin_inset Formula $\left(*,v\right)$
\end_inset

 feel like two diferent fibers above the point in the base space.
 The summation is happening on the fibers.
 That is, we've defined 
\begin_inset Formula 
\[
p\left(u,*\right)=\sum_{w}p\left(u,w\right)
\]

\end_inset

so that, first, we take the fiber sum, then the log, and then compare a
 point in base-space to it's two fiber-sums.
 That is, the ranked-MI 
\emph on
is
\emph default
 a kind of discrete Laplacian, but it's over a weird fibered space; its
 a comparison over fibers.
\end_layout

\begin_layout Subsection*
Hamming Laplacian
\end_layout

\begin_layout Standard
Consider the very high-dimensional difference equation
\begin_inset Formula 
\[
-\Delta E\left(u,v\right)=\log_{2}p\left(u,v\right)-\frac{1}{2\left(N-1\right)}\sum_{w\ne v}\log_{2}p\left(u,w\right)-\frac{1}{2\left(N-1\right)}\sum_{w\ne u}\log_{2}p\left(w,v\right)
\]

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the size of the vocabulary, viz.
 
\begin_inset Formula $N=\sum_{w}1$
\end_inset

.
 Using terminology from Chapter 6, this was being called 
\begin_inset Quotes eld
\end_inset

the energy
\begin_inset Quotes erd
\end_inset

, via analogy to the Boltzmann distribution.
 That is, 
\begin_inset Formula 
\[
E\left(u,v\right)=-\log_{2}p\left(u,v\right)
\]

\end_inset

and so the difference eqn is 
\begin_inset Formula 
\[
\Delta E\left(u,v\right)=E\left(u,v\right)-\frac{1}{2\left(N-1\right)}\left[\sum_{w\ne v}E\left(u,w\right)+\sum_{w\ne u}E\left(w,v\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
This difference equation is rightfull called a discrete Laplacian on a high-dime
nsional space.
 That this is the correct name can be seen as follows.
\end_layout

\begin_layout Standard
Basically, we're fixing a point 
\begin_inset Formula $\left(u,v\right)$
\end_inset

 in the high-dimensional space 
\begin_inset Formula $N\times N$
\end_inset

 and then we are differencing to all of it's nearest neighbors.
 This is confusing because we really should have started with single words.
 Consider the observation frequency of a single word 
\begin_inset Formula $p\left(w\right)$
\end_inset

 and define 
\begin_inset Formula $E\left(w\right)=-\log_{2}p\left(w\right)$
\end_inset

.
 Experimentally, we don't track these values (why not? they've never seemed
 useful.
 But perhaps we should revisit.) A single word 
\begin_inset Formula $w$
\end_inset

 can be thought of as a coordinate or direction in a high-dimensional space,
 so that that 
\begin_inset Formula $\left(w\right)\in N$
\end_inset

 is a location, a single point in that space.
 All the other words provide the other coordinates, so that the 
\begin_inset Formula $\left(u\right)$
\end_inset

's are all of the nearest-neighbors of 
\begin_inset Formula $\left(w\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
In this case, the Laplacian really is clear and unabiguous: it is
\begin_inset Formula 
\[
\Delta E\left(w\right)=E\left(w\right)-\frac{1}{N-1}\sum_{u\ne w}E\left(u\right)
\]

\end_inset

This is the conventional 
\begin_inset Formula $N$
\end_inset

-dimensional finite-difference Laplacian, where we've taken the liberty
 of dividing by 
\begin_inset Formula $N$
\end_inset

 because it is so large.
 If this still feels odd: bear in mind that all points 
\begin_inset Formula $\left(u\right)$
\end_inset

 are nearest neighbors of the point 
\begin_inset Formula $\left(w\right)$
\end_inset

.
 The space itself is a simplex: all 
\begin_inset Formula $N$
\end_inset

 points are equidistant from all the others.
 This is just Hamming distance one for a string of symbols that is one symbol
 long.
\end_layout

\begin_layout Standard
For word-pairs, fixing a word-pair 
\begin_inset Formula $\left(u,v\right)$
\end_inset

 and then asking what all the Hamming distance-one pairs are, these are
 precisely 
\begin_inset Formula $\left\{ \left(u,w\right):w\ne v\right\} \cup\left\{ \left(w,v\right):w\ne u\right\} $
\end_inset

.
 That is the set involved in the definition of the pair-Laplacian.
 Should we call this the Hamming-Laplacian? The generalization to N-grams
 is obvious; we don't need this generalization (yet; we'll need something
 like of for the disjuncts! As disjuncts are just skip-grams in disguise.)
 
\end_layout

\begin_layout Standard
Still, worth formalizing it.
 Given a 
\begin_inset Formula $k$
\end_inset

-gram 
\begin_inset Formula $\sigma=\left(w_{1},\cdots,w_{k}\right)$
\end_inset

, define the set of all 
\begin_inset Formula $k$
\end_inset

-grams that are Hamming-distance zero or one from 
\begin_inset Formula $\sigma$
\end_inset

 as
\begin_inset Formula 
\[
\left\{ s\right\} =\left(*,w_{2},\cdots,w_{k}\right)\cup\left(w_{1},*,w_{3},\cdots,w_{k}\right)\cup\left(w_{1},\cdots,*\right)
\]

\end_inset

The Hamming-Laplacian is then 
\begin_inset Formula 
\[
\Delta E\left(\sigma\right)=E\left(\sigma\right)-\frac{1}{k\left(N-1\right)}\sum_{\tau\in\left\{ s\right\} ;\tau\ne\sigma}E\left(\tau\right)
\]

\end_inset

The denominator is simply the number of terms in the summation.
\end_layout

\begin_layout Subsection*
Experimental exploration
\end_layout

\begin_layout Standard
We've not looked at this beastie before.
 Let's take a look now.
\end_layout

\begin_layout Standard
The dataset being used is `
\family sans
run-1-marg-tranche-123.rdb
\family default
` – this is well-decribed elsewhere.
 It's untrimmed, its got a vocabulary of N=391548 words and a total of 28184319
 unique word pairs.
 These were observed a total of 985483375 times.
 The graph below shows a histogram of the distribution of 
\begin_inset Formula $\Delta E\left(u,v\right)$
\end_inset

 of word-pairs 
\begin_inset Formula $\left(u,v\right)$
\end_inset

.
 There are 400 buckets in the histogram.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename p9-laplace/laplace.png
	width 80col%

\end_inset


\end_layout

\begin_layout Standard
The fit curve is exactly given by 
\begin_inset Formula $2^{3\Delta E/4}/N_{\mbox{pairs}}$
\end_inset

 .
 That is, the slope is 0.75.
 Note that 
\begin_inset Formula $2^{0.75}\approx1.682$
\end_inset

.
 As usual, the line is bowed, but we don't know why.
 Just for grins, there is a second fit: some numerology: 
\begin_inset Formula $\varphi^{\Delta}$
\end_inset

 where 
\begin_inset Formula $\varphi\approx1.618$
\end_inset

 the golden mean.
 This probably doesn't mean anything, though.
\end_layout

\begin_layout Standard
How does the data lead to this graph? Note that 
\begin_inset Formula $\log_{2}N=18.58$
\end_inset

 and that 
\begin_inset Formula $\log_{2}N_{\mbox{pairs}}=24.75$
\end_inset

 and finally 
\begin_inset Formula $\log_{2}T=29.88$
\end_inset

.
 The far-right hand side tells us that almost all word-pairs are observed
 once
\emph on
 i.e.

\emph default
 
\begin_inset Formula $E=-\log_{2}p=29.88$
\end_inset

 or maybe twice: 
\begin_inset Formula $E=29.88-1$
\end_inset

 or three times: 
\begin_inset Formula $E=29.88-\log_{2}3$
\end_inset

.
 At the same time, these words are observed with a far more frequent neighbor:
 
\emph on
e.g.

\emph default
 
\begin_inset Formula $\left(\mbox{the},X\right)$
\end_inset

 for some obscure word 
\begin_inset Formula $X$
\end_inset

(maybe a typo?), so that although the pair 
\begin_inset Formula $\left(\mbox{the},X\right)$
\end_inset

 is observed only once, the marginal sum 
\begin_inset Formula $\sum_{w\ne X}E\left(\mbox{the},w\right)$
\end_inset

 is small.
\end_layout

\begin_layout Section*
Hard lessons learned
\end_layout

\begin_layout Standard
Experiment-17 is the thing.
\end_layout

\begin_layout Itemize
The disjuncts in `r16-merge.rdb` and `r13-all-in-one.rdb` are insufficient
 to generate interesting sentences.
 There are too few of them.
\end_layout

\begin_layout Itemize
Apparently, trimming has depleted the ranks.
 Thus, although they "look good" when examine individually, they're not
 rich enough to be used.
 
\end_layout

\begin_layout Itemize
The LG `dict-atomese` backend was extended to use word-pairs and also ANY
 links.
 
\end_layout

\begin_layout Itemize
Existing disjuncts can be supplemented with optional single links taken
 from word-pairs.
 This explodes the RAM usage in LG, up to 10 GB or 20GB or maybe more, depending.
 
\end_layout

\begin_layout Itemize
This suggests clustering should be more aggressive.
 
\end_layout

\begin_layout Itemize
Since the LG atomese dict can now use single word-pairs, that means it can
 do MST/MPG parsing.
 Thus, we can get rid of the atomspace MST parser.
 
\end_layout

\begin_layout Itemize
The LG backend can also suppliment disjuncts with ANY links.
 
\end_layout

\begin_layout Itemize
The MST/MPG mode can also use ANY links.
 
\end_layout

\begin_layout Itemize
As a result, the LG parser can do all of it -- random-tree ANY parsing,
 MST/MPG parsing, and Section/disjunct parsing.
 
\end_layout

\begin_layout Itemize
This creates a possibility of doing "continuous learning": learning word
 pairs and disjuncts at the same time.
 
\end_layout

\begin_layout Itemize
However, the more complex portions cannot run until marginals recomputed.
 This suggests a natural awake/asleep cycle.
 During the awake cycle, data is ingested.
 During the asleep cycle, marginals are (re-) computed, MI is (re-)computed,
 similarities are updated.
 This is very nice, it gets rid of the pipeline.
 
\end_layout

\begin_layout Itemize
So it seems like it's time to abolish the pipeline.
 
\end_layout

\begin_layout Itemize
BTW, we can do GOE similarity with just word-pair MI.
 So clustering can begin before disjuncts have been created.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Nine of the diary.
 
\end_layout

\end_body
\end_document
