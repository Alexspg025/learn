#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Unsupervised Grammar Induction - Experimental Results
\end_layout

\begin_layout Date
February 2022
\end_layout

\begin_layout Author
Linas Vep≈°tas
\end_layout

\begin_layout Abstract
The OpenCog Learning project explores novel techniques for the induction
 of symbolic structure from environmental sources.
 As a first application of these ideas , the automated learning of the grammatic
al structure of English is explored.The language-learning effort involves
 research and software development to implement the ideas concerning unsupervise
d learning of grammar, syntax and semantics from corpora.
 This document provides a summary of results observed to date.
 
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
The goal of the OpenCog Learning project is to induce structure from raw
 observational data.
 In the most naive sense, this would be visual or auditory data.
 In a more sophisticated sense, 
\begin_inset Quotes eld
\end_inset

observational data
\begin_inset Quotes erd
\end_inset

 should be taken to be data that might already have some structure in it,
 as the goal of learning is to do so hierarchically, recursively: to deduce
 structure in structure inside of structure...
 If this can be made to work, then, 
\begin_inset Quotes eld
\end_inset

starting in the middle
\begin_inset Quotes erd
\end_inset

 is not a bad place to start.
 In the present case, 
\begin_inset Quotes eld
\end_inset

the middle
\begin_inset Quotes erd
\end_inset

 is a corpus of English text, and the goal is to extract grammatical structure.
 The words 
\begin_inset Quotes eld
\end_inset

grammatical structure
\begin_inset Quotes erd
\end_inset

 are meant to encompass morphological structure, syntactic structure, and
 lower reaches of semantic understanding, as far as this can be pushed.
 
\end_layout

\begin_layout Standard
The general theory of how this is to be done is sketched out in a companian
 article.
\begin_inset CommandInset citation
LatexCommand cite
key "Vepstas2022symbolic"
literal "false"

\end_inset

 The overall process is as follows: first, one computes the mutual information
 (MI) between word-pairs occurring in the corpus.
 The MI assigns a numerical score to how often a pair of words occur near
 each-other.
 This can be used to generate a maximum spanning tree (MST) parse of the
 text, which appears to capture the syntactic structure of the text reasonably
 well.
\begin_inset CommandInset citation
LatexCommand cite
key "Yuret1998"
literal "false"

\end_inset

 This parse tree can be disassembled into individual 
\begin_inset Quotes eld
\end_inset

jigsaw puzzle pieces
\begin_inset Quotes erd
\end_inset

 which encode the syntactic structure of the parse.
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"
literal "false"

\end_inset

 Individual jigsaw pieces encode the local syntactic environment of a given
 word.
 Collections of such jigsaw pieces can be understood to be vectors: each
 word is endowed with a lexis of all of the syntactic contexts in which
 it was observed.
 But this is too fine-grained; one whishes to generalize from the particular
 to the general.
 The vectors can be used both to discover words that have similar syntactic
 contexts (based on the similarity of the local environments) and also to
 factor out different word-senses, since different syntactic contexts associate
 strongly with distinct word-senses.
\begin_inset CommandInset citation
LatexCommand cite
key "Mihalcea2004,Mihalcea2005"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
This text provides reports on the algorithms developed to perform the above,
 and on the statistical properties of the resulting graphical networks.
 This report is a summary, encompassing more than a decade of research and
 development.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
A detailed diary of results, spanning a many hundreds of pages, can be found
 in the 
\begin_inset CommandInset href
LatexCommand href
name "Dairy"
target "https://github.com/opencog/learn/tree/master/learn-lang--diary"
literal "false"

\end_inset

, Parts One-Six, and the adjunct reports.
 
\end_layout

\end_inset

 There are obvious directions in which both the theory and the experiment
 could be taken, but so far haven't been.
 In the 
\begin_inset Quotes eld
\end_inset

downwards
\begin_inset Quotes erd
\end_inset

 direction, work on morphology, needed for Indo-European languages, and
 segmentation, needed for Chinese.
 Further down, the implementation of the same ideas to extract phonetic
 structure and perform general audio processing.
 In the upwards direction, one could could search for high-MI pairs across
 sentences, both to resolve references and to detect entities.
 The 
\begin_inset Quotes eld
\end_inset

syntatactic
\begin_inset Quotes erd
\end_inset

 relationships between entities presumably encodes thier semantic properties.
 Such syntactic relations can also be extracted between different sense
 streams, say, between words and blueprints/diagrams, or words and photographs,
 or even perhaps big-data settings such as computer vulnerability databases,
 or financial news and stock prices.
 The author beleives the basic algorithm is generic, and can be ratcheted
 upwards, to arbitrarily high reaches.
 Whether this proves to be true cannot be known without actual experiments;
 the current limitation to carrying these out is the develpment of a sutiable
 software infrastructure.
\end_layout

\begin_layout Standard
The remainder of this paper focuses on the narrow tasks defined above, in
 the same order as given.
 The computation of word-pairs results in a graph, whose vertexes are words,
 and whose edges are word-pairs.
 What are the statistical properties of this graph? MST parsing and the
 creating of jigsaw pieces results in a matrix of word, connector-sequence
 pairs.
 What asre the statistical properties of this matrix? The similarity between
 words can be judged with assorted different measures; how do these behave,
 and how does clustering and word-sense disambiguation proceed, in practice?
\end_layout

\begin_layout Subsection*
Software Infrastructure
\end_layout

\begin_layout Standard
The software infrastructure used to perform the above can be found in the
 GitHub repo for the OpenCog Learn project.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See 
\begin_inset CommandInset href
LatexCommand href
name "GitHub opencog/learn"
target "https://github.com/opencog/learn"
literal "false"

\end_inset


\end_layout

\end_inset

 It is built on top of the OpenCog AtomSpace
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See 
\begin_inset CommandInset href
LatexCommand href
name "GitHub opencog/atomspace"
target "https://github.com/opencog/atomspace"
literal "false"

\end_inset


\end_layout

\end_inset

 as a foundational layer.
 The AtomSpace is an in-RAM graph database with powerful graph query capabilitie
s, tuned for performance.
 It has shims that allow data to be stored in conventional disk databases
 and to be distributed across the network.
 
\end_layout

\begin_layout Standard
Most notable for the present task is the 
\begin_inset Quotes eld
\end_inset

matrix API
\begin_inset Quotes erd
\end_inset

:
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See 
\begin_inset CommandInset href
LatexCommand href
name "AtomSpace Matrix API"
target "https://github.com/opencog/atomspace/tree/master/opencog/matrix"
literal "false"

\end_inset


\end_layout

\end_inset

 it allows arbitary collections of subgraphs to be used as the basis elements
 of a vector space.
 If one has two such vector spaces, then pairs of elements form a matrix
 of rows and columns.
 Given a matrix, it is then straight-forward to compute marginal probabilites,
 conditional probabilities, and other typical statistical quantities.
 This is, of course, quite ordinary: what is new here is that the AtomSpace
 allows extremely sparse matricies to be represented: say, 100K 
\begin_inset Formula $\times$
\end_inset

 100K entires, of which all but a few million are zero.
 Conventional mathematics and statistics packages do not provide such tools
 for sparse data.
 The other difference between the AtomSpace and conventional statistical
 tools is that the rows and columns are themselves distinct collections
 of subgraphs of some larger graph.
 Given some large graph of data, one can slice-n-dice it in any number of
 ways, and ask how portions of it correlate, by imposing a matrix structure
 on the parts of interest.
 To put those parts under a microscope.
\end_layout

\begin_layout Section*
Pair Counting
\end_layout

\begin_layout Itemize
not WP, because verbs, action verbs
\end_layout

\begin_layout Itemize
matrix api for atomspace.
\end_layout

\begin_layout Standard
vector similarities allow similar w
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "/home/ubuntu/src/learn/learn-lang-diary/lang"
options "tufte"

\end_inset


\end_layout

\end_body
\end_document
