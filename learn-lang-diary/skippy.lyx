#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Skip-grams vs.
 Disjuncts
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Date
6 July 2018
\end_layout

\begin_layout Abstract
This document attempts to compare and contrast the various ways in which
 (adaptive)-skip-grams and disjuncts are similar, and how they differ.
 The comparison is not always obvious, as these two representations seem
 to be quite different, the goals and tasks that each are solving seem to
 be different, and the methods to obtain each seem to be quite different.
 In fact, many of these differences are superficial; there is more in common
 than there might seem.
 
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
The introduction here is minimal; it is assumed that the reader is generally
 conversant with both Link Grammar
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"

\end_inset

, and with SkipGram
\begin_inset CommandInset citation
LatexCommand cite
key "Mikolov2013"

\end_inset

 or AdaGram, as well as some of the concepts employed in the OpenCog language-le
arning project.
\end_layout

\begin_layout Subsection*
Link Grammar
\end_layout

\begin_layout Standard
Link Grammar defines a word-disjunct pair as an object of the general form
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	word: A- & B+ & C+ & ...;
\end_layout

\end_inset

The disjunct is the expression to the right of the colon.
 The notations A-, B+, C+, etc.
 are called connectors; they are types that indicate what other (classes
 of) words the given word can connect to.
 The minus and plus signs indicate whether the connectors link to the left
 or to the right.
 The number of such connectors is variable, depending on the disjunct.
 The ampersand serves as a reminder that, uring parsing, all connectors
 must be satisfied; that is, the connectors are conjoined.
 A given word can have multiple disjuncts associated with it, these are
 disjoined from one-another, thus the name 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
For the language learning project, it is convenient to extend the above
 to the notion of a pseudo-disjunct, where the connector types are replaced
 by instances of individual words.
 For example
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: girl- & home+;
\end_layout

\end_inset

is used to represent the grammatical structure of the sentence 
\begin_inset Quotes eld
\end_inset

the girl ran home
\begin_inset Quotes erd
\end_inset

, namely, that the verb 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 can attach to the word 
\begin_inset Quotes eld
\end_inset

girl
\begin_inset Quotes erd
\end_inset

 (the subject) on the left, and the word 
\begin_inset Quotes eld
\end_inset

home
\begin_inset Quotes erd
\end_inset

 (the object) on the right.
 An early goal of the language learning project is to automatically discern
 such pseudo-disjuncts; a later goal is to automatically classify such individua
l-word connectors into word-classes, and so generalizing individual words
 into connector types, just as in traditional Link Grammar.
\end_layout

\begin_layout Standard
The primary learning mechanism is to accumulate observation counts of different
 disjuncts, thus leading naturally to the idea of word-vectors.
 For example, one might observe the vector 
\begin_inset Formula $\vec{v}_{ran}$
\end_inset

 represented as
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: 3(girl- & home+) + 2(girl- & away+);
\end_layout

\end_inset

that might naturally arise if the sentence 
\begin_inset Quotes eld
\end_inset

the girl ran home
\begin_inset Quotes erd
\end_inset

 was observed three times, and 
\begin_inset Quotes eld
\end_inset

the girl ran away
\begin_inset Quotes erd
\end_inset

 was observed twice.
 Such vectors can be used to judge word-similarity.
 For example, a different vector 
\begin_inset Formula $\vec{v}_{walked}$
\end_inset

 represented as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	walked: 2(girl- & home+) + 3(girl- & away+);
\end_layout

\end_inset

suggests that the cosine-product 
\begin_inset Formula $\cos\left(\vec{v}_{ran},\vec{v}_{walked}\right)$
\end_inset

 between the two might be used to judge word-similarity: 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

walked
\begin_inset Quotes erd
\end_inset

 can be used in syntactically similar ways.
\end_layout

\begin_layout Standard
The disjunct representation also allows other kinds of vectors, such as
 those anchored on connectors.
 Using the examples above, one also has a vector for the word 
\begin_inset Quotes eld
\end_inset

home
\begin_inset Quotes erd
\end_inset

, which can be awkwardly written as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	3(ran: girl- & home+) + 2(walked: girl- & home+)
\end_layout

\end_inset

Note that the counts, here, of 3 and 2, are identical to the counts above:
 all of these counts are derived from the same observational dataset.
 What differs is the choice of the attachment-point for which the vector
 is to be formed.
\end_layout

\begin_layout Standard
A less awkward notation for these two kinds of vectors would be nice; however,
 for current purposes, it is enough to distinguish them with superscripts
 D and C: 
\emph on
viz
\emph default
.
 write 
\begin_inset Formula $\vec{v}_{ran}^{D}$
\end_inset

 for the disjunct-based vector, and 
\begin_inset Formula $\vec{v}_{home}^{C}$
\end_inset

 for the connector-based vector.
 These two vectors can be thought of as inhabiting two orthogonal subspaces
 of 
\begin_inset Formula $\vec{V}^{D}\otimes\vec{V}^{C}$
\end_inset

; there are many interesting relationships between these subspaces, they
 can be viewed as sections of a sheaf of a graph.
 
\end_layout

\begin_layout Standard
The correct conceptual model for the observational data is that of a large
 network graph, with observation counts attached to each vertex.
 This network graph can be understood as a sheaf (see reference).
 The above examples show how certain sections of that graph can be made
 to correspond to vectors.
 Obviously, these vectors are not independent of one-another, as they are
 all different slices through the same dataset.
 Rather, they provide a local, linear view of the language graph, reminiscent
 of the tangent-space of a manifold.
 
\end_layout

\begin_layout Subsection*
N-Grams, Skip Grams
\end_layout

\begin_layout Standard
The primary task set forth in N-gram and SkipGram papers (such as 
\begin_inset CommandInset citation
LatexCommand cite
key "Mikolov2013"

\end_inset

) is that of estimating the next words in a text, given all the previous
 ones.
 More precisely, one is interested in the probability 
\begin_inset Formula 
\[
p\left(w_{t}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

of observing a word 
\begin_inset Formula $w_{t}$
\end_inset

, at location 
\begin_inset Formula $t$
\end_inset

 in the text, given a local context of 
\begin_inset Formula $N-1=2c$
\end_inset

 surrounding words in the text.
 This probability can be obtained by observing text, and attempting to maximize
 the utility function (objective function)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi=\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\le j\le c;j\ne0}\log p\left(w_{t+j}\left|w_{t}\right.\right)
\]

\end_inset

on the training text, consisting of 
\begin_inset Formula $T$
\end_inset

 words in sequential order.
\end_layout

\begin_layout Standard
The SkipGram model defines the probability as the 
\begin_inset Quotes eld
\end_inset

softmax
\begin_inset Quotes erd
\end_inset

 function
\begin_inset Formula 
\[
p\left(w_{O}\left|w_{I}\right.\right)=\frac{\exp\left(\left.v_{w_{O}}^{\prime}\right.^{\intercal}v_{w_{I}}\right)}{\sum_{w=1}^{W}\exp\left(\left.v_{w}^{\prime}\right.^{\intercal}v_{w_{I}}\right)}
\]

\end_inset

 
\end_layout

\begin_layout Standard
Basically the input vector is like the one, and the output vector is like
 the other.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
