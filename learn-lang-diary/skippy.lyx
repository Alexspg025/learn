#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Skip-grams vs.
 Disjuncts
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Date
6 July 2018
\end_layout

\begin_layout Abstract
This document attempts to compare and contrast the various ways in which
 (adaptive)-skip-grams and disjuncts are similar, and how they differ.
 The comparison is not always obvious, as these two representations seem
 to be quite different, the goals and tasks that each are solving seem to
 be different, and the methods to obtain each seem to be quite different.
 In fact, many of these differences are superficial; there is more in common
 than there might seem.
 
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
The introduction here is minimal; it is assumed that the reader is generally
 conversant with both Link Grammar
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"

\end_inset

, and with SkipGram
\begin_inset CommandInset citation
LatexCommand cite
key "Mikolov2013,Mikolov2013a"

\end_inset

 or AdaGram, as well as some of the concepts employed in the OpenCog language-le
arning project.
\end_layout

\begin_layout Subsection*
Link Grammar
\end_layout

\begin_layout Standard
Link Grammar defines a word-disjunct pair as an object of the general form
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	word: A- & B+ & C+ & ...;
\end_layout

\end_inset

The disjunct is the expression to the right of the colon.
 The notations A-, B+, C+, etc.
 are called connectors; they are types that indicate what other (classes
 of) words the given word can connect to.
 The minus and plus signs indicate whether the connectors link to the left
 or to the right.
 The number of such connectors is variable, depending on the disjunct.
 The ampersand serves as a reminder that, uring parsing, all connectors
 must be satisfied; that is, the connectors are conjoined.
 A given word can have multiple disjuncts associated with it, these are
 disjoined from one-another, thus the name 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
For the language learning project, it is convenient to extend the above
 to the notion of a pseudo-disjunct, where the connector types are replaced
 by instances of individual words.
 For example
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: girl- & home+;
\end_layout

\end_inset

is used to represent the grammatical structure of the sentence 
\begin_inset Quotes eld
\end_inset

the girl ran home
\begin_inset Quotes erd
\end_inset

, namely, that the verb 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 can attach to the word 
\begin_inset Quotes eld
\end_inset

girl
\begin_inset Quotes erd
\end_inset

 (the subject) on the left, and the word 
\begin_inset Quotes eld
\end_inset

home
\begin_inset Quotes erd
\end_inset

 (the object) on the right.
 An early goal of the language learning project is to automatically discern
 such pseudo-disjuncts; a later goal is to automatically classify such individua
l-word connectors into word-classes, and so generalizing individual words
 into connector types, just as in traditional Link Grammar.
\end_layout

\begin_layout Standard
The primary learning mechanism is to accumulate observation counts of different
 disjuncts, thus leading naturally to the idea of word-vectors.
 For example, one might observe the vector 
\begin_inset Formula $\vec{v}_{ran}$
\end_inset

 represented as
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: 3(girl- & home+) + 2(girl- & away+);
\end_layout

\end_inset

that might naturally arise if the sentence 
\begin_inset Quotes eld
\end_inset

the girl ran home
\begin_inset Quotes erd
\end_inset

 was observed three times, and 
\begin_inset Quotes eld
\end_inset

the girl ran away
\begin_inset Quotes erd
\end_inset

 was observed twice.
 Such vectors can be used to judge word-similarity.
 For example, a different vector 
\begin_inset Formula $\vec{v}_{walked}$
\end_inset

 represented as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	walked: 2(girl- & home+) + 3(girl- & away+);
\end_layout

\end_inset

suggests that the cosine-product 
\begin_inset Formula $\cos\left(\vec{v}_{ran},\vec{v}_{walked}\right)$
\end_inset

 between the two might be used to judge word-similarity: 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

walked
\begin_inset Quotes erd
\end_inset

 can be used in syntactically similar ways.
\end_layout

\begin_layout Standard
The disjunct representation also allows other kinds of vectors, such as
 those anchored on connectors.
 Using the examples above, one also has a vector for the word 
\begin_inset Quotes eld
\end_inset

home
\begin_inset Quotes erd
\end_inset

, which can be awkwardly written as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	3(ran: girl- & home+) + 2(walked: girl- & home+)
\end_layout

\end_inset

Note that the counts, here, of 3 and 2, are identical to the counts above:
 all of these counts are derived from the same observational dataset.
 What differs is the choice of the attachment-point for which the vector
 is to be formed.
\end_layout

\begin_layout Standard
A less awkward notation for these two kinds of vectors would be nice; however,
 for current purposes, it is enough to distinguish them with superscripts
 D and C: 
\emph on
viz
\emph default
.
 write 
\begin_inset Formula $\vec{v}_{ran}^{D}$
\end_inset

 for the disjunct-based vector, and 
\begin_inset Formula $\vec{v}_{home}^{C}$
\end_inset

 for the connector-based vector.
 These two vectors can be thought of as inhabiting two orthogonal subspaces
 of 
\begin_inset Formula $\vec{V}^{D}\otimes\vec{V}^{C}$
\end_inset

; there are many interesting relationships between these subspaces, they
 can be viewed as sections of a sheaf of a graph.
 
\end_layout

\begin_layout Standard
The correct conceptual model for the observational data is that of a large
 network graph, with observation counts attached to each vertex.
 This network graph can be understood as a sheaf (see reference).
 The above examples show how certain sections of that graph can be made
 to correspond to vectors.
 Obviously, these vectors are not independent of one-another, as they are
 all different slices through the same dataset.
 Rather, they provide a local, linear view of the language graph, reminiscent
 of the tangent-space of a manifold.
 
\end_layout

\begin_layout Subsection*
Statistical Models, N-grams, Skip Grams
\end_layout

\begin_layout Standard
The task of langauge learning is commonly taken to be one of estimating
 the probability of a text, consisting of a sequence of words.
 One common model assumes that the probability of the text can be approximated
 by the product of the conditional probabilities of individual words, and
 specifically, of how each word conditionally depends on all of the previous
 ones:
\begin_inset CommandInset citation
LatexCommand cite
key "Bengio2003"

\end_inset


\begin_inset Formula 
\[
\widehat{P}\left(w_{1}^{T}\right)=\prod_{t=1}^{T}P\left(w_{t}\left|w_{1}^{t-1}\right.\right)
\]

\end_inset

Here, the text is presumed to consist of 
\begin_inset Formula $T$
\end_inset

 words 
\begin_inset Formula $w_{t}$
\end_inset

 occuring in sequential order.
 The notation 
\begin_inset Formula $w_{i}^{n}$
\end_inset

 is used to denote a sequence of words, that is, 
\begin_inset Formula $w_{i}^{n}=\left(w_{i},w_{i+1},\cdots,w_{n}\right)$
\end_inset

.
 Thus, the text as a whole is denoted by 
\begin_inset Formula $w_{1}^{T}$
\end_inset

, and so 
\begin_inset Formula $\widehat{P}\left(w_{1}^{T}\right)$
\end_inset

 is an approximate model for the probability 
\begin_inset Formula $P\left(w_{1}^{T}\right)$
\end_inset

 of observing the text (the carat over 
\begin_inset Formula $P$
\end_inset

 serving to remind that approximations are being made; that the model is
 an approximation for the 
\begin_inset Quotes eld
\end_inset

true
\begin_inset Quotes erd
\end_inset

 probability.)
\end_layout

\begin_layout Standard
Although this statistical model is commonly taken as gospel, it is, of course,
 wrong: we know, a priori, that sentences are constructed whole before being
 written, and so the current word also depends on future words, ones that
 follow it in the text.
 This is the case not just at the sentence-level, but also at the level
 of the entire text, as the writer already has a theme in mind.
 To estimate the probability of a word at a given location, one must look
 at words to both the left and right of the given location.
\end_layout

\begin_layout Standard
At any rate, for 
\begin_inset Formula $T$
\end_inset

 greater than a few dozen words, the above becomes computationally intractable,
 and so instead one approximates the conditional probabilities by limiting
 the word-sequence to a sliding window of length 
\begin_inset Formula $N$
\end_inset

.
 It is convenient, at this point, to also allow words on the left, as well
 as those on the right, to determine the conditional probability.
 Following Mikolov
\begin_inset CommandInset citation
LatexCommand cite
key "Mikolov2013"

\end_inset

, write the probability 
\begin_inset Formula 
\[
p\left(w_{t}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

of observing a word 
\begin_inset Formula $w_{t}$
\end_inset

, at location 
\begin_inset Formula $t$
\end_inset

 in the text, as being conditioned on a local context (sliding window) of
 
\begin_inset Formula $N-1=2c$
\end_inset

 surrounding words, to the left and right, in the text.
 The probability of the text is then modelled by
\begin_inset Formula 
\[
\widehat{P}\left(w_{1}^{T}\right)=\prod_{t=1}^{T}p\left(w_{t}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

The smaller window does make the computation more tractable.
 
\end_layout

\begin_layout Standard
The above contains another key simplifcation: the total probability is assumed
 to factor into the product of single-word probabilities, and each single-word
 probability is translationally invariant; that is, the probability has
 no explicit dependence on the index 
\begin_inset Formula $t$
\end_inset

.
 This is commonly taken to be a reasonable simplification, but is, of course,
 
\begin_inset Quotes eld
\end_inset

obviously
\begin_inset Quotes erd
\end_inset

 wrong.
 The words at the end of a text occur with different probabilities than
 those at the beginning; for example, in a dramatic story, a new character
 may appear mid-way, or the setting may move from indoors to outdoors, with
 furniture-words becoming uncommon, while nature-words becoming common.
 The translational invariance only becomes plausible in the limit of 
\begin_inset Formula $T\to\infty$
\end_inset

 where one is considering 
\begin_inset Quotes eld
\end_inset

all human language
\begin_inset Quotes erd
\end_inset

.
 This too, is preposterous; first, because not everything that can be said
 has been said; second, because different individuals speak differently,
 and third, because new words are invented regularly, as others become archaic.
 Despite all this, the assumption of translational invariance is entirely
 appropriate at this level.
\end_layout

\begin_layout Standard
In principle, this probability can be obtained by observing text, and attempting
 to maximize the utility function (objective function)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi=\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\le j\le c;j\ne0}\log p\left(w_{t+j}\left|w_{t}\right.\right)
\]

\end_inset

on the training text.
\end_layout

\begin_layout Standard
The above formulas define the 
\begin_inset Quotes eld
\end_inset

classic
\begin_inset Quotes erd
\end_inset

 N-gram model, in that there is a sliding window of 
\begin_inset Formula $N$
\end_inset

 words in width, and one is using all of those words to make a prediction.
 Because of the combinatorial explosion in the size of the vocabulary, 
\begin_inset Formula $N$
\end_inset

 is usually kept small: 
\begin_inset Formula $N=3$
\end_inset

 (trigrams) or 
\begin_inset Formula $N=5$
\end_inset

.
 That is, for a vocabulary of 
\begin_inset Formula $W$
\end_inset

 words, there are 
\begin_inset Formula $W^{N}$
\end_inset

 probabilities 
\begin_inset Formula $p$
\end_inset

 that must be computed (trained) and remembered.
 For 
\begin_inset Formula $W=10^{4}$
\end_inset

 and 
\begin_inset Formula $N=3$
\end_inset

, this requires 
\begin_inset Formula $W^{N}=10^{12}$
\end_inset

 probabilities to be maintained: clearly beyond the scope of present-day
 computers.
\end_layout

\begin_layout Standard
To avoid the combinatorial explosion, several tricks are used.
 One is to understand that not all of the 
\begin_inset Formula $N$
\end_inset

 words in the context are particularly relevant to the prediction, so that
 one can still make a good prediction while skipping some of them: this
 is the SkipGram model.
 The next obvious trick is to realize that the context size 
\begin_inset Formula $N$
\end_inset

 itself can be dynamically varied; this is the Adaptive SkipGram or AdaGram
 model.
\end_layout

\begin_layout Subsection*
Neural Nets
\end_layout

\begin_layout Standard
Bengio
\begin_inset CommandInset citation
LatexCommand cite
key "Bengio2003"

\end_inset

 proposes two different kinds of neural net models.
 One is used to approximate the conditional probability
\begin_inset Formula 
\[
p\left(w_{t}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

of observing the word 
\begin_inset Formula $w_{t}$
\end_inset

, given the context of surrounding words.
 The other, and more interesting model approximates the conditional probability
 
\begin_inset Formula 
\[
p\left(w_{O}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

of observing a set of words 
\begin_inset Formula $w_{O}$
\end_inset

 of which 
\begin_inset Formula $w_{t}$
\end_inset

 is a member (that is, 
\begin_inset Formula $w_{t}\in w_{O}$
\end_inset

).
 The set 
\begin_inset Formula $w_{O}$
\end_inset

 is to be thought of as a class of words having similar semantic (and probably
 syntactic) qualities.
\end_layout

\begin_layout Standard
The proposed model uses a single hidden-layer neural net, a softmax output
 function, represents each input word with a vector, and represents the
 output class as a vector as well.
 Some effort is needed to unpack these ideas into workable notation.
\end_layout

\begin_layout Standard
First, one represents each (input) word 
\begin_inset Formula $w$
\end_inset

 with an 
\begin_inset Quotes eld
\end_inset

input
\begin_inset Quotes erd
\end_inset

 vector 
\begin_inset Formula $\vec{v}_{w}$
\end_inset

.
 This is sometimes called the 
\begin_inset Quotes eld
\end_inset

projection
\begin_inset Quotes erd
\end_inset

 or the 
\begin_inset Quotes eld
\end_inset

dimensional reduction
\begin_inset Quotes erd
\end_inset

 of the larger vocabulary of 
\begin_inset Formula $W$
\end_inset

 words to a smaller 
\begin_inset Quotes eld
\end_inset

projection space
\begin_inset Quotes erd
\end_inset

 of dimension 
\begin_inset Formula $D\ll W$
\end_inset

.
 The projection, 
\emph on
viz
\emph default
, the value of the vector components, are determined during training.
 Commonly used values for 
\begin_inset Formula $D$
\end_inset

 are in the range of 50--300; by contrast, typical vocabulary sizes 
\begin_inset Formula $W$
\end_inset

 range from 
\begin_inset Formula $10^{4}$
\end_inset

 to 
\begin_inset Formula $10^{6}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The context of words is then represented as a product vector
\begin_inset Formula 
\[
\vec{v}_{I}=\vec{v}_{t-c}\times\vec{v}_{t-c+1}\times\cdots\times\vec{v}_{t+c}
\]

\end_inset

where 
\begin_inset Formula $\vec{v}_{k}\equiv\vec{v}_{w_{k}}$
\end_inset

abuses the notation to pull up the subscript.
 By another abuse of notation, it is convenient to drop to location index
 
\begin_inset Formula $t$
\end_inset

, since the assumption of decomposition into products means that the probabiliti
es are translationally invariant.
 The dimension of 
\begin_inset Formula $\vec{v}_{I}$
\end_inset

 is then 
\begin_inset Formula $ND$
\end_inset

, where, as before 
\begin_inset Formula $N$
\end_inset

 is the size of the context.
\end_layout

\begin_layout Standard
The input vector 
\begin_inset Formula $\vec{v}_{I}$
\end_inset

 is then sent to a 
\begin_inset Quotes eld
\end_inset

hidden
\begin_inset Quotes erd
\end_inset

 feed-forward (perceptron) neural layer, by means of a weight matrix 
\begin_inset Formula $H$
\end_inset

 and an offset vector 
\begin_inset Formula $\vec{d}$
\end_inset

.
 Taking 
\begin_inset Formula $h$
\end_inset

 as the number of hidden-layer neurons, the matrix 
\begin_inset Formula $H$
\end_inset

 has dimension 
\begin_inset Formula $h\times ND$
\end_inset

 and 
\begin_inset Formula $\vec{d}$
\end_inset

 has dimension 
\begin_inset Formula $h$
\end_inset

.
 The outputs of the hidden layer are passed through a sigmoid function 
\begin_inset Formula $\sigma\left(x\right)$
\end_inset

 (which can be 
\begin_inset Formula $\tanh x$
\end_inset

 or 
\begin_inset Formula $1/\left(1+e^{-x}\right)$
\end_inset

 or similar, according to taste) on a component-by-component basis.
 The result is a vector 
\begin_inset Formula $\vec{s}$
\end_inset

 whose vector components are given by 
\begin_inset Formula 
\[
\vec{s}=\sigma\left(H\vec{v}+\vec{d}\right)
\]

\end_inset

with the understanding that the sigmoid is applied to each compoent of 
\begin_inset Formula $H\vec{v}+\vec{d}$
\end_inset

; so that 
\begin_inset Formula $\vec{s}$
\end_inset

 is an 
\begin_inset Formula $h$
\end_inset

-dimensional vector.
 This is multiplied by an 
\begin_inset Quotes eld
\end_inset

output
\begin_inset Quotes erd
\end_inset

 vector 
\begin_inset Formula $\vec{v}_{O}^{\prime}$
\end_inset

 of dimension 
\begin_inset Formula $h$
\end_inset

 to given a scalar 
\begin_inset Quotes eld
\end_inset

energy functional
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula 
\[
E\left(\vec{v}_{I};\vec{v}_{O}^{\prime}\right)=\vec{v}_{O}^{\prime}\cdot\vec{s}
\]

\end_inset

This is a single real-valued number.
 The final probability is modeled by the 
\begin_inset Quotes eld
\end_inset

softmax
\begin_inset Quotes erd
\end_inset

 function
\begin_inset Formula 
\[
p\left(w_{O}\left|w_{I}\right.\right)=\frac{\exp\, E\left(\vec{v}_{I};\vec{v}_{O}^{\prime}\right)}{\sum_{j}\exp\, E\left(\vec{v}_{I};\vec{v}_{j}^{\prime}\right)}
\]

\end_inset

with the sum over 
\begin_inset Formula $j$
\end_inset

 running over all output word classes.
\end_layout

\begin_layout Standard
Training of the model can be done by hill-climbing, that is, by computing
 the local gradient
\begin_inset Formula 
\[
\nabla p\left(w_{O}\left|w_{I}\right.\right)
\]

\end_inset

and then adjusting parameters so as to follow the gradient uphill (
\emph on
viz
\emph default
 maximizing the probability).
\end_layout

\begin_layout Section*
Similarities and Differences
\end_layout

\begin_layout Standard
On the face of it, the descriptions given in the above two sections appear
 to be completely different.
 The similarities are obscured by the notation.
\end_layout

\begin_layout Subsection*
Disjuncts as Context
\end_layout

\begin_layout Standard
First, let us unpack the probability
\begin_inset Formula 
\[
p\left(w_{t}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

The above notation is meant to indicate the probability of observing the
 word 
\begin_inset Formula $w_{t}$
\end_inset

, given 
\begin_inset Formula $c$
\end_inset

 words that occur before it, and 
\begin_inset Formula $c$
\end_inset

 words that occur after it.
 Let 
\begin_inset Formula $c=1$
\end_inset

 and let 
\begin_inset Formula $w_{t}=ran$
\end_inset

, 
\begin_inset Formula $w_{t-1}=girl$
\end_inset

 and 
\begin_inset Formula $w_{t+1}=home$
\end_inset

.
 This clearly resembles the Link Grammar disjunct 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: girl- & home+;
\end_layout

\end_inset

One difference is the Link Grammar disjunct notation does not provide any
 location at which to attach a probability.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The Link Grammar software does provide a device, called the 
\begin_inset Quotes eld
\end_inset

cost
\begin_inset Quotes erd
\end_inset

, which is an additive floating point number that represents the penalty
 of using a particular disjunct.
 It can be thought of as being the same thing as 
\begin_inset Formula $-\log p\left(w|d\right)$
\end_inset

.
 The hand-crafted dictionaries provide hand-crafted estimates for this cost/log-
likelihood.
\end_layout

\end_inset

 This can be remedied in a straight-forward manner: write 
\begin_inset Formula $d$
\end_inset

 for the disjunct, 
\begin_inset Formula $girl-\&home+$
\end_inset

 in this example.
 One can then define the probability
\begin_inset Formula 
\[
p\left(w|d\right)=\frac{p\left(w,d\right)}{p\left(*,d\right)}
\]

\end_inset

 where 
\begin_inset Formula $p\left(w,d\right)$
\end_inset

 is the probability of observing the pair 
\begin_inset Formula $\left(w,d\right)$
\end_inset

, while 
\begin_inset Formula 
\[
p\left(*,d\right)=\sum_{w=1}^{W}p\left(w,d\right)
\]

\end_inset

is simply the sum over all words in the vocabulary.
 
\end_layout

\begin_layout Standard
The resemblance, at this point, should be obvious: the disjunct 
\begin_inset Formula $d$
\end_inset

 plays the role of the N-gram context.
 Abusing some notation, one might think that
\begin_inset Formula 
\[
d\approx w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}
\]

\end_inset

and the point of this section is that this approximate equivalence is not
 a bad way of understanding the situation.
 
\end_layout

\begin_layout Standard
Less obvious, perhaps, is that the disjunct is already capturing aspects
 of both skip-grams and adaptive N-grams.
 That is, disjuncts are intended to capture the dependency grammar description
 of a language.
 A dependency grammar naturally 
\begin_inset Quotes eld
\end_inset

skips
\begin_inset Quotes erd
\end_inset

 over words, and 
\begin_inset Quotes eld
\end_inset

adaptively
\begin_inset Quotes erd
\end_inset

 sizes the context to be appropriate.
 Consider the dependency parse of 
\begin_inset Quotes eld
\end_inset

The girl, upset by the taunting, ran home in tears.
\begin_inset Quotes erd
\end_inset

 There are four words, and two punctuation symbols separating the word 
\begin_inset Quotes eld
\end_inset

girl
\begin_inset Quotes erd
\end_inset

 from the word 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

.
 Dependency grammars do not have any difficulty in arranging for the attachement
 of the words 
\begin_inset Quotes eld
\end_inset

girl--ran
\begin_inset Quotes erd
\end_inset

, skipping over the post-nominal modifier phrase 
\begin_inset Quotes eld
\end_inset

upset by the taunting
\begin_inset Quotes erd
\end_inset

, which attaches to the noun, and not the verb: it's the girl who is upset,
 not the running.
 Such long-distance attachements are problematic for adaptive skip-grams:
 
\begin_inset Formula $N$
\end_inset

 must be quite very large to skip over the post-nominal modifier.
 
\end_layout

\begin_layout Standard
Similarly, dependency grammars are 
\begin_inset Quotes eld
\end_inset

adaptive
\begin_inset Quotes erd
\end_inset

 by design: verbs tend to have more attachments that nouns, which have more
 attachments than determiners or adjectives.
 That is, dependency grammars already 
\begin_inset Quotes eld
\end_inset

know
\begin_inset Quotes erd
\end_inset

 that the correct size of the context for determiners and adjectives is
 one: an adjective can modify only one noun.
\end_layout

\begin_layout Standard
One important difference should be immediately apparent: the Link Grammar
 probabilities are obtained by direct counting, and not by any training,
 relaxation or hill-climbing technique.
 That is, 
\begin_inset Formula 
\[
p\left(w,d\right)=\frac{N\left(w,d\right)}{N\left(*,*\right)}
\]

\end_inset

so that the probability is properly normalized: 
\begin_inset Formula $p\left(*,*\right)=1$
\end_inset

.
 The 
\begin_inset Formula $N\left(w,d\right)$
\end_inset

 is just the observational count of observing the pair 
\begin_inset Formula $\left(w,d\right)$
\end_inset

 in text; the probabilities is just the frequentist probability.
 This differs from the probability in N-gram/skipgram models, which is obtained
 by maximizing an objective function built from (defined in terms of) the
 probabilities.
\end_layout

\begin_layout Subsection*
Vectors
\end_layout

\begin_layout Standard
Consider the vector product
\begin_inset Formula 
\[
\left.v_{w_{O}}^{\prime}\right.^{\intercal}v_{w_{I}}
\]

\end_inset


\end_layout

\begin_layout Standard
Basically the input vector is like the one, and the output vector is like
 the other.
\end_layout

\begin_layout Section*
Generalization
\end_layout

\begin_layout Standard
The language-learning task requires one to infer the structure of language
 from a small number of instances and examples.
 Bengio 
\emph on
etal.
\begin_inset CommandInset citation
LatexCommand cite
key "Bengio2003"

\end_inset


\emph default
 describe this for continuous probabilistic models.
 First, one imagines some continuous, uniform space.
 Example sentences form a training corpus are associated with single points
 in this space: the probability mass is initially located at a collection
 of points.
 One then imagines that generalization consists of smearing out those points
 over an extended volume, thereby assigning non-zero probability weights
 to other 
\begin_inset Quotes eld
\end_inset

nearby
\begin_inset Quotes erd
\end_inset

 sentences.
 This suggests that there is a choice as to how this smearing-out is done:
 one can spread the probabilities uniformly, in all 
\begin_inset Quotes eld
\end_inset

directions
\begin_inset Quotes erd
\end_inset

, or one can preferentially spread probabilities only along certain directions.
 Bengio suggests that higher-quality learning and generalization can be
 acheived by finding and appropriately non-uniform way of smearing the probabili
ty masses from training.
\end_layout

\begin_layout Standard
This description seems like a useful and harmless way of guiding one's thoughts.
 But it leaves open and vague several undefined concepts: that of the 
\begin_inset Quotes eld
\end_inset

space
\begin_inset Quotes erd
\end_inset

: is this some topolgical space, perhaps linear, or something else? That
 of 
\begin_inset Quotes eld
\end_inset

nearby sentences
\begin_inset Quotes erd
\end_inset

: the presumption (the axiom?) that the space is endowed with a metric that
 measures distances.
 Finally, the concept of 
\begin_inset Quotes eld
\end_inset

direction
\begin_inset Quotes erd
\end_inset

, or at least, a local tangent manifold at each point of the space.
 It seems reasonable to assert that langauge lives on a manifold, but then,
 the structure of that manifold needs to be elucidated and demonstrated.
 In particular, the 
\begin_inset Quotes eld
\end_inset

non-uniform spreading
\begin_inset Quotes erd
\end_inset

 of probability weights suggests confusion or inconsistency: Perhaps the
 spreading appears to be non-uniform, because the initial metric assigned
 to the space is incorrect? In geometry, one usually works with normalized
 tangent vectors, so that when one extends them to geodesics, each geodesic
 moves with unit velocity.
 It seems plausible to spread out probability weights the same way: spread
 them uniformly, and adjust the shape of the underlying space so that this
 results in a high-quality langauge model.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
