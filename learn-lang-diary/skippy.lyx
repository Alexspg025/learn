#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Skip-grams vs.
 Disjuncts
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Date
6 July 2018
\end_layout

\begin_layout Abstract
This document attempts to compare and contrast the various ways in which
 (adaptive)-skip-grams and disjuncts are similar, and how they differ.
 The comparison is not always obvious, as these two representations seem
 to be quite different, the goals and tasks that each are solving seem to
 be different, and the methods to obtain each seem to be quite different.
 In fact, many of these differences are superficial; there is more in common
 than there might seem.
 
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
The introduction here is minimal; it is assumed that the reader is generally
 conversant with both Link Grammar
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"

\end_inset

, and with SkipGram
\begin_inset CommandInset citation
LatexCommand cite
key "Mikolov2013,Mikolov2013a"

\end_inset

 or AdaGram, as well as some of the concepts employed in the OpenCog language-le
arning project.
\end_layout

\begin_layout Subsection*
Link Grammar
\end_layout

\begin_layout Standard
Link Grammar defines a word-disjunct pair as an object of the general form
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	word: A- & B+ & C+ & ...;
\end_layout

\end_inset

The disjunct is the expression to the right of the colon.
 The notations A-, B+, C+, etc.
 are called connectors; they are types that indicate what other (classes
 of) words the given word can connect to.
 The minus and plus signs indicate whether the connectors link to the left
 or to the right.
 The number of such connectors is variable, depending on the disjunct.
 The ampersand serves as a reminder that, uring parsing, all connectors
 must be satisfied; that is, the connectors are conjoined.
 A given word can have multiple disjuncts associated with it, these are
 disjoined from one-another, thus the name 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
For the language learning project, it is convenient to extend the above
 to the notion of a pseudo-disjunct, where the connector types are replaced
 by instances of individual words.
 For example
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: girl- & home+;
\end_layout

\end_inset

is used to represent the grammatical structure of the sentence 
\begin_inset Quotes eld
\end_inset

the girl ran home
\begin_inset Quotes erd
\end_inset

, namely, that the verb 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 can attach to the word 
\begin_inset Quotes eld
\end_inset

girl
\begin_inset Quotes erd
\end_inset

 (the subject) on the left, and the word 
\begin_inset Quotes eld
\end_inset

home
\begin_inset Quotes erd
\end_inset

 (the object) on the right.
 An early goal of the language learning project is to automatically discern
 such pseudo-disjuncts; a later goal is to automatically classify such individua
l-word connectors into word-classes, and so generalizing individual words
 into connector types, just as in traditional Link Grammar.
\end_layout

\begin_layout Standard
The primary learning mechanism is to accumulate observation counts of different
 disjuncts, thus leading naturally to the idea of word-vectors.
 For example, one might observe the vector 
\begin_inset Formula $\vec{v}_{ran}$
\end_inset

 represented as
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: 3(girl- & home+) + 2(girl- & away+);
\end_layout

\end_inset

that might naturally arise if the sentence 
\begin_inset Quotes eld
\end_inset

the girl ran home
\begin_inset Quotes erd
\end_inset

 was observed three times, and 
\begin_inset Quotes eld
\end_inset

the girl ran away
\begin_inset Quotes erd
\end_inset

 was observed twice.
 Such vectors can be used to judge word-similarity.
 For example, a different vector 
\begin_inset Formula $\vec{v}_{walked}$
\end_inset

 represented as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	walked: 2(girl- & home+) + 3(girl- & away+);
\end_layout

\end_inset

suggests that the cosine-product 
\begin_inset Formula $\cos\left(\vec{v}_{ran},\vec{v}_{walked}\right)$
\end_inset

 between the two might be used to judge word-similarity: 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

walked
\begin_inset Quotes erd
\end_inset

 can be used in syntactically similar ways.
\end_layout

\begin_layout Standard
The disjunct representation also allows other kinds of vectors, such as
 those anchored on connectors.
 Using the examples above, one also has a vector for the word 
\begin_inset Quotes eld
\end_inset

home
\begin_inset Quotes erd
\end_inset

, which can be awkwardly written as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	3(ran: girl- & home+) + 2(walked: girl- & home+)
\end_layout

\end_inset

Note that the counts, here, of 3 and 2, are identical to the counts above:
 all of these counts are derived from the same observational dataset.
 What differs is the choice of the attachment-point for which the vector
 is to be formed.
\end_layout

\begin_layout Standard
A less awkward notation for these two kinds of vectors would be nice; however,
 for current purposes, it is enough to distinguish them with superscripts
 D and C: 
\emph on
viz
\emph default
.
 write 
\begin_inset Formula $\vec{v}_{ran}^{D}$
\end_inset

 for the disjunct-based vector, and 
\begin_inset Formula $\vec{v}_{home}^{C}$
\end_inset

 for the connector-based vector.
 These two vectors can be thought of as inhabiting two orthogonal subspaces
 of 
\begin_inset Formula $\vec{V}^{D}\otimes\vec{V}^{C}$
\end_inset

; there are many interesting relationships between these subspaces, they
 can be viewed as sections of a sheaf of a graph.
 
\end_layout

\begin_layout Standard
The correct conceptual model for the observational data is that of a large
 network graph, with observation counts attached to each vertex.
 This network graph can be understood as a sheaf (see reference).
 The above examples show how certain sections of that graph can be made
 to correspond to vectors.
 Obviously, these vectors are not independent of one-another, as they are
 all different slices through the same dataset.
 Rather, they provide a local, linear view of the language graph, reminiscent
 of the tangent-space of a manifold.
 
\end_layout

\begin_layout Subsection*
N-Grams, Skip Grams
\end_layout

\begin_layout Standard
The primary task set forth in N-gram and SkipGram papers (such as 
\begin_inset CommandInset citation
LatexCommand cite
key "Mikolov2013"

\end_inset

) is that of estimating the next words in a text, given all the previous
 ones.
 More precisely, one is interested in the probability 
\begin_inset Formula 
\[
p\left(w_{t}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

of observing a word 
\begin_inset Formula $w_{t}$
\end_inset

, at location 
\begin_inset Formula $t$
\end_inset

 in the text, given a local context of 
\begin_inset Formula $N-1=2c$
\end_inset

 surrounding words in the text.
 This probability can be obtained by observing text, and attempting to maximize
 the utility function (objective function)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi=\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\le j\le c;j\ne0}\log p\left(w_{t+j}\left|w_{t}\right.\right)
\]

\end_inset

on the training text, consisting of 
\begin_inset Formula $T$
\end_inset

 words in sequential order.
\end_layout

\begin_layout Standard
The above formulas define the 
\begin_inset Quotes eld
\end_inset

classic
\begin_inset Quotes erd
\end_inset

 N-gram model, in that there is a sliding window of 
\begin_inset Formula $N$
\end_inset

 words in width, and one is using all of those words to make a prediction.
 Because of the combinatorial explosion in the size of the vocabulary, 
\begin_inset Formula $N$
\end_inset

 is usually kept small: 
\begin_inset Formula $N=3$
\end_inset

 (trigrams) or 
\begin_inset Formula $N=5$
\end_inset

.
 That is, for a vocabulary of 
\begin_inset Formula $W$
\end_inset

 words, there are 
\begin_inset Formula $W^{N}$
\end_inset

 probabilities 
\begin_inset Formula $p$
\end_inset

 that must be computed (trained) and remembered.
 For 
\begin_inset Formula $W=10^{4}$
\end_inset

 and 
\begin_inset Formula $N=3$
\end_inset

, this requires 
\begin_inset Formula $W^{N}=10^{12}$
\end_inset

 probabilities to be maintained: clearly beyond the scope of present-day
 computers.
\end_layout

\begin_layout Standard
To avoid the combinatorial explosion, several tricks are used.
 One is to understand that not all of the 
\begin_inset Formula $N$
\end_inset

 words in the context are particularly relevant to the prediction, so that
 one can still make a good prediction while skipping some of them: this
 is the SkipGram model.
 The next obvious trick is to realize that the context size 
\begin_inset Formula $N$
\end_inset

 itself can be dynamically varied; this is the Adaptive SkipGram or AdaGram
 model.
\end_layout

\begin_layout Standard
The SkipGram model defines the probability as the 
\begin_inset Quotes eld
\end_inset

softmax
\begin_inset Quotes erd
\end_inset

 function
\begin_inset Formula 
\[
p\left(w_{O}\left|w_{I}\right.\right)=\frac{\exp\left(\left.v_{w_{O}}^{\prime}\right.^{\intercal}v_{w_{I}}\right)}{\sum_{w=1}^{W}\exp\left(\left.v_{w}^{\prime}\right.^{\intercal}v_{w_{I}}\right)}
\]

\end_inset

Here, 
\begin_inset Formula $w_{I}$
\end_inset

 is an input context of words, 
\begin_inset Formula $w_{O}$
\end_inset

 is a set of words one is trying to predict, 
\begin_inset Formula $v_{w}^{\prime}$
\end_inset

 and 
\begin_inset Formula $v_{w}$
\end_inset

 are two different vectors, the 
\begin_inset Quotes eld
\end_inset

output
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

input
\begin_inset Quotes erd
\end_inset

 vectors, defined by the SkipGram model.
 The sum in the denominator ranges over 
\begin_inset Formula $W$
\end_inset

 total words in the vocabulary.
\end_layout

\begin_layout Standard
Training of the softmax model can be done by hill-climbing, that is, by
 computing the local gradient
\begin_inset Formula 
\[
\nabla p\left(w_{O}\left|w_{I}\right.\right)
\]

\end_inset

and then adjusting parameters so as to follow the gradient uphill (viz maximizin
g the probability).
\end_layout

\begin_layout Section*
Similarities and Differences
\end_layout

\begin_layout Standard
On the face of it, the descriptions given in the above two sections appear
 to be completely different.
 The similarities are obscured by the notation.
\end_layout

\begin_layout Subsection*
Disjuncts as Context
\end_layout

\begin_layout Standard
First, let us unpack the probability
\begin_inset Formula 
\[
p\left(w_{t}\left|w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}\right.\right)
\]

\end_inset

The above notation is meant to indicate the probability of observing the
 word 
\begin_inset Formula $w_{t}$
\end_inset

, given 
\begin_inset Formula $c$
\end_inset

 words that occur before it, and 
\begin_inset Formula $c$
\end_inset

 words that occur after it.
 Let 
\begin_inset Formula $c=1$
\end_inset

 and let 
\begin_inset Formula $w_{t}=ran$
\end_inset

, 
\begin_inset Formula $w_{t-1}=girl$
\end_inset

 and 
\begin_inset Formula $w_{t+1}=home$
\end_inset

.
 This clearly resembles the Link Grammar disjunct 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	ran: girl- & home+;
\end_layout

\end_inset

One difference is the Link Grammar disjunct notation does not provide any
 location at which to attach a probability.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The Link Grammar software does provide a device, called the 
\begin_inset Quotes eld
\end_inset

cost
\begin_inset Quotes erd
\end_inset

, which is an additive floating point number that represents the penalty
 of using a particular disjunct.
 It can be thought of as being the same thing as 
\begin_inset Formula $-\log p\left(w|d\right)$
\end_inset

.
 The hand-crafted dictionaries provide hand-crafted estimates for this cost/log-
likelihood.
\end_layout

\end_inset

 This can be remedied in a straight-forward manner: write 
\begin_inset Formula $d$
\end_inset

 for the disjunct, 
\begin_inset Formula $girl-\&home+$
\end_inset

 in this example.
 One can then define the probability
\begin_inset Formula 
\[
p\left(w|d\right)=\frac{p\left(w,d\right)}{p\left(*,d\right)}
\]

\end_inset

 where 
\begin_inset Formula $p\left(w,d\right)$
\end_inset

 is the probability of observing the pair 
\begin_inset Formula $\left(w,d\right)$
\end_inset

, while 
\begin_inset Formula 
\[
p\left(*,d\right)=\sum_{w=1}^{W}p\left(w,d\right)
\]

\end_inset

is simply the sum over all words in the vocabulary.
 
\end_layout

\begin_layout Standard
The resemblance, at this point, should be obvious: the disjunct 
\begin_inset Formula $d$
\end_inset

 plays the role of the N-gram context.
 Abusing some notation, one might think that
\begin_inset Formula 
\[
d\approx w_{t-c},\cdots w_{t-1},w_{t+1},\cdots w_{t+c}
\]

\end_inset

and the point of this section is that this approximate equivalence is not
 a bad way of understanding the situation.
 
\end_layout

\begin_layout Standard
Less obvious, perhaps, is that the disjunct is already capturing aspects
 of both skip-grams and adaptive N-grams.
 That is, disjuncts are intended to capture the dependency grammar description
 of a language.
 A dependency grammar naturally 
\begin_inset Quotes eld
\end_inset

skips
\begin_inset Quotes erd
\end_inset

 over words, and 
\begin_inset Quotes eld
\end_inset

adaptively
\begin_inset Quotes erd
\end_inset

 sizes the context to be appropriate.
 Consider the dependency parse of 
\begin_inset Quotes eld
\end_inset

The girl, upset by the taunting, ran home in tears.
\begin_inset Quotes erd
\end_inset

 There are four words, and two punctuation symbols separating the word 
\begin_inset Quotes eld
\end_inset

girl
\begin_inset Quotes erd
\end_inset

 from the word 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

.
 Dependency grammars do not have any difficulty in arranging for the attachement
 of the words 
\begin_inset Quotes eld
\end_inset

girl--ran
\begin_inset Quotes erd
\end_inset

, skipping over the post-nominal modifier phrase 
\begin_inset Quotes eld
\end_inset

upset by the taunting
\begin_inset Quotes erd
\end_inset

, which attaches to the noun, and not the verb: it's the girl who is upset,
 not the running.
 Such long-distance attachements are problematic for adaptive skip-grams:
 
\begin_inset Formula $N$
\end_inset

 must be quite very large to skip over the post-nominal modifier.
 
\end_layout

\begin_layout Standard
Similarly, dependency grammars are 
\begin_inset Quotes eld
\end_inset

adaptive
\begin_inset Quotes erd
\end_inset

 by design: verbs tend to have more attachments that nouns, which have more
 attachments than determiners or adjectives.
 That is, dependency grammars already 
\begin_inset Quotes eld
\end_inset

know
\begin_inset Quotes erd
\end_inset

 that the correct size of the context for determiners and adjectives is
 one: an adjective can modify only one noun.
\end_layout

\begin_layout Standard
One important difference should be immediately apparent: the Link Grammar
 probabilities are obtained by direct counting, and not by any training,
 relaxation or hill-climbing technique.
 That is, 
\begin_inset Formula 
\[
p\left(w,d\right)=\frac{N\left(w,d\right)}{N\left(*,*\right)}
\]

\end_inset

so that the probability is properly normalized: 
\begin_inset Formula $p\left(*,*\right)=1$
\end_inset

.
 The 
\begin_inset Formula $N\left(w,d\right)$
\end_inset

 is just the observational count of observing the pair 
\begin_inset Formula $\left(w,d\right)$
\end_inset

 in text; the probabilities is just the frequentist probability.
 This differs from the probability in N-gram/skipgram models, which is obtained
 by maximizing an objective function built from (defined in terms of) the
 probabilities.
\end_layout

\begin_layout Subsection*
Vectors
\end_layout

\begin_layout Standard
Consider the vector product
\begin_inset Formula 
\[
\left.v_{w_{O}}^{\prime}\right.^{\intercal}v_{w_{I}}
\]

\end_inset


\end_layout

\begin_layout Standard
Basically the input vector is like the one, and the output vector is like
 the other.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
