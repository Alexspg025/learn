#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url} 
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 0
\use_mhchem 0
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Abstract
Collection of thoughts and notes, pursuant to language learning.
 Organized in mostly chronological order.
\end_layout

\begin_layout Section*
31 December 2013
\end_layout

\begin_layout Standard
Easier said than done.
 Seems 'obvious' but its not.
 Goal: constraint set of link rules to model only the observed language,
 via link-grammar.
 Definitions.
 Let 
\begin_inset Formula $P(R(w_{l},w_{r}))$
\end_inset

 represent the probability (frequency) of observing two words, 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in some relationship or pattern 
\begin_inset Formula $R$
\end_inset

.
 Typically, its a (link-grammar) linkage of type 
\begin_inset Formula $t$
\end_inset

 connecting word 
\begin_inset Formula $w_{l}$
\end_inset

 on the left to word 
\begin_inset Formula $w_{r}$
\end_inset

 on the right; however, 
\begin_inset Formula $R$
\end_inset

can be more general than that.
\end_layout

\begin_layout Standard
The simplest model has only on type 
\begin_inset Formula $t$
\end_inset

, the ANY type, and assigns equal probabilities to all words.
 But we know all words are not equi-probable, so let 
\begin_inset Formula $P(w)$
\end_inset

 be the probability of observing word 
\begin_inset Formula $w$
\end_inset

.
 We know from experiece this is a Zipfian distribution.
 We are then interested in the conditional probability 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 of observing the two words 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in a relation 
\begin_inset Formula $R=R(w_{l},w_{r})$
\end_inset

, given that the two individual words were observed.
 From the definition of conditional probabilities, we get 
\begin_inset Formula 
\[
P(R)=P(R|w_{l},w_{r})P(w_{l})P(w_{r})
\]

\end_inset

Or that 
\begin_inset Formula 
\[
P(R|w_{l},w_{r})=\frac{P(R)}{P(w_{l})P(w_{r})}
\]

\end_inset

Here, the relation 
\begin_inset Formula $R$
\end_inset

 encompases several facts: that one word is to the left of the other, and
 that they are connected by a certain link-type, as well as capturing other
 'ambient' information, perhaps such as other nearby words.
\end_layout

\begin_layout Standard
We need to harmonize here a little bit with the Deniz Yuret concepts and
 notation.
 He defines a probability 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 of seeing the ordered pair; that is, the relation 
\begin_inset Formula $R$
\end_inset

 is implicit.
 To make it explicit, we should write: 
\begin_inset Formula $P(w_{l},w_{r})=P(R(w_{l},w_{r}))$
\end_inset

 to indicate the relation explicitly, and to note that the order of the
 positions in the relation matter.
 Yuret also uses the notation 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 and 
\begin_inset Formula $P(*,w_{r})$
\end_inset

 for wild-card summations, defined as 
\begin_inset Formula 
\[
P(w_{l},*)=\sum_{w_{r}}P(w_{l},w_{r})\qquad\mbox{and}\qquad P(*,w_{r})=\sum_{w_{l}}P(w_{l},w_{r})
\]

\end_inset

In practical use, one quickly observes that 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is almost equal to 
\begin_inset Formula $P(w_{l})$
\end_inset

 but not quite, since 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is the probability of seeing 
\begin_inset Formula $w_{l}$
\end_inset

 within the certain relationship or pattern, which must be less than the
 probability of observing 
\begin_inset Formula $w_{l}$
\end_inset

 in general.
 Thus, one has 
\begin_inset Formula $P(w_{l},*)\le P(w_{l})$
\end_inset

 which can be viewed as a conditional probability:
\begin_inset Formula 
\[
P(w_{l},*)=P(R(w_{l},*))=P(R(w_{l},*)|w_{l})P(w_{l})
\]

\end_inset

In practice, then, for word-pairs, one has that 
\begin_inset Formula $P(R|w_{l})$
\end_inset

 is almost equal to 1, but not quite.
 Inserting this into the above gives 
\begin_inset Formula 
\[
P(R(w_{l},w_{r})|w_{l},w_{r})=\frac{P(R(w_{l},w_{r}))\ P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}{P(w_{l},*)P(*,w_{r})}
\]

\end_inset

Re-ordering this gives
\begin_inset Formula 
\begin{equation}
\frac{P(R(w_{l},w_{r})|w_{l},w_{r})}{P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}=\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}\label{eq:basic pair}
\end{equation}

\end_inset

The right hand side above is recognizable from Yurets work; he defines the
 mutual information as 
\begin_inset Formula 
\[
\mbox{MI}(w_{l},w_{r})=\log_{2}\,\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}
\]

\end_inset

so that laarge positive MI is associated with word that occur together only
 with themselves (e.g.
 '
\emph on
Northern Ireland
\emph default
' from his examples.) So, on the right, we have that 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 is usually very small, and that 
\begin_inset Formula $P(w,*)\approx P(*,w)\approx P(w)$
\end_inset

 subject to the inequality given before.
 
\end_layout

\begin_layout Standard
The LHS of eqn 
\begin_inset CommandInset ref
LatexCommand nameref
reference "eq:basic pair"

\end_inset

 is a stranger to me.
 First, we have observationally seen that 
\begin_inset Formula $P(R(w_{l},*)|w_{l})\approx P(R(*,w_{r})|w_{r})\approx1$
\end_inset

and thus must conclude that 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 is 'large'; much larger than the accustomed word-pair frequencies.
 Anyway, as an equation, its kind-of nicer, since it clearly isolates the
 role of the relation, since, by defintion, we have 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})=P(R(w_{l},w_{r})|w_{r},w_{l})$
\end_inset

 so that it functions like a real conditional probability, rather than this
 implicit-word-order thingy that Yuret uses.
 The Yuret notation is very convenient, but sometimes obscures things.
 OK, I think I'm done with that.
\end_layout

\begin_layout Section*
1 January 2014
\end_layout

\begin_layout Standard
OK, after that side distraction, which helped clear up notation, back to
 the main show ...
\end_layout

\begin_layout Standard
The main show is this: We want to model language, and specifically, find
 a 'minimal' set of relations R that are accurately generative.
 The meaning of 'minimal' seems obvious, intuitively, but a lot harder to
 pin down mathematically.
 We need to pin it down to get an algorithm that works in a trust-worthy,
 understandable fashion.
\end_layout

\begin_layout Standard
So: what is the total space of relations, and how do we find it? The simplest
 model is then a Zipfian distribution of words, but placed in random order.
 This model has a total entropy of 
\begin_inset Formula 
\[
H=-\sum_{w}P(w)\log_{2}P(w)
\]

\end_inset

For a recent swipe at parsing a few hundred articles from the French wikipedia,
 I get H=7.2.
 This is on 17K words, observed a total of XX times...
\end_layout

\begin_layout Standard
How does one count the entropy of the rule-set? It seems that Markov Networks
 provide the correct formalism, but its not clear how to convert this back
 into something useful; elucidating this is the goal here.
 
\end_layout

\end_body
\end_document
