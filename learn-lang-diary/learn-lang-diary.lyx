#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url} 
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 0
\use_mhchem 0
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Abstract
Collection of thoughts and notes, pursuant to language learning.
 Organized in mostly chronological order.
\end_layout

\begin_layout Section*
31 December 2013
\end_layout

\begin_layout Standard
Easier said than done.
 Seems 'obvious' but its not.
 Goal: constraint set of link rules to model only the observed language,
 via link-grammar.
 Definitions.
 Let 
\begin_inset Formula $P(R(w_{l},w_{r}))$
\end_inset

 represent the probability (frequency) of observing two words, 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in some relationship or pattern 
\begin_inset Formula $R$
\end_inset

.
 Typically, its a (link-grammar) linkage of type 
\begin_inset Formula $t$
\end_inset

 connecting word 
\begin_inset Formula $w_{l}$
\end_inset

 on the left to word 
\begin_inset Formula $w_{r}$
\end_inset

 on the right; however, 
\begin_inset Formula $R$
\end_inset

can be more general than that.
\end_layout

\begin_layout Standard
The simplest model has only on type 
\begin_inset Formula $t$
\end_inset

, the ANY type, and assigns equal probabilities to all words.
 But we know all words are not equi-probable, so let 
\begin_inset Formula $P(w)$
\end_inset

 be the probability of observing word 
\begin_inset Formula $w$
\end_inset

.
 We know from experiece this is a Zipfian distribution.
 We are then interested in the conditional probability 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 of observing the two words 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in a relation 
\begin_inset Formula $R=R(w_{l},w_{r})$
\end_inset

, given that the two individual words were observed.
 From the definition of conditional probabilities, we get 
\begin_inset Formula 
\[
P(R)=P(R|w_{l},w_{r})P(w_{l})P(w_{r})
\]

\end_inset

Or that 
\begin_inset Formula 
\[
P(R|w_{l},w_{r})=\frac{P(R)}{P(w_{l})P(w_{r})}
\]

\end_inset

Here, the relation 
\begin_inset Formula $R$
\end_inset

 encompases several facts: that one word is to the left of the other, and
 that they are connected by a certain link-type, as well as capturing other
 'ambient' information, perhaps such as other nearby words.
\end_layout

\begin_layout Standard
We need to harmonize here a little bit with the Deniz Yuret concepts and
 notation.
 He defines a probability 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 of seeing the ordered pair; that is, the relation 
\begin_inset Formula $R$
\end_inset

 is implicit.
 To make it explicit, we should write: 
\begin_inset Formula $P(w_{l},w_{r})=P(R(w_{l},w_{r}))$
\end_inset

 to indicate the relation explicitly, and to note that the order of the
 positions in the relation matter.
 Yuret also uses the notation 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 and 
\begin_inset Formula $P(*,w_{r})$
\end_inset

 for wild-card summations, defined as 
\begin_inset Formula 
\[
P(w_{l},*)=\sum_{w_{r}}P(w_{l},w_{r})\qquad\mbox{and}\qquad P(*,w_{r})=\sum_{w_{l}}P(w_{l},w_{r})
\]

\end_inset

In practical use, one quickly observes that 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is almost equal to 
\begin_inset Formula $P(w_{l})$
\end_inset

 but not quite, since 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is the probability of seeing 
\begin_inset Formula $w_{l}$
\end_inset

 within the certain relationship or pattern, which must be less than the
 probability of observing 
\begin_inset Formula $w_{l}$
\end_inset

 in general.
 Thus, one has 
\begin_inset Formula $P(w_{l},*)\le P(w_{l})$
\end_inset

 which can be viewed as a conditional probability:
\begin_inset Formula 
\[
P(w_{l},*)=P(R(w_{l},*))=P(R(w_{l},*)|w_{l})P(w_{l})
\]

\end_inset

In practice, then, for word-pairs, one has that 
\begin_inset Formula $P(R|w_{l})$
\end_inset

 is almost equal to 1, but not quite.
 Inserting this into the above gives 
\begin_inset Formula 
\[
P(R(w_{l},w_{r})|w_{l},w_{r})=\frac{P(R(w_{l},w_{r}))\ P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}{P(w_{l},*)P(*,w_{r})}
\]

\end_inset

Re-ordering this gives
\begin_inset Formula 
\begin{equation}
\frac{P(R(w_{l},w_{r})|w_{l},w_{r})}{P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}=\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}\label{eq:basic pair}
\end{equation}

\end_inset

The right hand side above is recognizable from Yurets work; he defines the
 mutual information as 
\begin_inset Formula 
\[
\mbox{MI}(w_{l},w_{r})=\log_{2}\,\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}
\]

\end_inset

so that laarge positive MI is associated with word that occur together only
 with themselves (e.g.
 '
\emph on
Northern Ireland
\emph default
' from his examples.) So, on the right, we have that 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 is usually very small, and that 
\begin_inset Formula $P(w,*)\approx P(*,w)\approx P(w)$
\end_inset

 subject to the inequality given before.
 
\end_layout

\begin_layout Standard
The LHS of eqn 
\begin_inset CommandInset ref
LatexCommand nameref
reference "eq:basic pair"

\end_inset

 is a stranger to me.
 First, we have observationally seen that 
\begin_inset Formula $P(R(w_{l},*)|w_{l})\approx P(R(*,w_{r})|w_{r})\approx1$
\end_inset

and thus must conclude that 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 is 'large'; much larger than the accustomed word-pair frequencies.
 Anyway, as an equation, its kind-of nicer, since it clearly isolates the
 role of the relation, since, by defintion, we have 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})=P(R(w_{l},w_{r})|w_{r},w_{l})$
\end_inset

 so that it functions like a real conditional probability, rather than this
 implicit-word-order thingy that Yuret uses.
 The Yuret notation is very convenient, but sometimes obscures things.
 OK, I think I'm done with that.
\end_layout

\begin_layout Section*
1 January 2014
\end_layout

\begin_layout Standard
OK, after that side distraction, which helped clear up notation, back to
 the main show ...
\end_layout

\begin_layout Standard
The main show is this: We want to model language, and specifically, find
 a 'minimal' set of relations R that are accurately generative.
 The meaning of 'minimal' seems obvious, intuitively, but a lot harder to
 pin down mathematically.
 We need to pin it down to get an algorithm that works in a trust-worthy,
 understandable fashion.
\end_layout

\begin_layout Standard
So: what is the total space of relations, and how do we find it? The simplest
 model is then a Zipfian distribution of words, but placed in random order.
 This model has a total entropy of 
\begin_inset Formula 
\[
H=-\sum_{w}P(w)\log_{2}P(w)
\]

\end_inset

For a recent swipe at parsing a few hundred articles from the French wikipedia,
 I get H=7.2.
 This is on 17K words, observed a total of 35M times (actually, observerd
 each sentence 100 times, so really just 350K 'true' observations of words).
\end_layout

\begin_layout Standard
How does one count the entropy of the rule-set? Elucidating this is the
 goal-set.
\end_layout

\begin_layout Standard
But first, step back: describe the rules.
 
\end_layout

\begin_layout Standard
OK ...
 so, once again ...
 sentence structure is to be described via link-grammar, using disjoined
 conjunctions of connectors.
 This is theoretically sound, as it seems to be isomorphic to categorical
 grammars (via type-theory of the connectors; need a formal proof of this
 someday, but for now it seems 'obvious').
 Also link-grammar is fully compatible with dependency grammar.
 So lets move forward.
 
\end_layout

\begin_layout Subsection*
How to count relations
\end_layout

\begin_layout Standard
Consider a sentence with 
\begin_inset Formula $n$
\end_inset

 words in it, numbered 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

 left to right.
 We want to constrain grammar by discovering a set of relations 
\begin_inset Formula $R(w_{1},w_{2},\cdots,w_{n})$
\end_inset

 such that 
\begin_inset Formula $P(R(w_{1},w_{2},\cdots,w_{n}))>0$
\end_inset

 when the sentence is gramatically valid (i.e.
 such an 
\begin_inset Formula $R$
\end_inset

 exists), and 
\begin_inset Formula $P$
\end_inset

 is zero when no such 
\begin_inset Formula $R$
\end_inset

 exists (i.e.
 the sentence is not gramatically valid.) The first and most obvious simplificati
on rule is to observe that 
\begin_inset Formula $R$
\end_inset

 can be replaced by 
\begin_inset Formula $R(W_{1},W_{2},\cdots,W_{n})$
\end_inset

 where each 
\begin_inset Formula $W_{k}$
\end_inset

 is a set of words.
 That is, instead of listing each sentences individually, we list certain
 classes of sentences.
 In other words, the relations 
\begin_inset Formula $R(w_{1},w_{2},\cdots,w_{n})$
\end_inset

 are in one-to-one correspondance with a list of grammatical sentences 
\begin_inset Formula $(w_{1},w_{2},\cdots,w_{n})$
\end_inset

, so simply listing all possible sentences is a very verbose way of specifying
 a grammar.
 It is linguistically 'obvious' that sentences fall into classes, and so
 the two relations 
\begin_inset Formula $R('this','is','a','dog')$
\end_inset

 and 
\begin_inset Formula $R('this','is','a','cat')$
\end_inset

 can be replaced by 
\begin_inset Formula $R('this','is','a',W_{n})$
\end_inset

 where 
\begin_inset Formula $W_{n}=\{'dog','cat'\}$
\end_inset

.
 In fact, 
\begin_inset Formula $W_{n}$
\end_inset

 can be a rather large set of nouns.
 
\end_layout

\begin_layout Standard
So ...
 the question is: what is the reduction of complexity, by performing this
 classification? What is the correct way of counting? I assume that 'complexity'
 is a synonym for 'entropy', so we are looking to do two things: enumerate
 the states of the system, and proivde a measure for complexity.
 So, lets consider a language with 
\begin_inset Formula $N$
\end_inset

 nouns, so that the cardinality of 
\begin_inset Formula $W_{n}$
\end_inset

 is 
\begin_inset Formula $|W_{n}|=N$
\end_inset

 and the only valid sentences are 
\begin_inset Formula $('this','is','a',w)$
\end_inset

 with 
\begin_inset Formula $w\in W_{n}$
\end_inset

 .
 Before simplification, we had 
\begin_inset Formula $N$
\end_inset

 relations 
\begin_inset Formula $R$
\end_inset

, one per sentence.
 We also had 
\begin_inset Formula $N+3$
\end_inset

 sets, each set containing a single word; the 
\begin_inset Formula $N$
\end_inset

 nouns, and the three words 
\begin_inset Formula $'this','is','a'$
\end_inset

.
 After simplification, we have one relation 
\begin_inset Formula $R$
\end_inset

, and four sets; three of the sets have cardinality 1, the fourth set has
 cardinality 
\begin_inset Formula $N$
\end_inset

.
 
\end_layout

\begin_layout Standard
I think the correct counting rule is to count set-membership relations on
 equal footing with structural relations.
 Thus, before simplification, we had 
\begin_inset Formula $N+3$
\end_inset

 sets, each a singleton, and thus 
\begin_inset Formula $N+3$
\end_inset

 set membership relations.
 After simplification, we have four sets, but still have 
\begin_inset Formula $N+3$
\end_inset

 set membership relations.
 Thus, this particular simplification step does not reduce the number of
 membership relations at all.
 I think this is the right way to count.
 Let provisionally go with this and see what happens.
 Thus, before simplification, we had 
\begin_inset Formula $2N+3$
\end_inset

 relations grand-total, and afterwords, we have 
\begin_inset Formula $N+4$
\end_inset

 relations grand-total.
\end_layout

\begin_layout Standard
What is the correct 'thermodynamic' picture of what's going on? In this
 toy problem, we have a grand-total state space of size 
\begin_inset Formula $(N+3)^{4}$
\end_inset

 since any of the 
\begin_inset Formula $N+3$
\end_inset

 words can appear in any of the four slots in a four-word sentence (micro-canonn
ical ensemble).
 The entropy, at 'infinite temperature' where all possible four-word sequences
 occur with equal probability is then 
\begin_inset Formula $4\log_{2}(N+3)$
\end_inset

.
 The entropy of the set of grammatical sentences is 
\begin_inset Formula $\log_{2}N$
\end_inset

 since there are only 
\begin_inset Formula $N$
\end_inset

 possible grammatical sentences.
 In this toy grammar, there are also invalid setences of length 1,2,3,5,6,7,...
 and so the total size of the space of word-sequences is clearly infinite.
 
\end_layout

\begin_layout Standard
OK, so the space of word-sequences is very concrete, and easy to describe
 and measure, at least for toy grammars.
 What about the space of relations? Well, the claim is that the entropies
 of the before-and-after models are 
\begin_inset Formula $\log_{2}(2N+3)$
\end_inset

 and 
\begin_inset Formula $\log_{2}(N+4)$
\end_inset

, respectively.
 Neither of these matches the entropy of the set of allowed sentences, so
 this seems paradoxical, and begs the questions 'did we count correctly?'
 and 'did we actually simplify anything by making the above change of descritpio
n?' Hmm.
 The correct answer seems to be 'no' and 'no'.
 The correct counting methodology seems to be to subtract 1 from the cardinality
 of every set.
 This would then give both 
\begin_inset Formula $\log_{2}N$
\end_inset

 as the entropy for both the before and after relation-sets.
 Thus, before, we had 
\begin_inset Formula $N$
\end_inset

 relations and 
\begin_inset Formula $N+3$
\end_inset

 sets, each of weight zero, for a total wieghted-relation count of 
\begin_inset Formula $N$
\end_inset

.
 After, we have one relation and four sets; three of the sets have wieght
 zero, one set has a weight of 
\begin_inset Formula $N-1$
\end_inset

 so the total wieghted relations is again 
\begin_inset Formula $N$
\end_inset

.
 This seems to resolve the paradox.
 Lets move on to more challenging domains.
 
\end_layout

\begin_layout Subsection*
Counting Link-Grammar Relations
\end_layout

\begin_layout Standard
Per link-grammar, each relation is decomposable into pair-wise relations;
 this is the so-called 'parse' of a sentence.
 If the relation is a single word-per-slot sentence relation, then the 'parse'
 is literal.
 We write 
\begin_inset Formula 
\begin{equation}
R(w_{1},w_{2},\cdots,w_{n})=\prod_{j,k,m}R_{\alpha}(w_{j},w_{k},t_{m})\ Q(R_{\alpha},R_{\beta},\cdots,R_{\omega})\label{eq:pair-decompose}
\end{equation}

\end_inset

 where 
\begin_inset Formula $R_{\alpha}(w_{j},w_{k},t_{m})$
\end_inset

 is a single connected pair of words, connected by the connector 
\begin_inset Formula $t_{m}$
\end_inset

.
 The product symbol 
\begin_inset Formula $\prod$
\end_inset

 implies that all such binary relations must hold.
 The awkward 
\begin_inset Formula $Q(R_{\alpha},R_{\beta},\cdots,R_{\omega})$
\end_inset

 at the end is the additional no-links-cross constraint in the current link-gram
mar parser.
 Its a non-local constraint involving all of the binary relations.
 It also subsumes any 'post-processing' rules, although, for the language
 learnign exercise, there won't be any post-processing rules.
 At any rate, 
\begin_inset Formula $Q$
\end_inset

 is a place where higher ordrer constraints can be applied.
 In particular, the most genneral form for 
\begin_inset Formula $Q$
\end_inset

 should be 
\begin_inset Formula $Q(R_{\alpha},R_{\beta},\cdots,R_{\omega},w_{1},w_{2},\cdots,w_{n})$
\end_inset

 since, in principle, it could depend on the word-choice, although the no-links-
cross constraint does not.
 
\end_layout

\begin_layout Standard
Yuret proposes a way of discovering the pair-wise relations.
 He makes the implicit, unvoiced assumption that there is a single, unique
 connector type 
\begin_inset Formula $t_{m}$
\end_inset

 for every ordered pair of words 
\begin_inset Formula $w_{j},w_{k}$
\end_inset

; that is, that 
\begin_inset Formula $t_{m}=t_{m}(w_{j},w_{k})$
\end_inset

.
 Viz, specifically, that such connectors are in 1-1 correspondance with
 work-pairs.
 (I don't think he's aware of this assumption; I don't think anyone has
 ever before realized that he's making such an assumption; certainly, I
 haven't).
 Yuret then makes two claims: first, that the only possible grammatically
 correct parses are those of the above form (eqn 
\begin_inset CommandInset ref
LatexCommand nameref
reference "eq:pair-decompose"

\end_inset

) for which the relations 
\begin_inset Formula $R_{\alpha}(w_{j},w_{k},t_{m}(w_{j},w_{k}))$
\end_inset

 have been previously observed; secondly, that there is a natural ranking
 of such allowed parses by summing the total mutual information associated
 with weach word-pair.
 
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\begin_layout Standard
x
\end_layout

\end_body
\end_document
