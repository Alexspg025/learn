#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url} 
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 0
\use_mhchem 0
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Abstract
Collection of thoughts and notes, pursuant to language learning.
 Organized in mostly chronological order.
\end_layout

\begin_layout Section*
31 December 2013
\end_layout

\begin_layout Standard
Easier said than done.
 Seems 'obvious' but its not.
 Goal: constraint set of link rules to model only the observed language,
 via link-grammar.
 Definitions.
 Let 
\begin_inset Formula $P(R(w_{l},w_{r}))$
\end_inset

 represent the probability (frequency) of observing two words, 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in some relationship or pattern 
\begin_inset Formula $R$
\end_inset

.
 Typically, its a (link-grammar) linkage of type 
\begin_inset Formula $t$
\end_inset

 connecting word 
\begin_inset Formula $w_{l}$
\end_inset

 on the left to word 
\begin_inset Formula $w_{r}$
\end_inset

 on the right; however, 
\begin_inset Formula $R$
\end_inset

can be more general than that.
\end_layout

\begin_layout Standard
The simplest model has only on type 
\begin_inset Formula $t$
\end_inset

, the ANY type, and assigns equal probabilities to all words.
 But we know all words are not equi-probable, so let 
\begin_inset Formula $P(w)$
\end_inset

 be the probability of observing word 
\begin_inset Formula $w$
\end_inset

.
 We know from experiece this is a Zipfian distribution.
 We are then interested in the conditional probability 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 of observing the two words 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in a relation 
\begin_inset Formula $R=R(w_{l},w_{r})$
\end_inset

, given that the two individual words were observed.
 From the definition of conditional probabilities, we get 
\begin_inset Formula 
\[
P(R)=P(R|w_{l},w_{r})P(w_{l})P(w_{r})
\]

\end_inset

Or that 
\begin_inset Formula 
\[
P(R|w_{l},w_{r})=\frac{P(R)}{P(w_{l})P(w_{r})}
\]

\end_inset

Here, the relation 
\begin_inset Formula $R$
\end_inset

 encompases several facts: that one word is to the left of the other, and
 that they are connected by a certain link-type, as well as capturing other
 'ambient' information, perhaps such as other nearby words.
\end_layout

\begin_layout Standard
We need to harmonize here a little bit with the Deniz Yuret concepts and
 notation.
 He defines a probability 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 of seeing the ordered pair; that is, the relation 
\begin_inset Formula $R$
\end_inset

 is implicit.
 To make it explicit, we should write: 
\begin_inset Formula $P(w_{l},w_{r})=P(R(w_{l},w_{r}))$
\end_inset

 to indicate the relation explicitly, and to note that the order of the
 positions in the relation matter.
 Yuret also uses the notation 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 and 
\begin_inset Formula $P(*,w_{r})$
\end_inset

 for wild-card summations, defined as 
\begin_inset Formula 
\[
P(w_{l},*)=\sum_{w_{r}}P(w_{l},w_{r})\qquad\mbox{and}\qquad P(*,w_{r})=\sum_{w_{l}}P(w_{l},w_{r})
\]

\end_inset

In practical use, one quickly observes that 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is almost equal to 
\begin_inset Formula $P(w_{l})$
\end_inset

 but not quite, since 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is the probability of seeing 
\begin_inset Formula $w_{l}$
\end_inset

 within the certain relationship or pattern, which must be less than the
 probability of observing 
\begin_inset Formula $w_{l}$
\end_inset

 in general.
 Thus, one has 
\begin_inset Formula $P(w_{l},*)\le P(w_{l})$
\end_inset

 which can be viewed as a conditional probability:
\begin_inset Formula 
\[
P(w_{l},*)=P(R(w_{l},*))=P(R(w_{l},*)|w_{l})P(w_{l})
\]

\end_inset

In practice, then, for word-pairs, one has that 
\begin_inset Formula $P(R|w_{l})$
\end_inset

 is almost equal to 1, but not quite.
 Inserting this into the above gives 
\begin_inset Formula 
\[
P(R(w_{l},w_{r})|w_{l},w_{r})=\frac{P(R(w_{l},w_{r}))\ P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}{P(w_{l},*)P(*,w_{r})}
\]

\end_inset

Re-ordering this gives
\begin_inset Formula 
\begin{equation}
\frac{P(R(w_{l},w_{r})|w_{l},w_{r})}{P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}=\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}\label{eq:basic pair}
\end{equation}

\end_inset

The right hand side above is recognizable from Yurets work; he defines the
 mutual information as 
\begin_inset Formula 
\[
\mbox{MI}(w_{l},w_{r})=\log_{2}\,\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}
\]

\end_inset

so that laarge positive MI is associated with word that occur together only
 with themselves (e.g.
 '
\emph on
Northern Ireland
\emph default
' from his examples.) So, on the right, we have that 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 is usually very small, and that 
\begin_inset Formula $P(w,*)\approx P(*,w)\approx P(w)$
\end_inset

 subject to the inequality given before.
 
\end_layout

\begin_layout Standard
The LHS of eqn 
\begin_inset CommandInset ref
LatexCommand nameref
reference "eq:basic pair"

\end_inset

 is a stranger to me.
 First, we have observationally seen that 
\begin_inset Formula $P(R(w_{l},*)|w_{l})\approx P(R(*,w_{r})|w_{r})\approx1$
\end_inset

and thus must conclude that 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 is 'large'; much larger than the accustomed word-pair frequencies.
 Anyway, as an equation, its kind-of nicer, since it clearly isolates the
 role of the relation, since, by defintion, we have 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})=P(R(w_{l},w_{r})|w_{r},w_{l})$
\end_inset

 so that it functions like a real conditional probability, rather than this
 implicit-word-order thingy that Yuret uses.
 The Yuret notation is very convenient, but sometimes obscures things.
 OK, I think I'm done with that.
\end_layout

\begin_layout Section*
1 January 2014
\end_layout

\begin_layout Standard
OK, after that side distraction, which helped clear up notation, back to
 the main show ...
\end_layout

\begin_layout Standard
The main show is this: We want to model language, and specifically, find
 a 'minimal' set of relations R that are accurately generative.
 The meaning of 'minimal' seems obvious, intuitively, but a lot harder to
 pin down mathematically.
 We need to pin it down to get an algorithm that works in a trust-worthy,
 understandable fashion.
\end_layout

\begin_layout Standard
So: what is the total space of relations, and how do we find it? The simplest
 model is then a Zipfian distribution of words, but placed in random order.
 This model has a total entropy of 
\begin_inset Formula 
\[
H=-\sum_{w}P(w)\log_{2}P(w)
\]

\end_inset

For a recent swipe at parsing a few hundred articles from the French wikipedia,
 I get H=7.2.
 This is on 17K words, observed a total of 35M times (actually, observerd
 each sentence 100 times, so really just 350K 'true' observations of words).
\end_layout

\begin_layout Standard
How does one count the entropy of the rule-set? Elucidating this is the
 goal-set.
\end_layout

\begin_layout Standard
But first, step back: describe the rules.
 
\end_layout

\begin_layout Standard
OK ...
 so, once again ...
 sentence structure is to be described via link-grammar, using disjoined
 conjunctions of connectors.
 This is theoretically sound, as it seems to be isomorphic to categorical
 grammars (via type-theory of the connectors; need a formal proof of this
 someday, but for now it seems 'obvious').
 Also link-grammar is fully compatible with dependency grammar.
 So lets move forward.
 
\end_layout

\begin_layout Subsection*
How to count relations
\end_layout

\begin_layout Standard
Consider a sentence with 
\begin_inset Formula $n$
\end_inset

 words in it, numbered 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

 left to right.
 We want to constrain grammar by discovering a set of relations 
\begin_inset Formula $R(w_{1},w_{2},\cdots,w_{n})$
\end_inset

 such that 
\begin_inset Formula $P(R(w_{1},w_{2},\cdots,w_{n}))>0$
\end_inset

 when the sentence is gramatically valid (i.e.
 such an 
\begin_inset Formula $R$
\end_inset

 exists), and 
\begin_inset Formula $P$
\end_inset

 is zero when no such 
\begin_inset Formula $R$
\end_inset

 exists (i.e.
 the sentence is not gramatically valid.) The first and most obvious simplificati
on rule is to observe that 
\begin_inset Formula $R$
\end_inset

 can be replaced by 
\begin_inset Formula $R(W_{1},W_{2},\cdots,W_{n})$
\end_inset

 where each 
\begin_inset Formula $W_{k}$
\end_inset

 is a set of words.
 That is, instead of listing each sentences individually, we list certain
 classes of sentences.
 In other words, the relations 
\begin_inset Formula $R(w_{1},w_{2},\cdots,w_{n})$
\end_inset

 are in one-to-one correspondance with a list of grammatical sentences 
\begin_inset Formula $(w_{1},w_{2},\cdots,w_{n})$
\end_inset

, so simply listing all possible sentences is a very verbose way of specifying
 a grammar.
 It is linguistically 'obvious' that sentences fall into classes, and so
 the two relations 
\begin_inset Formula $R('this','is','a','dog')$
\end_inset

 and 
\begin_inset Formula $R('this','is','a','cat')$
\end_inset

 can be replaced by 
\begin_inset Formula $R('this','is','a',W_{n})$
\end_inset

 where 
\begin_inset Formula $W_{n}=\{'dog','cat'\}$
\end_inset

.
 In fact, 
\begin_inset Formula $W_{n}$
\end_inset

 can be a rather large set of nouns.
 
\end_layout

\begin_layout Standard
So ...
 the question is: what is the reduction of complexity, by performing this
 classification? What is the correct way of counting? I assume that 'complexity'
 is a synonym for 'entropy', so we are looking to do two things: enumerate
 the states of the system, and proivde a measure for complexity.
 So, lets consider a language with 
\begin_inset Formula $N$
\end_inset

 nouns, so that the cardinality of 
\begin_inset Formula $W_{n}$
\end_inset

 is 
\begin_inset Formula $|W_{n}|=N$
\end_inset

 and the only valid sentences are 
\begin_inset Formula $('this','is','a',w)$
\end_inset

 with 
\begin_inset Formula $w\in W_{n}$
\end_inset

 .
 Before simplification, we had 
\begin_inset Formula $N$
\end_inset

 relations 
\begin_inset Formula $R$
\end_inset

, one per sentence.
 We also had 
\begin_inset Formula $N+3$
\end_inset

 sets, each set containing a single word; the 
\begin_inset Formula $N$
\end_inset

 nouns, and the three words 
\begin_inset Formula $'this','is','a'$
\end_inset

.
 After simplification, we have one relation 
\begin_inset Formula $R$
\end_inset

, and four sets; three of the sets have cardinality 1, the fourth set has
 cardinality 
\begin_inset Formula $N$
\end_inset

.
 
\end_layout

\begin_layout Standard
I think the correct counting rule is to count set-membership relations on
 equal footing with structural relations.
 Thus, before simplification, we had 
\begin_inset Formula $N+3$
\end_inset

 sets, each a singleton, and thus 
\begin_inset Formula $N+3$
\end_inset

 set membership relations.
 After simplification, we have four sets, but still have 
\begin_inset Formula $N+3$
\end_inset

 set membership relations.
 Thus, this particular simplification step does not reduce the number of
 membership relations at all.
 I think this is the right way to count.
 Let provisionally go with this and see what happens.
 Thus, before simplification, we had 
\begin_inset Formula $2N+3$
\end_inset

 relations grand-total, and afterwords, we have 
\begin_inset Formula $N+4$
\end_inset

 relations grand-total.
\end_layout

\begin_layout Standard
What is the correct 'thermodynamic' picture of what's going on? In this
 toy problem, we have a grand-total state space of size 
\begin_inset Formula $(N+3)^{4}$
\end_inset

 since any of the 
\begin_inset Formula $N+3$
\end_inset

 words can appear in any of the four slots in a four-word sentence (micro-canonn
ical ensemble).
 The entropy, at 'infinite temperature' where all possible four-word sequences
 occur with equal probability is then 
\begin_inset Formula $4\log_{2}(N+3)$
\end_inset

.
 The entropy of the set of grammatical sentences is 
\begin_inset Formula $\log_{2}N$
\end_inset

 since there are only 
\begin_inset Formula $N$
\end_inset

 possible grammatical sentences.
 In this toy grammar, there are also invalid setences of length 1,2,3,5,6,7,...
 and so the total size of the space of word-sequences is clearly infinite.
 
\end_layout

\begin_layout Standard
OK, so the space of word-sequences is very concrete, and easy to describe
 and measure, at least for toy grammars.
 What about the space of relations? Well, the claim is that the entropies
 of the before-and-after models are 
\begin_inset Formula $\log_{2}(2N+3)$
\end_inset

 and 
\begin_inset Formula $\log_{2}(N+4)$
\end_inset

, respectively.
 Neither of these matches the entropy of the set of allowed sentences, so
 this seems paradoxical, and begs the questions 'did we count correctly?'
 and 'did we actually simplify anything by making the above change of descritpio
n?' Hmm.
 The correct answer seems to be 'no' and 'no'.
 The correct counting methodology seems to be to subtract 1 from the cardinality
 of every set.
 This would then give both 
\begin_inset Formula $\log_{2}N$
\end_inset

 as the entropy for both the before and after relation-sets.
 Thus, before, we had 
\begin_inset Formula $N$
\end_inset

 relations and 
\begin_inset Formula $N+3$
\end_inset

 sets, each of weight zero, for a total wieghted-relation count of 
\begin_inset Formula $N$
\end_inset

.
 After, we have one relation and four sets; three of the sets have wieght
 zero, one set has a weight of 
\begin_inset Formula $N-1$
\end_inset

 so the total wieghted relations is again 
\begin_inset Formula $N$
\end_inset

.
 This seems to resolve the paradox.
 Lets move on to more challenging domains.
 
\end_layout

\begin_layout Subsection*
Counting Link-Grammar Relations
\end_layout

\begin_layout Standard
Per link-grammar, each relation is decomposable into pair-wise relations;
 this is the so-called 'parse' of a sentence.
 If the relation is a single word-per-slot sentence relation, then the 'parse'
 is literal.
 We write 
\begin_inset Formula 
\begin{equation}
R(w_{1},w_{2},\cdots,w_{n})=\prod_{j,k,m}R_{\alpha}(w_{j},w_{k},t_{m})\ Q(R_{\alpha},R_{\beta},\cdots,R_{\omega})\label{eq:pair-decompose}
\end{equation}

\end_inset

 where 
\begin_inset Formula $R_{\alpha}(w_{j},w_{k},t_{m})$
\end_inset

 is a single connected pair of words, connected by the connector 
\begin_inset Formula $t_{m}$
\end_inset

.
 The product symbol 
\begin_inset Formula $\prod$
\end_inset

 implies that all such binary relations must hold.
 The awkward 
\begin_inset Formula $Q(R_{\alpha},R_{\beta},\cdots,R_{\omega})$
\end_inset

 at the end is the additional no-links-cross constraint in the current link-gram
mar parser.
 Its a non-local constraint involving all of the binary relations.
 It also subsumes any 'post-processing' rules, although, for the language
 learnign exercise, there won't be any post-processing rules.
 At any rate, 
\begin_inset Formula $Q$
\end_inset

 is a place where higher ordrer constraints can be applied.
 In particular, the most genneral form for 
\begin_inset Formula $Q$
\end_inset

 should be 
\begin_inset Formula $Q(R_{\alpha},R_{\beta},\cdots,R_{\omega},w_{1},w_{2},\cdots,w_{n})$
\end_inset

 since, in principle, it could depend on the word-choice, although the no-links-
cross constraint does not.
 
\end_layout

\begin_layout Standard
Yuret proposes a way of discovering the pair-wise relations
\begin_inset CommandInset citation
LatexCommand cite
key "Yuret1998"

\end_inset

.
 He makes the implicit, unvoiced assumption that there is a single, unique
 connector type 
\begin_inset Formula $t_{m}$
\end_inset

 for every ordered pair of words 
\begin_inset Formula $w_{j},w_{k}$
\end_inset

; that is, that 
\begin_inset Formula $t_{m}=t_{m}(w_{j},w_{k})$
\end_inset

.
 Viz, specifically, that such connectors are in 1-1 correspondance with
 work-pairs.
 (I don't think he's aware of this assumption; I don't think anyone has
 ever before realized that he's making such an assumption; certainly, I
 haven't).
 Yuret then makes two claims: first, that the only possible grammatically
 correct parses are those of the above form (eqn 
\begin_inset CommandInset ref
LatexCommand nameref
reference "eq:pair-decompose"

\end_inset

) for which the relations 
\begin_inset Formula $R_{\alpha}(w_{j},w_{k},t_{m}(w_{j},w_{k}))$
\end_inset

 have been previously observed; secondly, that there is a natural ranking
 of such allowed parses by summing the total mutual information associated
 with each word-pair.
\end_layout

\begin_layout Standard
These two concepts give rise to the idea of minimum-spanning-tree parsers.
 Such parsers work in a two-step process: a training phase, and a parse
 phase.
 In the training phase, one gathers a lot of statistics about mutual information.
 The important point here is that this is unsupervised training.
 To parse, one first creates a graph clique, with every word connected to
 every other.
 One uses the gathered MI to define graph edge lengths.
 Finally, the corrrect parse is then the maximum spanning tree of the graph
 (maximizing the MI, summed over the tree edges in the graph).
\end_layout

\begin_layout Standard
Here, we use the same idea, but then take the next step.
 The spanning tree can be decomposed into a set of link-grammar disjuncts,
 one disjunct per word.
 The disjunct is merely a list of the connections that one word makes.
 It consists of the type, and the direction.
 The direction is left or right.
 The type is the 
\begin_inset Formula $t_{m}=t_{m}(w_{j},w_{k})$
\end_inset

 defined above.
 By parsing a large number of sentences, we can now automatically discover
 a large number of disjuncts, in an unsupervised manner.
\end_layout

\begin_layout Standard
The goal, the next step, is then to reduce the total number of disjuncts,
 and the total number of types, by clustering and discovering similarities.
\end_layout

\begin_layout Section*
3 January 2014
\end_layout

\begin_layout Subsection*
No-crossing Minimum Spanning Trees
\end_layout

\begin_layout Standard
It turns out that writing an algorithm for a no-crossing minimum spanning
 tree is surprisingly painful; enforcing the no-crossing constraint requies
 treatment of a number of special cases.
 But perhaps this is not actually rquired! R.
 Ferrer i Cancho in 
\begin_inset Quotes eld
\end_inset

Why do syntactic links not cross?
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Ferrer2006"

\end_inset

 shows that, when attempting to arrange a random set of points on a line,
 in such a way as to minimize euclidean distances between connected points,
 one ends up with trees that almost never cross!
\end_layout

\begin_layout Standard
Other related references:
\end_layout

\begin_layout Itemize
Crossings are rare: Havelka, J.
 (2007).
 Beyond projectivity: multilingual evaluation of constraints and measures
 on non-projective structures.
 In: Proceedings of the 45th Annual Meeting of the Association of Computational
 Linguistics (ACL-07): 608-615.
 Prague, Czech Republic: Association for Computational Linguistics.
 
\end_layout

\begin_layout Itemize
Hubbiness is a better model of sentence complexity than mean dependency
 distance: Ramon Ferrer-i-Cancho (2013) 
\begin_inset Quotes eld
\end_inset

Hubiness, length, crossings and their relationships in dependency trees
\begin_inset Quotes erd
\end_inset

, ArXiv 1304.4086 --- also states: maximum number of crossings is bounded
 above by mean dependency length.
 Also, mean dependency length is bounded below by variance of degrees of
 vertexes (i.e.
 variance in number of connectors a word can have).
\end_layout

\begin_layout Itemize
Languages tends to be close to the theoretical minimum possible dependency
 distance, if it was legal to re-arrange words arbitrarily.
 See Temperley, D.
 (2008).
 Dependency length minimization in natural and artificial languages.
 Journal of Quantitative Linguistics, 15(3):256-282.
 
\end_layout

\begin_layout Itemize
Park, Y.
 A.
 and Levy, R.
 (2009).
 Minimal-length linearizations for mildly context-sensitive dependency trees.
 In Proceedings of the North American Chapter of the Association for Computation
al Linguistics - Human Language Technologies (NAACL-HLT) conference.
 
\end_layout

\begin_layout Itemize
Sentences with long dependencies are hard to understand: The original claim
 is from Yngve, 1960, having to do with phrase-structure depth.
 See -- Gibson, E.
 (2000).
 The dependency locality theory: A distance-based theory of linguistic complexit
y.
 In Marantz, A., Miyashita, Y., and O'Neil, W., editors, Image, Language, Brain.
 Papers from the first Mind Articulation Project Symposium.
 MIT Press, Cambridge, MA.
 
\end_layout

\begin_layout Itemize
(Cite this, its good) Mean dependency distance is a good measure of sentence
 complexity -- for 20 languages -- give overview startgin from Yngve.
 Haitao Liu (2008) 
\begin_inset Quotes eld
\end_inset

Dependency distance as a metric of language comprehension difficulty
\begin_inset Quotes erd
\end_inset

 Journal of Cognitive Science, 2008 9(2): 159-191.
 
\end_layout

\begin_layout Itemize
Sentences with long dependencies are rarely spoken: Hawkins, J.
 A.
 (1994).
 A Performance Theory of Order and Constituency.
 Cambridge University Press, Cambridge, UK.
 ----Hawkins, J.
 A.
 (2004).
 Efficiency and Complexity in Grammars.
 Oxford University Press, Oxford, UK.
 ----Wasow, T.
 (2002).
 Postverbal Behavior.
 CSLI Publications, Stanford, CA.
 Distributed by University of Chicago Press.
 
\end_layout

\begin_layout Standard
So, ratherr than imposing no-crossing as a constraint on the parser, instead,
 let it find its own way into the grammar.
 Just implement a plain-old MST parser, punt on crossing.
\end_layout

\begin_layout Section*
The End 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "alpha"

\end_inset


\end_layout

\end_body
\end_document
