Fake (Artificial) Languages
===========================

Tools to create custom-taylored fake languages having custom-tailored
statistical properties. This will allow the learning code to be
evaluated on the corpora generated by these languages, thus allowing
a better understanding of how the learning code is able to extract
structure from language.

The advantages of doing this:

* The ability to strictly control the size of the vocabulary.
  (This is much easier than curating a corpus of child-directed
  writing and speech, or a corpus of simple English (Simple English
  Wikipedia; Thing Explainer).

* The ability to control the grammar of the language. This includes:

  * Controlling the arity (number of dependents) that a word may have
    (e.g. transitive verbs have an arity of two: a connector to the
    subject, and a connector to the object. Common nouns have an arity
    of one: a connector to the verb.)

  * Controlling the relative frequency of nouns, verbs, modifiers.

  * Controlling the number of word-senses that a word might participate
    in, thus validating word-sense disambiguation abilities.

* The ability to control the frequency of word distributions. In natural
  language, word distributions tend to be Zipfian, and thus have a
  vocabulary dependent on the corpus size. In Hanzi languages, the
  number of Hanzi is relatively fixed, and one can potentially
  over-sample. Using an artificial languge can help disentangle such
  effects.

* The ability to generate a perfect corpus, free of stray markup
  typically found in natural corpora. Natural corpora tend to have
  typesetting markup (e.g. html) that leaks through, no matter how
  much scrubbing is applied. Natural corpora also tend to have
  tables, indexes and lists, which are not grammatically structured
  or are only weakly structured. By using a perfect corpus, these
  can be eliminated, or they can be introduced in known ways.

* Most importantly: the ability to perform a perfect evaluation of
  the learning results, since the grammar is known exactly. The
  measurement of accuracy, precision, and recall can be measured
  precisely, without any need for a "golden reference" or
  linguist-generated reference parses.

Status
------
Basic Link Grammar flat-file dictionaries can be created.

To create a dictionary, load the `random-dict.scm` file, and follow
the instructions at the top of that file.


Generating a corpus
-------------------
A corpus will be generated in one of two different ways.

=== Method 1 "Link-Grammar hack"
Amir is creating (has created) a tool that will create a corpus of
sentences, given a Link Grammar dictionary. The current plan is to
use the code in this directory to create random grammars, and feed
them to Amir's tool to create a corpus of sentences.

=== Method 2 "The right way"
The "right way" to generate a corpus is to use a properly designed
corpus generator that can create corpora of not just natural language
sentences, but also of more complex graphs (e.g. chemistry, social
networks, etc.)

The [Network Generation project](https://github.com/opencog/generate/)
is supposed to be able to do this.  The current prototype there is able
to generate a corpus of "grammatically valid sentences" aka syntactic
trees from small grammars. It should work ... it might get slow on
grammars larger than a few dozen words, a few dozen disjuncts; such
larger generations have not yet been attempted. This is partly because
the code there is also trying to do far more sophisticated things, and
to solve far more complex problems as well.

=== Method 3 "More ideas"
Several more ways to generate text:
1. Start with sections, and build a sentence
2. Create a random planar tree, and assign sections to it.

Approach 1. is difficult: it basically means we have to run the
parser, using a dictionary containing the desired sections, and
allowing an "any word" mode during parsing. This can be done,
because we already have scheme interfaces into LG, via the
ParseMinimalLink. But its complex and awkard.

Approach 2. is easier(?): create an unlabelled tree; that's easy.
Start adding random labels to it, veryifying that each disjunct
is in the dictionary. This is harder, as this is a coloring problem,
and requires backtracking if the first coloring attempt fails.
As the final step, one randomly picks a word from the dictionary that
appears in a section for that disjunct.

HOWTO
-----
Start the guile shell, and load `fake.scm`:
```
$ guile
scheme@(guile-user)> (load "fake.scm")
scheme@(guile-user)> (create-classes 10 10)
```
